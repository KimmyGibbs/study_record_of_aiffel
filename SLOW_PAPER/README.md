# papers list

1. Attention is All You Need (Transformer)
    - https://arxiv.org/abs/1706.03762
2. LoRA: Low-Rank Adaptation of Large Language Models (LoRA)
    - https://arxiv.org/abs/2106.09685
3. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale (LLM.int8())
    - https://arxiv.org/abs/2208.07339
4. QLoRA: Efficient Finetuning of Quantized LLMs (QLoRA)
    - https://arxiv.org/abs/2305.14314