{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP Frameworkì˜ í™œìš©**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-1. ë“¤ì–´ê°€ë©°**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NLP ê¸°ìˆ ì˜ ë°œì „ê³¼ framework**\n",
    "\n",
    "---\n",
    "\n",
    "ìš°ë¦¬ëŠ” ë§¤ì¼ë§¤ì¼ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆëŠ” ë‹¤ì–‘í•œ Modern NLPì˜ ë…¼ë¬¸ê³¼ ëª¨ë¸ë“¤ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ì´ ëª¨ë“  ëª¨ë¸ë“¤ì„ ì§ì ‘ ì§œë³´ëŠ” ê²ƒì´ ì‹¤ë ¥ í–¥ìƒì— ë„ì›€ì´ ë˜ì§€ë§Œ, ë§¤ë²ˆ ìƒˆë¡œìš´ ë…¼ë¬¸ë“¤ì„ ì§ì ‘ êµ¬í˜„í•  ìˆ˜ëŠ” ì—†ê² ì£ . ì„¤ë ¹ êµ¬í˜„í•´ ë³¸ë‹¤ í•˜ë”ë¼ë„ ê°ê°ì˜ ëª¨ë¸ë“¤ì´ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•´ì„œëŠ” ì—„ì²­ë‚˜ê²Œ ë§ì€ ì»´í“¨íŒ… ìì›ì„ ë™ì›í•œ pre-training ì‘ì—…ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ë˜í•œ, ë…¼ë¬¸ê³¼ í•¨ê»˜ ê³µê°œëœ ë¦¬ì„œì¹˜ ì½”ë“œë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ ë‚´ê°€ ê°€ì§„ ë°ì´í„°ì…‹ê³¼ ë¬¸ì œì— ì ìš©í•  ìˆ˜ëŠ” ìˆì§€ë§Œ, ë§¤ë²ˆ í”„ë¡œì íŠ¸ë§ˆë‹¤ ë‹¤ë¥¸ ì½”ë“œ ìŠ¤íƒ€ì¼ê³¼, ë‹¤ë¥¸ framework(tensorflow, pytorch ë“±)ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë§ì€ ëª¨ë¸ë“¤ì„ ë¶„ì„í•´ì„œ êµ¬ì¡°ë¥¼ íŒŒì•…í•œ í›„ ì§ì ‘ ëŒë ¤ë³´ê³  ì´ë¥¼ ì ìš©í•˜ê¸°ë€ ì‰½ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ì´ëŸ° ê³ ë¯¼ë“¤ì„ ëª¨ë‘ í•œêº¼ë²ˆì— í•´ê²°í•´ ì¤„ ìˆ˜ ìˆëŠ” ê²Œ ë°”ë¡œ NLP Frameworkì…ë‹ˆë‹¤!!\n",
    "<br><br>\n",
    "ì†Œí”„íŠ¸ì›¨ì–´ì—ì„œ frameworkë€ í”„ë¡œì íŠ¸ì˜ ë¼ˆëŒ€ë¥¼ ì´ë£¨ëŠ” í´ë˜ìŠ¤ì™€ ì¸í„°í˜ì´ìŠ¤ì˜ ì§‘í•©ì„ ë§í•©ë‹ˆë‹¤. í•´ë‹¹ ë¶„ì•¼ì˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ë°˜ì˜í•˜ì—¬ í™•ì¥ ê°€ëŠ¥í•œ í…œí”Œë¦¿ í˜•íƒœë¡œ ì„¤ê³„ë˜ì—ˆê¸° ë•Œë¬¸ì—, frameworkë¥¼ ì´ìš©í•´ ì†ì‰½ê²Œ ë‹¤ì–‘í•œ ì‘ìš© í”„ë¡œê·¸ë¨ì„ ì œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµœê·¼ NLP ë¶„ì•¼ì—ì„  transformer ê¸°ë°˜ì˜ BERT ë“± ë‹¤ì–‘í•œ pretrained modelì´ ë°œí‘œë˜ê³ , ì´ë¥¼ í™œìš©í•œ ì „ì´í•™ìŠµ(transfer learning)ì„ í†µí•´ ë‹¤ì–‘í•œ NLP íƒœìŠ¤í¬ë¥¼ ì†ì‰½ê²Œ êµ¬í˜„í•˜ëŠ” íë¦„ì´ ë‘ë“œëŸ¬ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ° NLP ë¶„ì•¼ì˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ì–‘í•œ NLP ë¶„ì•¼ì˜ frameworkê°€ ì†ì† ë°œí‘œë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ NLP frameworkë“¤ì€ NLP ë¶„ì•¼ì˜ ìµœì‹  ë…¼ë¬¸ë“¤ì˜ ë¦¬ì„œì¹˜ ì½”ë“œë¥¼ ë¯¸ë¦¬ êµ¬í˜„í•˜ì—¬ pretrained modelì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ frameworkì˜ ì‚¬ìš©ìê°€ ì•„ì£¼ ì†ì‰½ê²Œ ì´ë¥¼ ê°€ì ¸ë‹¤ê°€ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì— ë§ê²Œ finetuningí•˜ê±°ë‚˜ í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ ì¤ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ NLP frameworkë“¤ì€ íƒœìŠ¤í¬ë‚˜ ë°ì´í„°ì…‹, ëª¨ë¸ì— ë¬´ê´€í•˜ê²Œ í†µì¼ì ì¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ëœ í´ë˜ìŠ¤ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆì–´ì„œ, ìµœì†Œí•œì˜ ì½”ë“œ êµ¬í˜„ë§Œìœ¼ë¡œë„ ë‹¤ì–‘í•œ ë³€í™”ì— ëŒ€ì‘í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ì •ë§ ë‹¤ì–‘í•œ frameworkë“¤ì´ ì¡´ì¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë©‹ì§„ NLP frameworkë“¤ì´ ìˆëŠ”ì§€ ê°„ë‹¨íˆ ì‚´í´ë³´ê² ì§€ë§Œ, ì´ë²ˆ ì‹œê°„ì— ìš°ë¦¬ëŠ” íŠ¹íˆ **Huggingface(ğŸ¤—)ì˜ transformers**ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¤ì–‘í•œ NLPì˜ ëª¨ë¸ë“¤ì„ ë‹¤ë£¨ëŠ” ë°©ë²•ì— ëŒ€í•´ ì´ì•¼ê¸° ë‚˜ëˆ ë³¼ê¹Œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **í•™ìŠµ ëª©í‘œ**\n",
    "\n",
    "---\n",
    "\n",
    "- ë‹¤ì–‘í•œ NLP Frameworkì˜ ì¢…ë¥˜ë¥¼ ì•Œê³  í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- Huggingface ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ ì˜¬ë°”ë¥¸ taskì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- Huggingface í”„ë ˆì„ì›Œí¬ì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê³ , ìƒí™©ì— ë§ê²Œ ì´ë¥¼ ë¯¸ì„¸ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **í•™ìŠµ ëª©ì°¨**\n",
    "- 2.ë‹¤ì–‘í•œ NLP Frameworkì˜ ì¶œí˜„\n",
    "    - íŠ¸ëœìŠ¤í¬ë¨¸, ì–¸ì œê¹Œì§€ êµ¬í˜„í•´ì„œ ì“¸ ê±´ê°€ìš”?\n",
    "- 3.Huggingface transformers ê°œìš”\n",
    "    - ë¬´ì—‡ì´ Huggingfaceë¥¼ íŠ¹ë³„í•˜ê²Œ ë§Œë“œë‚˜ìš”?\n",
    "- 4.Huggingface transformers (1) Model\n",
    "    - Huggingfaceì˜ ê°€ì¥ í•µì‹¬ì¸ Modelì„ ì•Œì•„ë´…ì‹œë‹¤.\n",
    "- 5.Huggingface transformers (2) Tokenizer\n",
    "    - ì •ê·œí‘œí˜„ì‹ì€ ê·¸ë§Œ! ë‚´ì¥ Tokenizerì„ ì¨ë³´ì•„ìš”.\n",
    "- 6.Huggingface transformers (4) Config\n",
    "    - ëª¨ë¸ì„ ì†ì‰½ê²Œ êµ¬ì„±í•´ë´…ì‹œë‹¤.\n",
    "- 7.Huggingface transformers (5) Trainer\n",
    "    - ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ Trainer í´ë˜ìŠ¤ë¥¼ ì•Œì•„ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-2. ë‹¤ì–‘í•œ NLP Frameworkì˜ ì¶œí˜„**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì˜¤ëŠ˜ ìš°ë¦¬ëŠ” ì£¼ë¡œ `Huggingface transformers` Frameworkë¥¼ í†µí•´ NLP framworkê°€ ì–´ë–¤ ê²ƒì¸ì§€ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³¼ ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ `Huggingface transformers`ì´ì „ì—ë„ NLP ë¶„ì•¼ì— ë„ë¦¬ ì•Œë ¤ì§„ frameworkë“¤ì´ ë§ì´ ìˆì—ˆìŠµë‹ˆë‹¤. ì§€ê¸ˆë„ ë„ë¦¬ í™œìš©ë˜ê³  ìˆëŠ” ê²ƒë“¤ ìœ„ì£¼ë¡œ ì¢…ë¥˜ë³„ë¡œ ëª‡ ê°€ì§€ë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "(ì°¸ê³ ) ì´ë²ˆ ìŠ¤í…ì˜ ë‚´ìš©ì€ [Popular NLP Libraries of 2022](https://medium.com/nlplanet/awesome-nlp-21-popular-nlp-libraries-of-2022-2e07a914248b)ì— ì •ë¦¬ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ìŒì„ ë°í™ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **General Framework for NLP**\n",
    "\n",
    "---\n",
    "\n",
    "ì—¬ê¸°ì„œ ì†Œê°œí•  frameworkë“¤ì€ NLP ë¬¸ì œë¥¼ ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆëŠ” í†µí•©ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ëª©í‘œë¡œ ì„¤ê³„ëœ ê²ƒë“¤ì…ë‹ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œëŠ” AllenNLP, Fairseq, Fast.aiê°€ ìˆìœ¼ë©°, Googleì˜ tensor2tensor í”„ë¡œì íŠ¸ë„ ê°™ì€ ë²”ì£¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ”¶ AllenNLP\n",
    "<br><br>\n",
    "- ì œê³µì : Allen AI Institute\n",
    "- Website : https://allennlp.org/\n",
    "- Github : https://github.com/allenai/allennlp\n",
    "- Backend : PyTorch\n",
    "<br><br>\n",
    "\n",
    "2018ë…„ ì´ˆë°˜ì— Contextual Word Embeddingì˜ ëŒ€í‘œì ì¸ ëª¨ë¸ì¸ ELMOë¥¼ ë°œí‘œí•˜ë©´ì„œ ìœ ëª…í•´ì§„ Allen Instituteì—ì„œ ë§Œë“  NLP frameworkì…ë‹ˆë‹¤. ë‹¹ì‹œ ELMOëŠ” [GLUE Benchmark Test](https://gluebenchmark.com/)ì™€ ê°™ì´ 10ê°€ì§€ë‚˜ ë˜ëŠ” ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ì˜ ëª¨ë¸ì„ finetuneí•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ ê¸°ì¡´ì˜ State-of-the-art ê¸°ë¡ì„ ê²½ì‹ í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³ ì í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ë¬ê¸°ì— í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ì†ì‰½ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìœ ì—°í•œ í”„ë¡œì íŠ¸ë¥¼ êµ¬ì„±í•´ì•¼ í–ˆê³ , ì´ë¥¼ í™•ì¥í•˜ë©´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ NLP frameworkë¡œ ë°œì „í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´í›„ AllenNLPëŠ” Glue datasetì˜ baseline í”„ë¡œì íŠ¸ [Starting Baseline](https://github.com/nyu-mll/GLUE-baselines)ë¥¼ ì œê³µí•˜ê¸°ë„ í–ˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "\n",
    "![ì¶œì²˜ : AllenNLP Guide(https://guide.allennlp.org/building-your-model#1)](../Images/lec_22/1.png)\n",
    "<br><br>\n",
    "íƒœìŠ¤í¬ì™€ ëª¨ë¸ì„ ë¶„ë¦¬í•´ì„œ, í•œ ê°€ì§€ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ í•˜ë‚˜ì˜ íƒœìŠ¤í¬ë¥¼ ë‹¤ì–‘í•œ ëª¨ë¸ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì„¤ê³„ëŠ” AllenNLPê°€ ì²˜ìŒ ì‹œë„í•œ ê²ƒì€ ë¬¼ë¡  ì•„ë‹™ë‹ˆë‹¤ë§Œ, ELMOì™€ ê°™ì€ pretrained modelì˜ ì„±ê³µì„ ë°”íƒ•ìœ¼ë¡œ NLP frameworkë¥¼ ì™„ì„±í•´ ë‚˜ê°€ë ¤ëŠ” AllenNLPì˜ ì‹œë„ëŠ” ì´í›„ ë§ì€ ì•„ì´ë””ì–´ë¥¼ ì œê³µí•˜ì˜€ìŠµë‹ˆë‹¤. AllenNLPëŠ” í˜„ì¬ëŠ” ELMO ì´ì™¸ì—ë„ BERT ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì˜ í™œìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ë‹¨, AllenNLPëŠ” PyTorch ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìœ¼ë©° ëª¨ë¸ì´ `torch.nn.Module`ì„ ìƒì†ë°›ëŠ” êµ¬ì¡°ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. Tensorflowë‚˜ Keras ê¸°ë°˜ìœ¼ë¡œ AllenNLPë¥¼ í™œìš©í•˜ëŠ” ê²ƒì€ ì–´ë µìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ”¶ Fairseq\n",
    "<br><br>\n",
    "- ì œê³µì : Facebook AI Research\n",
    "- Website : https://fairseq.readthedocs.io/en/latest\n",
    "- Github : https://github.com/pytorch/fairseq\n",
    "- Backend : PyTorch\n",
    "<br><br>\n",
    "\n",
    "FairseqëŠ” ê¾¸ì¤€íˆ NLP ì—°êµ¬ì„±ê³¼ë¥¼ ë‚´ê³  ìˆëŠ” Facebook AI Researchì˜ NLP Frameworkì…ë‹ˆë‹¤. ë¹„ë‹¨ ìì—°ì–´ì²˜ë¦¬ì—ë§Œ êµ­í•œëœ ê²ƒì´ ì•„ë‹ˆë¼ ì´ë¦„ì—ì„œë„ ì•Œ ìˆ˜ ìˆë“¯ì´ CNN, LSTM ë“± ì „í†µì ì¸ ëª¨ë¸ë¡œë¶€í„°, ìŒì„±ì¸ì‹/í•©ì„± ë“± sequentialí•œ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ë¶„ì•¼ë¥¼ ë‘ë£¨ ë‹¤ë£¨ëŠ” ë‹¤ì–‘í•œ pretrained modelì„ í•¨ê»˜ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì—­ì‹œ Facebookì˜ frameworkë‹µê²Œ PyTorch ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ”¶ Fast.ai\n",
    "<br><br>\n",
    "- ì œê³µì : fast.ai\n",
    "- Website : http://docs.fast.ai/\n",
    "- Github : https://github.com/fastai/fastai\n",
    "- Backend : PyTorch\n",
    "<br><br>\n",
    "\n",
    "fast.aiëŠ” ì´ë¦„ì— ê±¸ë§ê²Œ, ë¹ ë¥´ê²Œ ë°°ìš°ê³  ì‰½ê²Œ ëª¨ë¸ì„ êµ¬ì„±í•  ìˆ˜ ìˆë„ë¡ í•˜ì´ë ˆë²¨ APIì™€ Application ë¸”ë¡ê¹Œì§€ ì†ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¹„ë‹¨ NLP ë¶„ì•¼ ë¿ ì•„ë‹ˆë¼ ë‹¤ì–‘í•œ ë¶„ì•¼ë¡œ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì—­ì‹œ PyTorch ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "\n",
    "![ì¶œì²˜ : fast.ai (https://github.com/fastai/fastai)](../Images/lec_22/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ”¶ tensor2tensor\n",
    "<br><br>\n",
    "- ì œê³µì : Google Brain\n",
    "- Github : https://github.com/tensorflow/tensor2tensor (deprecated)\n",
    "- New Github : https://github.com/google/trax\n",
    "- Backend : Tensorflow\n",
    "<br><br>\n",
    "\n",
    "Google Brainì—ì„œ 2017ë…„ì— transformer ë…¼ë¬¸ì„ ë°œí‘œí•˜ë©´ì„œ ê·¸ êµ¬í˜„ì²´ë¡œ í•¨ê»˜ ê³µìœ í–ˆë˜ í”„ë¡œì íŠ¸ê°€ ë°”ë¡œ tensor2tensorì˜€ìŠµë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ ì—­ì‹œ 'Attention is all you need'ë¼ëŠ” ë…¼ë¬¸ì˜ ì œëª©ì²˜ëŸ¼, transformerë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì™€ ë‹¤ì–‘í•œ ëª¨ë¸ì„ í•˜ë‚˜ì˜ frameworkì— í†µí•©í•˜ë ¤ëŠ” ì‹œë„ë¥¼ í•˜ì˜€ìŠµë‹ˆë‹¤. ì´í›„ 2019ë…„ë¶€í„° Googleì€ Tensorflow V2 ê¸°ë°˜ìœ¼ë¡œ pretrained modelì˜ ì§€ì›ì„ ê°•í™”í•œ traxë¼ëŠ” í”„ë¡œì íŠ¸ë¥¼ ìƒì„±í•˜ë©´ì„œ, 2020ë…„ë„ë¶€í„°ëŠ” tensor2tensorì˜ ê°œë°œì„ ì¤‘ë‹¨í•˜ê³  ê´€ë ¨ ê¸°ëŠ¥ì„ traxë¡œ í†µí•©ì´ê´€í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "Tensorflow ê¸°ë°˜ì˜ NLP frameworkì´ ìƒëŒ€ì ìœ¼ë¡œ ë“œë¬¸ ê°€ìš´ë°, Tensorflow ê¸°ë°˜ì˜ NLP ì—°êµ¬ê°œë°œì„ ì§„í–‰í•œë‹¤ë©´ ì£¼ëª©í•´ ë³¼ ë§Œí•  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preprocessing Libraries**\n",
    "\n",
    "---\n",
    "\n",
    "ì•„ë˜ëŠ” ì „í†µì ìœ¼ë¡œ ì‚¬ìš©ë˜ì—ˆë˜ NLP ë¶„ì•¼ì˜ ì „ì²˜ë¦¬ ê´€ë ¨ frameworkë“¤ì…ë‹ˆë‹¤. ìœ„ì—ì„œ ì†Œê°œí•œ frameworkë“¤ì²˜ëŸ¼ ì „ì²˜ë¦¬-ëª¨ë¸ë§-íƒœìŠ¤í¬ í›ˆë ¨/í‰ê°€ë¥¼ í†µí•©ì ìœ¼ë¡œ ì„¤ê³„í•˜ì—¬ NLP íƒœìŠ¤í¬ë¥¼ ì œë„ˆëŸ´í•˜ê²Œ ìˆ˜í–‰í•˜ê²Œ ì„¤ê³„í•œ ê²ƒì´ ì•„ë‹ˆë¼ tokenization, tagging, parsing ë“± íŠ¹ì • ì „ì²˜ë¦¬ ì‘ì—…ì„ ìœ„í•´ ì„¤ê³„ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ê°€ê¹ìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” ì•„ë˜ ì˜ˆì‹œë¡œ ë“  Spacy, NLTK, TorchText ë“±ì´ ìˆìŠµë‹ˆë‹¤. í•œêµ­ì–´ì¸ ê²½ìš° KoNLPy ë¼ì´ë¸ŒëŸ¬ë¦¬ë„ ë™ì¼í•œ ì—­í• ì„ í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ”¶ Spacy\n",
    "<br><br>\n",
    "- Website : https://spacy.io/\n",
    "- Github : https://github.com/explosion/spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ”¶ NLTK\n",
    "<br><br>\n",
    "- Website : https://www.nltk.org/\n",
    "- Github : https://github.com/nltk/nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ”¶ TorchText\n",
    "<br><br>\n",
    "- Website : https://torchtext.readthedocs.io/en/latest/\n",
    "- Github : https://github.com/pytorch/text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transformer-based Framework**\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n",
    "ğŸ”¶ Huggingface transformers\n",
    "<br><br>\n",
    "- ì œê³µì : Huggingface.co\n",
    "- Website : https://huggingface.co/transformers/\n",
    "- Github : https://github.com/huggingface/transformers\n",
    "- Backend : PyTorch and Tensorflow\n",
    "<br><br>\n",
    "\n",
    "ì—¬ê¸°ì—ëŠ” í˜„ì¬ ê°€ì¥ ì£¼ëª©ë°›ê³  ìˆëŠ” NLP Frameworkì¸ `Huggingface transformers`ê°€ ìˆìŠµë‹ˆë‹¤. ì‚¬ì‹¤ Huggingfaceì˜ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ìµœê·¼ ëª¨ìŠµì€ ì´ë¯¸ ì•„ì£¼ generalí•œ NLP frameworkì˜ ëª¨ìŠµì„ ì¶©ë¶„íˆ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ˆê¸°ì—ëŠ” BERT ë“± ë‹¤ì–‘í•œ transformer ê¸°ë°˜ì˜ pretrained modelì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ PyTorch ê¸°ë°˜ì˜ wrapper í˜•íƒœë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì „í†µì ì¸ ëª¨ë¸ê¹Œì§€ í¬ê´„í•˜ë ¤ê³  í–ˆë˜ ì´ì „ì˜ general NLP Framework ë“¤ì— ë¹„í•´, Huggingfaceì˜ transformersëŠ” pretrained model í™œìš©ì„ ì£¼ë¡œ ì§€ì›í•˜ë©°, tokenizer ë“± ì „ì²˜ë¦¬ ë¶€ë¶„ë„ pretrained modelë“¤ì´ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” Subword tokenizer ê¸°ë²•ì— ì§‘ì¤‘ë˜ì–´ ìˆëŠ” íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´í›„ ë‹¤ìŒ ìŠ¤í…ë¶€í„°ëŠ” Huggingfaceì˜ transformersì— ëŒ€í•´ ì§‘ì¤‘ì ìœ¼ë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-3. Huggingface transformers ê°œìš”**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Huggingface?**\n",
    "\n",
    "---\n",
    "\n",
    "**(1) ê´‘ë²”ìœ„í•˜ê³  ì‹ ì†í•œ NLP ëª¨ë¸ ì§€ì›**\n",
    "HuggingfaceëŠ” ë§ì€ ì‚¬ëŒë“¤ì´ ìµœì‹  NLP ëª¨ë¸ë“¤ì„ ë”ìš± ì†ì‰½ê²Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ ë§Œë“¤ê¸° ì‹œì‘í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ê·¸ëŸ°ì§€ ìƒˆë¡œìš´ ë…¼ë¬¸ë“¤ì´ ë°œí‘œë  ë•Œë§ˆë‹¤, ë³¸ì¸ë“¤ì˜ frameworkì— í¡ìˆ˜ì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ, pretrained modelì„ ì œê³µí•˜ê³ , datasetê³¼ tokenizerë¥¼ ë”ìš± ì‰½ê²Œ ì´ìš©í•  ìˆ˜ ìˆë„ë¡ frameworkí™”ì‹œí‚¤ê³  ìˆëŠ” í–‰ë³´ë„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ frameworkë“¤ë„ ì´ëŸ° ì‘ì—…ì„ í•˜ì§€ ì•ŠëŠ” ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, Huggingfaceì˜ ì§€ì› ë²”ìœ„ê°€ ê°€ì¥ ê´‘ë²”ìœ„í•˜ê³ , ìµœì‹  ë…¼ë¬¸ì„ ì§€ì›í•˜ëŠ” ì†ë„ë„ ë¹ ë¦…ë‹ˆë‹¤.\n",
    "<br><br>\n",
    "**(2) PyTorchì™€ Tensorflow ëª¨ë‘ì—ì„œ ì‚¬ìš© ê°€ëŠ¥**\n",
    "transformersëŠ” ê¸°ë³¸ì ìœ¼ë¡œ PyTorchë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì ¸ìˆìŠµë‹ˆë‹¤. ë§ì€ utilityê°€ PyTorch ìœ„ì£¼ë¡œ ì‘ì„±ì´ ë˜ì–´ìˆê¸´ í•˜ì§€ë§Œ, ìµœê·¼ì—ëŠ” Tensorflowë¡œë„ í•™ìŠµí•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆê²Œë” ê³„ì†í•´ì„œ frameworkë¥¼ í™•ì¥í•˜ê³  ìˆëŠ” ì¤‘ì…ë‹ˆë‹¤. ì´ë ‡ë“¯ Huggingface transformersë¥¼ ë°”íƒ•ìœ¼ë¡œ Tensorflowì™€ PyTorchë¼ëŠ” Backendì˜ í•œê³„ë¥¼ ë›°ì–´ë„˜ì–´ ì–´ë–¤ í™˜ê²½ì—ë“  ì‰½ê²Œ ì ìš© ê°€ëŠ¥í•œ í‘œì¤€ frameworkì˜ ì§€ìœ„ë¥¼ ë‹¤ì ¸ê°€ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "**(3) ì˜ ì„¤ê³„ëœ framework êµ¬ì¡°**\n",
    "HuggingFaceì˜ ëª©í‘œì²˜ëŸ¼ ì´ frameworkëŠ” ì‰½ê³  ë¹ ë¥´ê²Œ ì–´ë– í•œ í™˜ê²½ì—ì„œë„ NLPëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ëŠì„ì—†ì´ ë³€í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ë˜í•œ ì‚¬ìš©í•˜ê¸° ì‰½ê³  ì§ê´€ì ì¼ë¿ë”ëŸ¬ ëª¨ë¸ì´ë‚˜ íƒœìŠ¤í¬, ë°ì´í„°ì…‹ì´ ë‹¬ë¼ì§€ë”ë¼ë„ ë™ì¼í•œ í˜•íƒœë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì˜ ì¶”ìƒí™”ë˜ê³  ëª¨ë“ˆí™”ëœ API ì„¤ê³„ê°€ ìˆê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•œ ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ì‹œì‘í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "ì—­ì‹œ ê°€ì¥ ë¨¼ì € í•´ì•¼ í•  ê²ƒì€ í”„ë ˆì„ì›Œí¬ë¥¼ ì„¤ì¹˜í•˜ëŠ” ê²ƒì´ì£ ! tensorflowë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ ì²˜ìŒ ì„¤ì¹˜í–ˆë˜ ê²ƒì²˜ëŸ¼, huggingfaceì˜ Transformersë„ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. (í´ë¼ìš°ë“œ ì‚¬ìš©ìë¶„ë“¤ì€ ì´ë¯¸ ì„¤ì¹˜ê°€ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ëˆˆìœ¼ë¡œ í™•ì¸ë§Œ í•˜ê³  ê°€ì‹œë©´ ë©ë‹ˆë‹¤.)\n",
    "```shell\n",
    "$ pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9978194236755371}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', framework='tf')\n",
    "classifier('We are very happy to include pipeline into the transformers repository.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.8451395034790039}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = 'í”¼ê³¤í–ˆì§€ë§Œ ë¿Œë“¯í•œ ë‚˜ë‚ ì´ì—ˆë‹¤.'\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', framework='tf')\n",
    "classifier(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Huggingface transformers ì„¤ê³„êµ¬ì¡° ê°œìš”**\n",
    "\n",
    "---\n",
    "\n",
    "frameworkë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” frameworkê°€ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ ê·¸ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ê² ì£ ??\n",
    "<br><br>\n",
    "NLP frameworkê°€ NLPëª¨ë¸ì„ í†µí•´ ì–´ë– í•œ ë¬¸ì œë¥¼ í‘¸ëŠ” ê³¼ì •ì´ ì–´ë–»ê²Œ ì§„í–‰ë ì§€ ìƒê°í•´ ë´…ì‹œë‹¤.\n",
    "<br><br>\n",
    "ë¨¼ì €, 1) Taskë¥¼ ì •ì˜í•˜ê³  ê·¸ì— ë§ê²Œ datasetì„ ê°€ê³µì‹œí‚µë‹ˆë‹¤. ê·¸ ì´í›„ 2) ì ë‹¹í•œ modelì„ ì„ íƒí•˜ê³  ì´ë¥¼ ë§Œë“­ë‹ˆë‹¤. 3)modelì— ë°ì´í„°ë“¤ì„ íƒœì›Œì„œ í•™ìŠµì„ ì‹œí‚¤ê³ , ì´ë¥¼ í†µí•´ ë‚˜ì˜¨ 4)weightì™€ ì„¤ì •(config)ë“¤ì„ ì €ì¥í•©ë‹ˆë‹¤. ì €ì¥í•œ modelì˜ checkpointëŠ” 5)ë°°í¬í•˜ê±°ë‚˜, evaluationì„ í•  ë•Œ ì‚¬ìš©í•˜ê³ ëŠ” í•˜ì£ .\n",
    "<br><br>\n",
    "transformersëŠ” ìœ„ì™€ ê°™ì€ íë¦„ì— ë§ì¶”ì–´ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "taskë¥¼ ì •ì˜í•˜ê³  datasetì„ ì•Œë§ê²Œ ê°€ê³µí•˜ëŠ” `Processors`, í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” `Tokenizer`, ë‹¤ì–‘í•œ modelì„ ì •ì˜í•œ `Model`, optimizerì™€ í•™ìŠµ schedule(warm up ë“±)ì„ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” `Optimization`, í•™ìŠµ ê³¼ì •ì„ ì „ë°˜ì„ ê´€ë¦¬í•˜ëŠ” `Trainer`, weightì™€ tokenizer, modelì„ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë„ë¡ ê°ì¢… ì„¤ì •ì„ ì €ì¥í•˜ëŠ” `Config` ë“±ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ìš°ë¦¬ëŠ” ë‹¤ìŒ ìŠ¤í…ë¶€í„°, Huggingfaceì˜ ê° ë¶€ë¶„ì„ ì´ë£¨ê³  ìˆëŠ” í´ë˜ìŠ¤ êµ¬ì¡°ì— ëŒ€í•´ ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-4. Huggingface transformers (1) Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformersì˜ ê°€ì¥ í•µì‹¬ì ì¸ ë¶€ë¶„ì€ ì•„ë¬´ë˜ë„ ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë¸ë“¤ì€ `PretrainedModel` í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ê³  ìˆìŠµë‹ˆë‹¤. `PretrainedModel` í´ë˜ìŠ¤ëŠ” í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³ , ë‹¤ìš´ë¡œë“œí•˜ê³ , ì €ì¥í•˜ëŠ” ë“± ëª¨ë¸ ì „ë°˜ì— ê±¸ì³ ì ìš©ë˜ëŠ” ë©”ì†Œë“œë“¤ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ìƒì† êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì—, ì‹¤ì œë¡œ ì‚¬ìš©í•  ëª¨ë¸ì´ BERTì´ê±´, GPTì´ê±´ ìƒê´€ì—†ì´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³  ë‹¤ìš´ë¡œë“œ/ì €ì¥í•˜ëŠ” ë“±ì˜ ì‘ì—…ì— í™œìš©í•˜ëŠ” ë©”ì†Œë“œëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ê²ƒì„ ë™ì¼í•˜ê²Œ í™œìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ëª¨ë¸ì€ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ì²« ë²ˆì§¸ë¡œëŠ” taskì— ì í•©í•œ ëª¨ë¸ì„ ì§ì ‘ ì„ íƒí•˜ì—¬ importí•˜ê³ , ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ì‹ì´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë¡œë“œí•  ë•ŒëŠ” `from_pretrained`ë¼ëŠ” ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ë©°, Huggingfaceì˜ pretrained ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„, ì§ì ‘ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "Huggingfaceì—ì„œ ì œê³µí•˜ëŠ” pretrained ëª¨ë¸ì´ë¼ë©´ ëª¨ë¸ì˜ ì´ë¦„ì„ stringìœ¼ë¡œ, ì§ì ‘ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì´ë¼ë©´ configì™€ ëª¨ë¸ì„ ì €ì¥í•œ ê²½ë¡œë¥¼ stringìœ¼ë¡œ ë„˜ê²¨ì£¼ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_tf_bert.TFBertForPreTraining'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForPreTraining\n",
    "model = TFBertForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‘ ë²ˆì§¸ ë°©ë²•ì€, AutoModelì„ ì´ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ëª¨ë¸ì— ê´€í•œ ì •ë³´ë¥¼ ì²˜ìŒë¶€í„° ëª…ì‹œí•˜ì§€ ì•Šì•„ë„ ë˜ì–´ ì¡°ê¸ˆ ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_tf_bert.TFBertModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "model = TFAutoModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°©ê¸ˆ ìœ„ì—ì„œ `bert-base-cased`ë¼ê³  ì–¸ê¸‰ëœ ë¶€ë¶„ì´ ë³´ì´ì‹œë‚˜ìš”? ì´ê²ƒì€ **Model ID**ì…ë‹ˆë‹¤. Huggingfaceê°€ ì§€ì›í•˜ëŠ” ë‹¤ì–‘í•œ pretrained modelì´ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ ì¤‘ ì–´ëŠ ê²ƒì„ ì„ íƒí• ì§€ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ ì´ IDë¥¼ í™œìš©í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì–´ë–¤ ëª¨ë¸ì´ ì§€ì›ë˜ëŠ”ì§€ ì•„ë˜ ë§í¬ì—ì„œ í™•ì¸í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "- [Pretrained models](https://huggingface.co/transformers/v4.11.3/pretrained_models.html)\n",
    "<br><br>\n",
    "\n",
    "ê·¸ëŸ°ë°, ìœ„ì—ì„œ ì†Œê°œí•œ ë‘ ê°€ì§€ ë°©ë²•ì˜ ì°¨ì´ê°€ íŒŒì•…ë˜ì‹œë‚˜ìš”? ë¶ˆëŸ¬ì˜¤ê³ ì í•˜ëŠ” ëª¨ë¸ì˜ IDëŠ” `bert-base-cased`ë¡œì„œ ë™ì¼í•©ë‹ˆë‹¤. ì‚¬ìš©ë²•ë„ ê±°ì˜ ë™ì¼í•œë°ìš”, ê²°ê³¼ì ìœ¼ë¡œ `model.__class__`ë¥¼ í™•ì¸í•´ ë³´ë©´ ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘˜ ë‹¤ ë™ì¼í•œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ Pretrain, Downstream Task ë“± ìš©ë„ì— ë”°ë¼ ëª¨ë¸ì˜ Inputì´ë‚˜ Output shapeê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. AutoModelì„ í™œìš©í•œë‹¤ë©´ ëª¨ë¸ì˜ ìƒì„¸ì •ë³´ë¥¼ í™•ì¸í•  í•„ìš” ì—†ì´ Model IDë§Œìœ¼ë¡œë„ ì†ì‰½ê²Œ ëª¨ë¸ êµ¬ì„±ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ì •í™•í•œ ìš©ë„ì— ë§ê²Œ ì‚¬ìš©í•˜ë ¤ë©´ ëª¨ë¸ë³„ ìƒì„¸ ì•ˆë‚´ í˜ì´ì§€ë¥¼ ì°¸ê³ í•´ì„œ ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ `BERT`ì˜ ìƒì„¸ í˜ì´ì§€ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "- [MODELS - BERT](https://huggingface.co/docs/transformers/model_doc/bert)\n",
    "<br><br>\n",
    "\n",
    "ëª¨ë¸ë§ˆë‹¤ ê·¸ êµ¬ì¡°ëŠ” ë‹¤ë¥´ì§€ë§Œ ëŒ€ë¶€ë¶„ í•´ë‹¹ ëª¨ë¸ ì´ë¦„ì„ ê°€ì§„ í´ë˜ìŠ¤(eg. TFBertModel)ê³¼ MainLayer class(eg. TFBertMainLayer)ì™€ Attention Class, Embedding Class ë“±ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹µë‹ˆë‹¤. ì¶”í›„ì— ëª¨ë¸ì´ ì–´ë–»ê²Œ ì§œì—¬ì¡ŒëŠ”ì§€ ë³´ì‹¤ ë•Œ `__init__()` ë©”ì†Œë“œ ì•ˆì— êµ¬ì„±ëœ ë¼ˆëŒ€ë¥¼ ë¨¼ì € ì‚´í´ë³´ë„ë¡ í•˜ì„¸ìš” :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-5. Huggingface transformers (2) Tokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‚´ê°€ ê°€ì§€ê³  ìˆëŠ” ë¬¸ì œë¥¼ í’€ ëª¨ë¸ì„ ì •í–ˆë‹¤ë©´, ì´ì œ ëª¨ë¸ì— ë„£ì„ inputì„ ë§Œë“¤ì–´ ì¤„ ì°¨ë¡€ì…ë‹ˆë‹¤.\n",
    "<br><br>\n",
    "transformersëŠ” ë‹¤ì–‘í•œ tokenizerë¥¼ ê° ëª¨ë¸ì— ë§ì¶”ì–´ ì´ë¯¸ êµ¬ë¹„í•´ë‘ì—ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ê°€ í•  ì¼ì€ tokenizerë¥¼ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•˜ëŠ” ê²ƒë¿ì´ëë‹ˆë‹¤. ì‚¬ìš©í•˜ê¸°ì— ì•ì„œì„œ, ë‚´ê°€ ì„ íƒí•œ modelì´ ì–´ë– í•œ tokenizerë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ ì •ë„ ë¯¸ë¦¬ ì²´í¬í•´ë‘ëŠ” ì„¼ìŠ¤ëŠ” ëª¨ë‘ ì±™ê²¨ë‘ì…¨ê² ì£ ?:)\n",
    "<br><br>\n",
    "Pretrained model ê¸°ë°˜ì˜ NLP frameworkë¥¼ ì‚¬ìš©í•  ë•Œ ê°€ì¥ ì¤‘ìš”í•œ ë‘ ê°€ì§€ í´ë˜ìŠ¤ëŠ” Modelê³¼ Tokenizerë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ë‘ ê°€ì§€ëŠ” ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„° êµ¬ì¡°ê°€ ë™ì¼í•œ Modelì´ë¼ í•˜ë”ë¼ë„ Tokenizerê°€ ë‹¤ë¥´ê±°ë‚˜ Tokenizer ë‚´ì˜ Dictionaryê°€ ë‹¬ë¼ì§€ë©´ ì‚¬ì‹¤ìƒ ì™„ì „íˆ ë‹¤ë¥¸ ëª¨ë¸ì´ ë©ë‹ˆë‹¤. ê·¸ë¦¬ê³  TokenizerëŠ” ì–´ë–¤ ì–¸ì–´ë¥¼ ë‹¤ë£¨ëŠëƒ í•˜ëŠ” ì½”í¼ìŠ¤ ë°ì´í„°ì…‹ì— ë”°ë¼ì„œë„ ë‹¬ë¼ì§‘ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì „ ìŠ¤í…ì—ì„œ ì†Œê°œí–ˆë˜ Huggingfaceê°€ ì œê³µí•˜ëŠ” ëª¨ë¸ ì¢…ë¥˜ ì¤‘ ëª‡ ê°œë§Œ ì˜ˆë¡œ ë“¤ì–´ë³¼ê¹Œìš”?\n",
    "<br><br>\n",
    "- **bert-base-uncased** : BERT ëª¨ë¸ì¸ë°, 108MB íŒŒë¼ë¯¸í„°ì˜ ê¸°ë³¸ ëª¨ë¸ì´ë©´ì„œ, ì½”í¼ìŠ¤ëŠ” ì˜ë¬¸ ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠìŒ(ì „ì²´ ì†Œë¬¸ìí™”)\n",
    "- **bert-large-cased** : BERT ëª¨ë¸ì¸ë°, 340MB íŒŒë¼ë¯¸í„°ì˜ ëŒ€í˜• ëª¨ë¸ì´ë©´ì„œ, ì½”í¼ìŠ¤ëŠ” ì˜ë¬¸ ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„\n",
    "- **bert-base-multilingual-cased** : BERT ëª¨ë¸ì¸ë°, 108MB íŒŒë¼ë¯¸í„°ì˜ ê¸°ë³¸ ëª¨ë¸ì´ë©´ì„œ, ì½”í¼ìŠ¤ëŠ” ë‹¤êµ­ì–´ ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„\n",
    "<br><br>\n",
    "\n",
    "tokenizer ë˜í•œ ì§ì ‘ ëª…ì‹œí•˜ì—¬ ë‚´ê°€ ì‚¬ìš©í•  ê²ƒì„ ì§€ì •í•´ ì£¼ê±°ë‚˜, AutoTokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ êµ¬ë¹„ëœ modelì— ì•Œë§ì€ tokenizerë¥¼ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "ì´ë•Œ ìœ ì˜í•  ì ì€, **modelì„ ì‚¬ìš©í•  ë•Œ ëª…ì‹œí–ˆë˜ ê²ƒê³¼ ë™ì¼í•œ IDë¡œ tokenizerë¥¼ ìƒì„±**í•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¶ˆëŸ¬ì˜¨ tokenizerë¥¼ í•œ ë²ˆ ì‚¬ìš©í•´ë³¼ê¹Œìš”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1188, 1110, 5960, 1111, 170, 11093, 1883, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(\"This is Test for aiffel\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ê²½ìš°ëŠ” **BERTì˜ tokenizerì´ê¸° ë•Œë¬¸ì—** ì¸ì½”ë”©ì´ ëœ `input_ids` ë¿ë§Œ ì•„ë‹ˆë¼, `token_type_ids`ì™€ `attention_mask`ê¹Œì§€ ëª¨ë‘ ìƒì„±ëœ input ê°ì²´ë¥¼ ë°›ì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "tokenizerì˜ `tokenize()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ í† í° ë‹¨ìœ„ë¡œ ë¶„í• ëœ ë¬¸ì¥ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œ ë²ˆ í™•ì¸í•´ë³¼ê¹Œìš”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í°í™”ëœ ë¬¸ì¥: {'input_ids': tensor([[  101,  1188,  1110,  5960,  1111,   170, 11093,  1883,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Q. ìœ„ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ \"This is Test for aiffel\" ë¬¸ì¥ì„ ë‚˜ëˆ„ì–´ ì£¼ì„¸ìš”.\n",
    "# ì…ë ¥ ë¬¸ì¥\n",
    "text = \"This is Test for aiffel\"\n",
    "\n",
    "# ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë³€í™˜\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# í† í° ì¶œë ¥\n",
    "print(\"í† í°í™”ëœ ë¬¸ì¥:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizerëŠ” batch ë‹¨ìœ„ë¡œ inputì„ ë°›ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102], [101, 1262, 1330, 5650, 102], [101, 1262, 1103, 1304, 1304, 1314, 1141, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                    \"And another sentence\",\n",
    "                    \"And the very very last one\"]\n",
    "\n",
    "encoded_batch = tokenizer(batch_sentences)\n",
    "print(encoded_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ë°–ì—ë„ tokenizeí•  ë•Œì— `padding`, `truncation` ë“± ë‹¤ì–‘í•œ ì˜µì…˜ì„ ì„¤ì •í•  ìˆ˜ ìˆìœ¼ë©°, ëª¨ë¸ì´ ì–´ë–¤ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€(Tensorflow ë˜ëŠ” PyTorch)ì— ë”°ë¼ input íƒ€ì…ì„ ë³€ê²½ ì‹œì¼œì£¼ëŠ” `return_tensors` ì¸ìë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[ 101, 8667,  146,  112,  182,  170, 1423, 5650,  102],\n",
      "       [ 101, 1262, 1330, 5650,  102,    0,    0,    0,    0],\n",
      "       [ 101, 1262, 1103, 1304, 1304, 1314, 1141,  102,    0]],\n",
      "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-6. Huggingfacae transformers (3) Config**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ìš”ì†Œë“¤ì„ ëª…ì‹œí•œ `json`íŒŒì¼ë¡œ ë˜ì–´ìˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ì´ jsoníŒŒì¼ì—ëŠ” batch size, learning rate, weight_decayë“± trainì— í•„ìš”í•œ ìš”ì†Œë“¤ë¶€í„° tokenizerì— íŠ¹ìˆ˜ í† í°(special token eg.`[MASK]`)ë“¤ì„ ë¯¸ë¦¬ ì„¤ì •í•˜ëŠ” ë“± ì„¤ì •ì— ê´€í•œ ì „ë°˜ì ì¸ ê²ƒë“¤ì´ ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "PretrainedModelì„ save_pretrained ë©”ì†Œë“œë¥¼ ì´ìš©í•˜ë©´ ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ì™€ í•¨ê»˜ ì €ì¥ë˜ë„ë¡ ë˜ì–´ìˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "hugging faceì˜ pretrained modelì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ìë™ìœ¼ë¡œ configíŒŒì¼ì´ ë¡œë“œë˜ì–´ ëª…ì‹œí•  í•„ìš”ê°€ ì—†ì§€ë§Œ, ì„¤ì •ì„ ë³€ê²½í•˜ê³  ì‹¶ê±°ë‚˜ ë‚˜ë§Œì˜ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë•Œì—ëŠ” configíŒŒì¼ì„ ì§ì ‘ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤.\n",
    "<br><br>\n",
    "config ë˜í•œ model, tokenizerì²˜ëŸ¼ Model IDë§Œ ìˆìœ¼ë©´, Config í´ë˜ìŠ¤ë¥¼ ëª…í™•íˆ ì§€ì •í•˜ê±°ë‚˜ í˜¹ì€ `AutoConfig`ë¥¼ ì´ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘ ë°©ì‹ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•´ ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‘ ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¨ configì˜ ë‚´ìš©ì— ë³„ë‹¤ë¥¸ ì°¨ì´ê°€ ì—†ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ ëª¨ë¸ì„ ì´ë¯¸ ìƒì„±í–ˆë‹¤ë©´ `model.config`ìœ¼ë¡œ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-cased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.11.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFBertForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "# Q. ìƒì„±ëœ ëª¨ë¸ì—ì„œ configë¥¼ ê°€ì ¸ì™€ë´…ì‹œë‹¤\n",
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-7. Huggingface transformers (4) Trainer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainerëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. training, fine-tuning, evaluation ëª¨ë‘ trainer classë¥¼ ì´ìš©í•˜ì—¬ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "trainerì„ ì‚¬ìš©í•  ê²½ìš°, `TrainingArguments` ë¥¼ í†µí•´ Huggingface í”„ë ˆì„ì›Œí¬ì—ì„œ ì œê³µí•˜ëŠ” ê¸°ëŠ¥ë“¤ì„ í†µí•©ì ìœ¼ë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ì—¬ ëª¨ë¸ì„ ì†ì‰½ê²Œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "trainer APIë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„  `TrainingArguments`ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•´ì•¼ í•˜ë©°, ì´ ë•Œ í•™ìŠµì— í•„ìš”í•œ ì—¬ëŸ¬ argumentsë“¤ì´ ì •ì˜ë©ë‹ˆë‹¤. ì•„ë˜ ì˜ˆì‹œë¥¼ í†µí•´ ì´ë²ˆ ë…¸ë“œì—ì„œ ì‚´í´ë³¸ Model, Tokenizer ë° ë°ì´í„°ì…‹ êµ¬ì„±ì´ TFTrainingArgumentsë¥¼ í†µí•´ì„œ TFTrainerì— ì–´ë–»ê²Œ ë°˜ì˜ë˜ëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/aiffel/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616c793f662749bc8263725b434a526a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 8551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"cola\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë²ˆì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ì€ GLUE benchmark ì¤‘ í•˜ë‚˜ì¸ COLA datasetì…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ë’¤ì´ì–´ ë‚˜ì˜¬ ë…¸ë“œì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ìš°ì„  ë°ì´í„°ì…‹ì˜ êµ¬ì„±ì€ `sentence`, `label`, `idx`ì„¸ í•­ëª©ìœ¼ë¡œ ë‚˜ë‰˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¬ ëª¨ë¸ê³¼ ì´ë“¤ì„ í† í°í™”í•˜ê¸° ìœ„í•œ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , í† í°í™” í•¨ìˆ˜ë„ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b18959ac3246be87e8517569751466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc04e2a47e241939c0665fe2e38f538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b43176f99f41ee9a61de0a115bd99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, num_labels=2)    # COLA datasetì˜ ë¼ë²¨ì€ 0(unacceptable)ê³¼ 1(accpetable) ë‘ ê°€ì§€ë¡œ êµ¬ë¶„ë¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í† í°í™”ê°€ ì™„ë£Œë˜ì—ˆë‹¤ë©´, trainerì— í•„ìš”í•œ `TrainingArguments`ë¥¼ ì„ ì–¸í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "`TrainingArguments`ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¸ìë“¤ì´ í¬í•¨ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',              # outputì´ ì €ì¥ë  ê²½ë¡œ\n",
    "    num_train_epochs=1,              # train ì‹œí‚¬ ì´ epochs\n",
    "    per_device_train_batch_size=16,  # ê° device ë‹¹ batch size\n",
    "    per_device_eval_batch_size=64,   # evaluation ì‹œì— batch size\n",
    "    warmup_steps=500,                # learning rate schedulerì— ë”°ë¥¸ warmup_step ì„¤ì •\n",
    "    weight_decay=0.01,                 # weight decay\n",
    "    logging_dir='./logs',                 # logê°€ ì €ì¥ë  ê²½ë¡œ\n",
    "    do_train=True,                        # train ìˆ˜í–‰ì—¬ë¶€\n",
    "    do_eval=True,                        # eval ìˆ˜í–‰ì—¬ë¶€\n",
    "    eval_steps=1000,\n",
    "    group_by_length=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TFTrainingArguments`ì˜ ì¸ì ì¤‘ `group_by_length`ì€ í•™ìŠµ ê³¼ì •ì„ íš¨ìœ¨í™”í•˜ëŠ” ê¸°ëŠ¥ì´ ìˆë‹¤.\n",
    ">  group_by_length (bool, optional, defaults to False) â€” Whether or not to group together samples of roughly the same length in the training dataset (to minimize padding applied and be more efficient). Only useful if applying dynamic padding.\n",
    ">> ì´ ì¸ìëŠ” í•™ìŠµ ë°ì´í„°ì—ì„œ í¬ê¸°ê°€ ë¹„ìŠ·í•œ ê²ƒë“¤ì„ ë¬¶ì–´ ì‚¬ìš©í•˜ëŠ” ê¸°ëŠ¥ì´ ìˆë‹¤.\n",
    ">>> ë™ì  íŒ¨ë”©ì„ ì‚¬ìš©í•  ê²½ìš°, ë°ì´í„°ì— ë¶™ëŠ” íŒ¨ë”©ì„ ìµœì†Œí™”í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì, ì´ì œ ìœ„ì—ì„œ ì™„ì„±ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ í›ˆë ¨ê³¼ ì¶”ë¡ ì„ ì§„í–‰í•´ ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx.\n",
      "***** Running training *****\n",
      "  Num examples = 8551\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 535\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='535' max='535' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [535/535 01:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.520700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=535, training_loss=0.5134526885558511, metrics={'train_runtime': 60.6872, 'train_samples_per_second': 140.903, 'train_steps_per_second': 8.816, 'total_flos': 91292341767000.0, 'train_loss': 0.5134526885558511, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,                                                                    # í•™ìŠµì‹œí‚¬ model\n",
    "    args=training_args,                                                # TrainingArgumentsì„ í†µí•´ ì„¤ì •í•œ arguments\n",
    "    train_dataset=tokenized_datasets[\"train\"],         # training dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"], # validation dataset\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ì°¸ê³  ì´ë¯¸ì§€\n",
    ">> í•™ìŠµ ë…¸ë“œì—ì„œ ì²˜ìŒ(init) í•™ìŠµ ì‹œì¼°ì„ ë•Œì˜ Training loss ê²°ê³¼\n",
    "\n",
    "![Training loss at 500 steps](../Images/lec_22/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì–´ë–¤ê°€ìš”? ë²ˆê±°ë¡­ê²Œ ëª¨ë¸ì„ êµ¬í˜„í•˜ê³  ë°ì´í„°ì…‹ì„ ë‹¤ë“¬ì§€ ì•Šì•„ë„ ì‰½ê³  ë¹ ë¥´ê²Œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆëŠ” ì ì´ Huggingfaceì˜ ê°€ì¥ í° ì¥ì ì…ë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ì´ë²ˆ ë…¸ë“œì—ì„œ ê³µë¶€í•˜ì‹  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë…¸ë“œì—ì„œ íš¨ê³¼ì ì¸ ëª¨ë¸ì„ ë§Œë“¤ê³  í›ˆë ¨ì‹œì¼œë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-8. ë§ˆë¬´ë¦¬í•˜ë©°**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë ‡ê²Œ Huggingfaceê°€ ë§Œë“  frameworkì¸ transformersë¥¼ í›‘ì–´ë³´ì•˜ìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ë‚´ê°€ ì–´ë–»ê²Œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³ , í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ê³ , ë°ì´í„°ë¥¼ ê°€ê³µí•´ì•¼ í•˜ëŠ”êµ¬ë‚˜ ì¡°ê¸ˆ ê°ì´ ì¡íˆì…¨ì„ê¹Œìš”? ë¹„ë‹¨ transformersë¿ë§Œ ì•„ë‹ˆë¼, ë‹¤ë¥¸ frameworkë¥¼ ì‚¬ìš©í•˜ì‹¤ ë•Œì—ë„ ì´ì²˜ëŸ¼ frameworkì˜ ì „ë°˜ì ì¸ êµ¬ì¡°ë¥¼ ë¨¼ì € íŒŒì•…í•˜ì‹ ë‹¤ë©´, ë”ìš± ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆì„ ê±°ë¼ê³  ìì‹  ìˆê²Œ ë§ì”€ë“œë¦½ë‹ˆë‹¤!!!!!!\n",
    "<br><br>\n",
    "ì´ì œ transformersë¥¼ ì´ìš©í•˜ì—¬ ì´ì „ ì‹œê°„ì— ë°°ì› ë˜ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ê³¼ íŠ¹ì„±ì„ ë¹„êµí•´ ë³´ê¸°ë„ í•˜ê³ , ë‹¤ë¥¸ ì‚¬ëŒë“¤ì´ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ë„ ìˆê² ì£ ?\n",
    "<br><br>\n",
    "ë”.ë‚˜.ì•„.ê°€.\n",
    "<br><br>\n",
    "ë‚˜ë§Œì˜ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ ë¬¸ì œë¥¼ í’€ì–´ë³´ê¸°ë„ í•˜ë©´ ì™„-ë²½-â˜† í•˜ê²Œ NLPì˜ ë§ˆìŠ¤í„°ê°€ ë˜ì‹¤ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”ğŸ¤—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ì¢…í•© ë¬¸ì œ**\n",
    "\n",
    "---\n",
    "\n",
    "ì§€ê¸ˆê¹Œì§€ ì—¬ëŸ¬ë¶„ë“¤ì´ ì–¼ë§ˆë‚˜ í•™ìŠµì„ ì¶©ì‹¤íˆ í•˜ì…¨ëŠ”ì§€ ì•Œì•„ë³¼ê¹Œ í•©ë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì˜ ì‹¤ë ¥ì„ ì‘¥ì‘¥ í–¥ìƒì‹œì¼œì¤„ ìˆ˜ ìˆëŠ” í€´ì¦ˆì´ê¸°ë„ í•˜ë¯€ë¡œ ë°°ìš´ ë‚´ìš©ì„ ë‹¤ì‹œ ìƒê°í•˜ë©´ì„œ ì•„ë˜ì˜ í€´ì¦ˆë¥¼ ë¹ ì§ì—†ì´ í’€ì–´ë³´ì„¸ìš”. ğŸ¤—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q. Huggingfaceê°€ ë‹¤ë¥¸ NLP frame workì— ë¹„í•´ ê°–ëŠ” ê°•ì  ì„¸ ê°€ì§€ ì¤‘ í•˜ë‚˜ë¥¼ ì„¤ëª…í•´ë³¼ê¹Œìš”?**\n",
    "1. AI ëª¨ë¸ ì§€ì›ì´ ê´‘ë²”ìœ„í•˜ë©°, ìµœì‹  ë…¼ë¬¸ì„ ì§€ì›í•˜ëŠ” ì†ë„ê°€ ë¹ ë¦„\n",
    "> (1) ê´‘ë²”ìœ„í•˜ê³  ì‹ ì†í•œ NLP ëª¨ë¸ ì§€ì› HuggingfaceëŠ” ë§ì€ ì‚¬ëŒë“¤ì´ ìµœì‹  NLP ëª¨ë¸ë“¤ì„ ë”ìš± ì†ì‰½ê²Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ ë§Œë“¤ê¸° ì‹œì‘í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ê·¸ëŸ°ì§€ ìƒˆë¡œìš´ ë…¼ë¬¸ë“¤ì´ ë°œí‘œë  ë•Œë§ˆë‹¤, ë³¸ì¸ë“¤ì˜ frameworkì— í¡ìˆ˜ì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ, pretrained modelì„ ì œê³µí•˜ê³ , datasetê³¼ tokenizerë¥¼ ë”ìš± ì‰½ê²Œ ì´ìš©í•  ìˆ˜ ìˆë„ë¡ frameworkí™”ì‹œí‚¤ê³  ìˆëŠ” í–‰ë³´ë„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ frameworkë“¤ë„ ì´ëŸ° ì‘ì—…ì„ í•˜ì§€ ì•ŠëŠ” ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, Huggingfaceì˜ ì§€ì› ë²”ìœ„ê°€ ê°€ì¥ ê´‘ë²”ìœ„í•˜ê³ , ìµœì‹  ë…¼ë¬¸ì„ ì§€ì›í•˜ëŠ” ì†ë„ë„ ë¹ ë¦…ë‹ˆë‹¤.\n",
    "2. ëŒ€í‘œ AI í”„ë ˆì„ì›Œí¬ë“¤(`Pytorch`, `Tensorflow`) ëª¨ë‘ì—ì„œ ì‚¬ìš©ê°€ëŠ¥í•¨\n",
    "> (2) PyTorchì™€ Tensorflow ëª¨ë‘ì—ì„œ ì‚¬ìš© ê°€ëŠ¥ transformersëŠ” ê¸°ë³¸ì ìœ¼ë¡œ PyTorchë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì ¸ìˆìŠµë‹ˆë‹¤. ë§ì€ utilityê°€ PyTorch ìœ„ì£¼ë¡œ ì‘ì„±ì´ ë˜ì–´ìˆê¸´ í•˜ì§€ë§Œ, ìµœê·¼ì—ëŠ” Tensorflowë¡œë„ í•™ìŠµí•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆê²Œë” ê³„ì†í•´ì„œ frameworkë¥¼ í™•ì¥í•˜ê³  ìˆëŠ” ì¤‘ì…ë‹ˆë‹¤. ì´ë ‡ë“¯ Huggingface transformersë¥¼ ë°”íƒ•ìœ¼ë¡œ Tensorflowì™€ PyTorchë¼ëŠ” Backendì˜ í•œê³„ë¥¼ ë›°ì–´ë„˜ì–´ ì–´ë–¤ í™˜ê²½ì—ë“  ì‰½ê²Œ ì ìš© ê°€ëŠ¥í•œ í‘œì¤€ frameworkì˜ ì§€ìœ„ë¥¼ ë‹¤ì ¸ê°€ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "3. frameworkì˜ ì„¤ê³„ êµ¬ì¡°ê°€ ì¢‹ìŒ\n",
    "> (3) ì˜ ì„¤ê³„ëœ framework êµ¬ì¡° HuggingFaceì˜ ëª©í‘œì²˜ëŸ¼ ì´ frameworkëŠ” ì‰½ê³  ë¹ ë¥´ê²Œ ì–´ë– í•œ í™˜ê²½ì—ì„œë„ NLPëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ëŠì„ì—†ì´ ë³€í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ë˜í•œ ì‚¬ìš©í•˜ê¸° ì‰½ê³  ì§ê´€ì ì¼ë¿ë”ëŸ¬ ëª¨ë¸ì´ë‚˜ íƒœìŠ¤í¬, ë°ì´í„°ì…‹ì´ ë‹¬ë¼ì§€ë”ë¼ë„ ë™ì¼í•œ í˜•íƒœë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì˜ ì¶”ìƒí™”ë˜ê³  ëª¨ë“ˆí™”ëœ API ì„¤ê³„ê°€ ìˆê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•œ ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q. íŠ¹ì • ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ê²½ìš°, ì´ì— ë§ëŠ” tokenizerì„ ë¶ˆëŸ¬ì™€ì•¼ ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ì§€ ì•ŠëŠ” tokenizerì„ ë¶ˆëŸ¬ì˜¬ ê²½ìš° ì–´ë–¤ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆë‚˜ìš”?**\n",
    "- ë‹¨ì–´ì˜ êµ¬ë¶„ì´ ë‹¬ë¼ì ¸ ì›í•˜ëŠ” ì„±ëŠ¥ì´ ë‚˜ì˜¤ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.\n",
    "> ê° ëª¨ë¸ì€ íŠ¹ì • í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë°ì´í„°ë¥¼ í† í°í™”í•˜ê³ , ì´ í† í°í™”ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í›ˆë ¨ë©ë‹ˆë‹¤. ì´ í† í°í™” ê³¼ì •ì€ ëª¨ë¸ì´ ì–´ë–»ê²Œ ë‹¨ì–´ì™€ ë¬¸ì¥ì„ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. ë”°ë¼ì„œ í† í¬ë‚˜ì´ì €ë¥¼ ì˜ëª» ì„ íƒí•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "> ì˜ˆë¥¼ ë“¤ì–´, BERT ëª¨ë¸ì€ WordPiece í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ë§Œì•½ BERT ëª¨ë¸ì— ëŒ€í•´ WordPieceê°€ ì•„ë‹Œ ë‹¤ë¥¸ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ë©´, ëª¨ë¸ì€ ì…ë ¥ ë°ì´í„°ë¥¼ ì œëŒ€ë¡œ ì´í•´í•˜ì§€ ëª»í•˜ê³ , ë”°ë¼ì„œ ì •í™•ë„ê°€ í¬ê²Œ ì €í•˜ë  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¢…í•© í€´ì¦ˆì˜ ë‚œë„ëŠ” ì–´ë– ì…¨ë‚˜ìš”?\n",
    "\n",
    "í•™ìŠµì„ ì¶©ì‹¤íˆ í•˜ì…¨ë‹¤ë©´ ì‰½ê²Œ í•´ê²°í•˜ì…¨ì„ ê²ƒì´ë¼ ìƒê°í•©ë‹ˆë‹¤. í˜¹ì‹œë¼ë„ ë‹µì„ ë§ì¶”ì§€ ëª»í•˜ì…¨ë‹¤ë©´ ë‹¤ì‹œ í•œë²ˆ ë°°ì› ë˜ ë‚´ìš©ì„ ë³µìŠµí•´ ë³´ì„¸ìš”.\n",
    "\n",
    "ì§€ê¸ˆê¹Œì§€ ìˆ˜ê³  ë§ì´ í•˜ì…¨ìŠµë‹ˆë‹¤! ğŸ«¡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
