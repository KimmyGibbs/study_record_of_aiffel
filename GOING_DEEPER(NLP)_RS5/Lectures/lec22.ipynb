{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP Framework의 활용**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-1. 들어가며**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NLP 기술의 발전과 framework**\n",
    "\n",
    "---\n",
    "\n",
    "우리는 매일매일 빠르게 발전하고 있는 다양한 Modern NLP의 논문과 모델들을 확인할 수 있습니다.\n",
    "<br><br>\n",
    "이 모든 모델들을 직접 짜보는 것이 실력 향상에 도움이 되지만, 매번 새로운 논문들을 직접 구현할 수는 없겠죠. 설령 구현해 본다 하더라도 각각의 모델들이 최고의 성능을 내기 위해서는 엄청나게 많은 컴퓨팅 자원을 동원한 pre-training 작업을 필요로 합니다.\n",
    "<br><br>\n",
    "또한, 논문과 함께 공개된 리서치 코드를 이용하여 모델을 내가 가진 데이터셋과 문제에 적용할 수는 있지만, 매번 프로젝트마다 다른 코드 스타일과, 다른 framework(tensorflow, pytorch 등)을 사용하기 때문에 많은 모델들을 분석해서 구조를 파악한 후 직접 돌려보고 이를 적용하기란 쉽지 않을 것입니다.\n",
    "<br><br>\n",
    "이런 고민들을 모두 한꺼번에 해결해 줄 수 있는 게 바로 NLP Framework입니다!!\n",
    "<br><br>\n",
    "소프트웨어에서 framework란 프로젝트의 뼈대를 이루는 클래스와 인터페이스의 집합을 말합니다. 해당 분야의 베스트 프랙티스를 반영하여 확장 가능한 템플릿 형태로 설계되었기 때문에, framework를 이용해 손쉽게 다양한 응용 프로그램을 제작할 수 있습니다. 최근 NLP 분야에선 transformer 기반의 BERT 등 다양한 pretrained model이 발표되고, 이를 활용한 전이학습(transfer learning)을 통해 다양한 NLP 태스크를 손쉽게 구현하는 흐름이 두드러지고 있습니다. 이런 NLP 분야의 베스트 프랙티스를 바탕으로 다양한 NLP 분야의 framework가 속속 발표되고 있습니다. 이러한 NLP framework들은 NLP 분야의 최신 논문들의 리서치 코드를 미리 구현하여 pretrained model을 제공하고 있습니다. 그래서 framework의 사용자가 아주 손쉽게 이를 가져다가 다양한 태스크에 맞게 finetuning하거나 하여 사용할 수 있게 해 줍니다. 대부분의 NLP framework들은 태스크나 데이터셋, 모델에 무관하게 통일적인 인터페이스를 기반으로 설계된 클래스 구조를 가지고 있어서, 최소한의 코드 구현만으로도 다양한 변화에 대응할 수 있게 해주는 장점이 있습니다.\n",
    "<br><br>\n",
    "정말 다양한 framework들이 존재하고 있습니다. 어떤 멋진 NLP framework들이 있는지 간단히 살펴보겠지만, 이번 시간에 우리는 특히 **Huggingface(🤗)의 transformers**를 중심으로 다양한 NLP의 모델들을 다루는 방법에 대해 이야기 나눠볼까 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **학습 목표**\n",
    "\n",
    "---\n",
    "\n",
    "- 다양한 NLP Framework의 종류를 알고 활용할 수 있습니다.\n",
    "- Huggingface 모델을 불러와 올바른 task에 사용할 수 있습니다.\n",
    "- Huggingface 프레임워크의 구조를 파악하고, 상황에 맞게 이를 미세조정할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **학습 목차**\n",
    "- 2.다양한 NLP Framework의 출현\n",
    "    - 트랜스포머, 언제까지 구현해서 쓸 건가요?\n",
    "- 3.Huggingface transformers 개요\n",
    "    - 무엇이 Huggingface를 특별하게 만드나요?\n",
    "- 4.Huggingface transformers (1) Model\n",
    "    - Huggingface의 가장 핵심인 Model을 알아봅시다.\n",
    "- 5.Huggingface transformers (2) Tokenizer\n",
    "    - 정규표현식은 그만! 내장 Tokenizer을 써보아요.\n",
    "- 6.Huggingface transformers (4) Config\n",
    "    - 모델을 손쉽게 구성해봅시다.\n",
    "- 7.Huggingface transformers (5) Trainer\n",
    "    - 모델 학습을 위한 Trainer 클래스를 알아봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-2. 다양한 NLP Framework의 출현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오늘 우리는 주로 `Huggingface transformers` Framework를 통해 NLP framwork가 어떤 것인지를 구체적으로 살펴볼 것입니다. 그러나 `Huggingface transformers`이전에도 NLP 분야에 널리 알려진 framework들이 많이 있었습니다. 지금도 널리 활용되고 있는 것들 위주로 종류별로 몇 가지를 살펴보도록 하겠습니다.\n",
    "\n",
    "(참고) 이번 스텝의 내용은 [Popular NLP Libraries of 2022](https://medium.com/nlplanet/awesome-nlp-21-popular-nlp-libraries-of-2022-2e07a914248b)에 정리된 내용을 바탕으로 작성하였음을 밝힙니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **General Framework for NLP**\n",
    "\n",
    "---\n",
    "\n",
    "여기서 소개할 framework들은 NLP 문제를 가장 일반적으로 해결할 수 있는 통합적인 프레임워크를 목표로 설계된 것들입니다. 대표적으로는 AllenNLP, Fairseq, Fast.ai가 있으며, Google의 tensor2tensor 프로젝트도 같은 범주로 생각할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 AllenNLP\n",
    "<br><br>\n",
    "- 제공자 : Allen AI Institute\n",
    "- Website : https://allennlp.org/\n",
    "- Github : https://github.com/allenai/allennlp\n",
    "- Backend : PyTorch\n",
    "<br><br>\n",
    "\n",
    "2018년 초반에 Contextual Word Embedding의 대표적인 모델인 ELMO를 발표하면서 유명해진 Allen Institute에서 만든 NLP framework입니다. 당시 ELMO는 [GLUE Benchmark Test](https://gluebenchmark.com/)와 같이 10가지나 되는 다양한 태스크로 구성된 데이터셋을 하나의 모델을 finetune하는 것만으로도 기존의 State-of-the-art 기록을 경신하는 성능을 보여주고자 하였습니다. 그랬기에 하나의 모델로 다양한 태스크를 손쉽게 처리할 수 있는 유연한 프로젝트를 구성해야 했고, 이를 확장하면서 자연스럽게 NLP framework로 발전하게 되었습니다. 이후 AllenNLP는 Glue dataset의 baseline 프로젝트 [Starting Baseline](https://github.com/nyu-mll/GLUE-baselines)를 제공하기도 했습니다.\n",
    "<br><br>\n",
    "\n",
    "![출처 : AllenNLP Guide(https://guide.allennlp.org/building-your-model#1)](../Images/lec_22/1.png)\n",
    "<br><br>\n",
    "태스크와 모델을 분리해서, 한 가지 모델로 다양한 태스크를 처리하거나 하나의 태스크를 다양한 모델로 처리할 수 있도록 하는 설계는 AllenNLP가 처음 시도한 것은 물론 아닙니다만, ELMO와 같은 pretrained model의 성공을 바탕으로 NLP framework를 완성해 나가려는 AllenNLP의 시도는 이후 많은 아이디어를 제공하였습니다. AllenNLP는 현재는 ELMO 이외에도 BERT 등 다양한 모델의 활용이 가능합니다.\n",
    "<br><br>\n",
    "단, AllenNLP는 PyTorch 기반으로 설계되었으며 모델이 `torch.nn.Module`을 상속받는 구조로 설계되었습니다. Tensorflow나 Keras 기반으로 AllenNLP를 활용하는 것은 어렵습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 Fairseq\n",
    "<br><br>\n",
    "- 제공자 : Facebook AI Research\n",
    "- Website : https://fairseq.readthedocs.io/en/latest\n",
    "- Github : https://github.com/pytorch/fairseq\n",
    "- Backend : PyTorch\n",
    "<br><br>\n",
    "\n",
    "Fairseq는 꾸준히 NLP 연구성과를 내고 있는 Facebook AI Research의 NLP Framework입니다. 비단 자연어처리에만 국한된 것이 아니라 이름에서도 알 수 있듯이 CNN, LSTM 등 전통적인 모델로부터, 음성인식/합성 등 sequential한 데이터를 다루는 분야를 두루 다루는 다양한 pretrained model을 함께 제공하고 있습니다. 역시 Facebook의 framework답게 PyTorch 기반으로 설계되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 Fast.ai\n",
    "<br><br>\n",
    "- 제공자 : fast.ai\n",
    "- Website : http://docs.fast.ai/\n",
    "- Github : https://github.com/fastai/fastai\n",
    "- Backend : PyTorch\n",
    "<br><br>\n",
    "\n",
    "fast.ai는 이름에 걸맞게, 빠르게 배우고 쉽게 모델을 구성할 수 있도록 하이레벨 API와 Application 블록까지 손쉽게 사용할 수 있도록 구성되어 있습니다. 비단 NLP 분야 뿐 아니라 다양한 분야로 확장 가능합니다. 역시 PyTorch 기반으로 설계되었습니다.\n",
    "<br><br>\n",
    "\n",
    "![출처 : fast.ai (https://github.com/fastai/fastai)](../Images/lec_22/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 tensor2tensor\n",
    "<br><br>\n",
    "- 제공자 : Google Brain\n",
    "- Github : https://github.com/tensorflow/tensor2tensor (deprecated)\n",
    "- New Github : https://github.com/google/trax\n",
    "- Backend : Tensorflow\n",
    "<br><br>\n",
    "\n",
    "Google Brain에서 2017년에 transformer 논문을 발표하면서 그 구현체로 함께 공유했던 프로젝트가 바로 tensor2tensor였습니다. 이 프로젝트 역시 'Attention is all you need'라는 논문의 제목처럼, transformer를 중심으로 다양한 태스크와 다양한 모델을 하나의 framework에 통합하려는 시도를 하였습니다. 이후 2019년부터 Google은 Tensorflow V2 기반으로 pretrained model의 지원을 강화한 trax라는 프로젝트를 생성하면서, 2020년도부터는 tensor2tensor의 개발을 중단하고 관련 기능을 trax로 통합이관하였습니다.\n",
    "<br><br>\n",
    "Tensorflow 기반의 NLP framework이 상대적으로 드문 가운데, Tensorflow 기반의 NLP 연구개발을 진행한다면 주목해 볼 만할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preprocessing Libraries**\n",
    "\n",
    "---\n",
    "\n",
    "아래는 전통적으로 사용되었던 NLP 분야의 전처리 관련 framework들입니다. 위에서 소개한 framework들처럼 전처리-모델링-태스크 훈련/평가를 통합적으로 설계하여 NLP 태스크를 제너럴하게 수행하게 설계한 것이 아니라 tokenization, tagging, parsing 등 특정 전처리 작업을 위해 설계된 라이브러리에 가깝습니다. 여기에는 아래 예시로 든 Spacy, NLTK, TorchText 등이 있습니다. 한국어인 경우 KoNLPy 라이브러리도 동일한 역할을 한다고 볼 수 있겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 Spacy\n",
    "<br><br>\n",
    "- Website : https://spacy.io/\n",
    "- Github : https://github.com/explosion/spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 NLTK\n",
    "<br><br>\n",
    "- Website : https://www.nltk.org/\n",
    "- Github : https://github.com/nltk/nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 TorchText\n",
    "<br><br>\n",
    "- Website : https://torchtext.readthedocs.io/en/latest/\n",
    "- Github : https://github.com/pytorch/text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transformer-based Framework**\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n",
    "🔶 Huggingface transformers\n",
    "<br><br>\n",
    "- 제공자 : Huggingface.co\n",
    "- Website : https://huggingface.co/transformers/\n",
    "- Github : https://github.com/huggingface/transformers\n",
    "- Backend : PyTorch and Tensorflow\n",
    "<br><br>\n",
    "\n",
    "여기에는 현재 가장 주목받고 있는 NLP Framework인 `Huggingface transformers`가 있습니다. 사실 Huggingface의 transformers 라이브러리의 최근 모습은 이미 아주 general한 NLP framework의 모습을 충분히 가지고 있습니다. 하지만 초기에는 BERT 등 다양한 transformer 기반의 pretrained model을 사용하기 위한 PyTorch 기반의 wrapper 형태로 시작되었습니다. 그래서 전통적인 모델까지 포괄하려고 했던 이전의 general NLP Framework 들에 비해, Huggingface의 transformers는 pretrained model 활용을 주로 지원하며, tokenizer 등 전처리 부분도 pretrained model들이 주로 사용하는 Subword tokenizer 기법에 집중되어 있는 특징이 있습니다.\n",
    "\n",
    "이후 다음 스텝부터는 Huggingface의 transformers에 대해 집중적으로 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-3. Huggingface transformers 개요**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Huggingface?**\n",
    "\n",
    "---\n",
    "\n",
    "**(1) 광범위하고 신속한 NLP 모델 지원**\n",
    "Huggingface는 많은 사람들이 최신 NLP 모델들을 더욱 손쉽게 사용하는 것을 목표로 만들기 시작했다고 합니다. 그래서 그런지 새로운 논문들이 발표될 때마다, 본인들의 framework에 흡수시키고 있습니다. 또한, pretrained model을 제공하고, dataset과 tokenizer를 더욱 쉽게 이용할 수 있도록 framework화시키고 있는 행보도 보이고 있습니다. 다른 framework들도 이런 작업을 하지 않는 것은 아니지만, Huggingface의 지원 범위가 가장 광범위하고, 최신 논문을 지원하는 속도도 빠릅니다.\n",
    "<br><br>\n",
    "**(2) PyTorch와 Tensorflow 모두에서 사용 가능**\n",
    "transformers는 기본적으로 PyTorch를 기반으로 만들어져있습니다. 많은 utility가 PyTorch 위주로 작성이 되어있긴 하지만, 최근에는 Tensorflow로도 학습하고 사용할 수 있게끔 계속해서 framework를 확장하고 있는 중입니다. 이렇듯 Huggingface transformers를 바탕으로 Tensorflow와 PyTorch라는 Backend의 한계를 뛰어넘어 어떤 환경에든 쉽게 적용 가능한 표준 framework의 지위를 다져가고 있습니다.\n",
    "<br><br>\n",
    "**(3) 잘 설계된 framework 구조**\n",
    "HuggingFace의 목표처럼 이 framework는 쉽고 빠르게 어떠한 환경에서도 NLP모델을 사용할 수 있도록 끊임없이 변화하고 있습니다. 이것은 또한 사용하기 쉽고 직관적일뿐더러 모델이나 태스크, 데이터셋이 달라지더라도 동일한 형태로 사용 가능하도록 잘 추상화되고 모듈화된 API 설계가 있기 때문에 가능한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **시작하기**\n",
    "\n",
    "---\n",
    "\n",
    "역시 가장 먼저 해야 할 것은 프레임워크를 설치하는 것이죠! tensorflow를 사용하기 위해서 처음 설치했던 것처럼, huggingface의 Transformers도 설치하는 방법은 아래와 같습니다. (클라우드 사용자분들은 이미 설치가 되어 있으므로 눈으로 확인만 하고 가시면 됩니다.)\n",
    "```shell\n",
    "$ pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9978194236755371}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', framework='tf')\n",
    "classifier('We are very happy to include pipeline into the transformers repository.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.8451395034790039}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = '피곤했지만 뿌듯한 나날이었다.'\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', framework='tf')\n",
    "classifier(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Huggingface transformers 설계구조 개요**\n",
    "\n",
    "---\n",
    "\n",
    "framework를 사용하기 위해서는 framework가 어떻게 생겼는지 그 구조를 파악하는 것이 필요하겠죠??\n",
    "<br><br>\n",
    "NLP framework가 NLP모델을 통해 어떠한 문제를 푸는 과정이 어떻게 진행될지 생각해 봅시다.\n",
    "<br><br>\n",
    "먼저, 1) Task를 정의하고 그에 맞게 dataset을 가공시킵니다. 그 이후 2) 적당한 model을 선택하고 이를 만듭니다. 3)model에 데이터들을 태워서 학습을 시키고, 이를 통해 나온 4)weight와 설정(config)들을 저장합니다. 저장한 model의 checkpoint는 5)배포하거나, evaluation을 할 때 사용하고는 하죠.\n",
    "<br><br>\n",
    "transformers는 위와 같은 흐름에 맞추어 설계되어 있습니다.\n",
    "<br><br>\n",
    "task를 정의하고 dataset을 알맞게 가공하는 `Processors`, 텍스트 데이터를 전처리할 수 있는 `Tokenizer`, 다양한 model을 정의한 `Model`, optimizer와 학습 schedule(warm up 등)을 관리할 수 있는 `Optimization`, 학습 과정을 전반을 관리하는 `Trainer`, weight와 tokenizer, model을 쉽게 불러올 수 있도록 각종 설정을 저장하는 `Config` 등으로 이루어져 있습니다.\n",
    "<br><br>\n",
    "우리는 다음 스텝부터, Huggingface의 각 부분을 이루고 있는 클래스 구조에 대해 좀 더 자세히 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-4. Huggingface transformers (1) Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers의 가장 핵심적인 부분은 아무래도 모델이라고 할 수 있을 것입니다.\n",
    "<br><br>\n",
    "기본적으로 모델들은 `PretrainedModel` 클래스를 상속받고 있습니다. `PretrainedModel` 클래스는 학습된 모델을 불러오고, 다운로드하고, 저장하는 등 모델 전반에 걸쳐 적용되는 메소드들을 가지고 있습니다. 이런 상속 구조를 가지고 있기 때문에, 실제로 사용할 모델이 BERT이건, GPT이건 상관없이 모델을 불러오고 다운로드/저장하는 등의 작업에 활용하는 메소드는 부모 클래스의 것을 동일하게 활용할 수 있게 됩니다.\n",
    "<br><br>\n",
    "모델은 두 가지 방식으로 불러올 수 있습니다.\n",
    "<br><br>\n",
    "첫 번째로는 task에 적합한 모델을 직접 선택하여 import하고, 불러오는 방식이 있습니다. 모델을 로드할 때는 `from_pretrained`라는 메소드를 사용하며, Huggingface의 pretrained 모델을 불러올 수도, 직접 학습시킨 모델을 불러올 수도 있습니다.\n",
    "<br><br>\n",
    "Huggingface에서 제공하는 pretrained 모델이라면 모델의 이름을 string으로, 직접 학습시킨 모델이라면 config와 모델을 저장한 경로를 string으로 넘겨주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_tf_bert.TFBertForPreTraining'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForPreTraining\n",
    "model = TFBertForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 번째 방법은, AutoModel을 이용하는 것입니다.\n",
    "<br><br>\n",
    "모델에 관한 정보를 처음부터 명시하지 않아도 되어 조금 유용하게 사용하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_tf_bert.TFBertModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "model = TFAutoModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "방금 위에서 `bert-base-cased`라고 언급된 부분이 보이시나요? 이것은 **Model ID**입니다. Huggingface가 지원하는 다양한 pretrained model이 있습니다. 이들 중 어느 것을 선택할지를 결정하기 위해 이 ID를 활용하게 됩니다.\n",
    "\n",
    "어떤 모델이 지원되는지 아래 링크에서 확인해 볼 수 있습니다.\n",
    "<br><br>\n",
    "- [Pretrained models](https://huggingface.co/transformers/v4.11.3/pretrained_models.html)\n",
    "<br><br>\n",
    "\n",
    "그런데, 위에서 소개한 두 가지 방법의 차이가 파악되시나요? 불러오고자 하는 모델의 ID는 `bert-base-cased`로서 동일합니다. 사용법도 거의 동일한데요, 결과적으로 `model.__class__`를 확인해 보면 약간의 차이가 있음을 알 수 있습니다. 둘 다 동일한 모델 파라미터를 사용하지만 Pretrain, Downstream Task 등 용도에 따라 모델의 Input이나 Output shape가 다를 수 있습니다. AutoModel을 활용한다면 모델의 상세정보를 확인할 필요 없이 Model ID만으로도 손쉽게 모델 구성이 가능하지만, 정확한 용도에 맞게 사용하려면 모델별 상세 안내 페이지를 참고해서 최적의 모델을 선택하는 것이 좋습니다. 예를 들어 `BERT`의 상세 페이지는 아래와 같습니다.\n",
    "<br><br>\n",
    "- [MODELS - BERT](https://huggingface.co/docs/transformers/model_doc/bert)\n",
    "<br><br>\n",
    "\n",
    "모델마다 그 구조는 다르지만 대부분 해당 모델 이름을 가진 클래스(eg. TFBertModel)과 MainLayer class(eg. TFBertMainLayer)와 Attention Class, Embedding Class 등으로 이루어져 있답니다. 추후에 모델이 어떻게 짜여졌는지 보실 때 `__init__()` 메소드 안에 구성된 뼈대를 먼저 살펴보도록 하세요 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-5. Huggingface transformers (2) Tokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내가 가지고 있는 문제를 풀 모델을 정했다면, 이제 모델에 넣을 input을 만들어 줄 차례입니다.\n",
    "<br><br>\n",
    "transformers는 다양한 tokenizer를 각 모델에 맞추어 이미 구비해두었습니다. 우리가 할 일은 tokenizer를 불러와 사용하는 것뿐이랍니다. 사용하기에 앞서서, 내가 선택한 model이 어떠한 tokenizer를 사용하는지 정도 미리 체크해두는 센스는 모두 챙겨두셨겠죠?:)\n",
    "<br><br>\n",
    "Pretrained model 기반의 NLP framework를 사용할 때 가장 중요한 두 가지 클래스는 Model과 Tokenizer라고 할 수 있습니다. 그리고 두 가지는 밀접한 관련이 있습니다. 파라미터 구조가 동일한 Model이라 하더라도 Tokenizer가 다르거나 Tokenizer 내의 Dictionary가 달라지면 사실상 완전히 다른 모델이 됩니다. 그리고 Tokenizer는 어떤 언어를 다루느냐 하는 코퍼스 데이터셋에 따라서도 달라집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 스텝에서 소개했던 Huggingface가 제공하는 모델 종류 중 몇 개만 예로 들어볼까요?\n",
    "<br><br>\n",
    "- **bert-base-uncased** : BERT 모델인데, 108MB 파라미터의 기본 모델이면서, 코퍼스는 영문 대소문자를 구분하지 않음(전체 소문자화)\n",
    "- **bert-large-cased** : BERT 모델인데, 340MB 파라미터의 대형 모델이면서, 코퍼스는 영문 대소문자를 구분\n",
    "- **bert-base-multilingual-cased** : BERT 모델인데, 108MB 파라미터의 기본 모델이면서, 코퍼스는 다국어 대소문자를 구분\n",
    "<br><br>\n",
    "\n",
    "tokenizer 또한 직접 명시하여 내가 사용할 것을 지정해 주거나, AutoTokenizer를 사용하여 이미 구비된 model에 알맞은 tokenizer를 자동으로 불러올 수도 있습니다.\n",
    "이때 유의할 점은, **model을 사용할 때 명시했던 것과 동일한 ID로 tokenizer를 생성**해야 한다는 점입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불러온 tokenizer를 한 번 사용해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1188, 1110, 5960, 1111, 170, 11093, 1883, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(\"This is Test for aiffel\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 경우는 **BERT의 tokenizer이기 때문에** 인코딩이 된 `input_ids` 뿐만 아니라, `token_type_ids`와 `attention_mask`까지 모두 생성된 input 객체를 받아볼 수 있습니다.\n",
    "\n",
    "tokenizer의 `tokenize()` 메서드를 사용하면 토큰 단위로 분할된 문장을 확인할 수 있습니다. 한 번 확인해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화된 문장: {'input_ids': tensor([[  101,  1188,  1110,  5960,  1111,   170, 11093,  1883,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Q. 위 메서드를 사용하여 \"This is Test for aiffel\" 문장을 나누어 주세요.\n",
    "# 입력 문장\n",
    "text = \"This is Test for aiffel\"\n",
    "\n",
    "# 문장을 토큰으로 변환\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# 토큰 출력\n",
    "print(\"토큰화된 문장:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer는 batch 단위로 input을 받을 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102], [101, 1262, 1330, 5650, 102], [101, 1262, 1103, 1304, 1304, 1314, 1141, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                    \"And another sentence\",\n",
    "                    \"And the very very last one\"]\n",
    "\n",
    "encoded_batch = tokenizer(batch_sentences)\n",
    "print(encoded_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 밖에도 tokenize할 때에 `padding`, `truncation` 등 다양한 옵션을 설정할 수 있으며, 모델이 어떤 프레임워크를 사용하는가(Tensorflow 또는 PyTorch)에 따라 input 타입을 변경 시켜주는 `return_tensors` 인자도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[ 101, 8667,  146,  112,  182,  170, 1423, 5650,  102],\n",
      "       [ 101, 1262, 1330, 5650,  102,    0,    0,    0,    0],\n",
      "       [ 101, 1262, 1103, 1304, 1304, 1314, 1141,  102,    0]],\n",
      "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-6. Huggingfacae transformers (3) Config**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config는 모델을 학습시키기 위한 요소들을 명시한 `json`파일로 되어있습니다.\n",
    "<br><br>\n",
    "이 json파일에는 batch size, learning rate, weight_decay등 train에 필요한 요소들부터 tokenizer에 특수 토큰(special token eg.`[MASK]`)들을 미리 설정하는 등 설정에 관한 전반적인 것들이 명시되어 있습니다.\n",
    "<br><br>\n",
    "PretrainedModel을 save_pretrained 메소드를 이용하면 모델의 체크포인트와 함께 저장되도록 되어있습니다.\n",
    "<br><br>\n",
    "hugging face의 pretrained model을 그대로 사용하게 되면 자동으로 config파일이 로드되어 명시할 필요가 없지만, 설정을 변경하고 싶거나 나만의 모델을 학습시킬 때에는 config파일을 직접 불러와야 합니다.\n",
    "<br><br>\n",
    "config 또한 model, tokenizer처럼 Model ID만 있으면, Config 클래스를 명확히 지정하거나 혹은 `AutoConfig`를 이용하는 방식으로 불러올 수 있습니다. 두 방식의 결과를 비교해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 방식으로 불러온 config의 내용에 별다른 차이가 없다는 것을 알 수 있습니다. 만약 모델을 이미 생성했다면 `model.config`으로 가져올 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-cased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.11.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFBertForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "# Q. 생성된 모델에서 config를 가져와봅시다\n",
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-7. Huggingface transformers (4) Trainer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer는 모델을 학습시키기 위한 클래스입니다. training, fine-tuning, evaluation 모두 trainer class를 이용하여 할 수 있습니다.\n",
    "<br><br>\n",
    "trainer을 사용할 경우, `TrainingArguments` 를 통해 Huggingface 프레임워크에서 제공하는 기능들을 통합적으로 커스터마이징하여 모델을 손쉽게 학습시킬 수 있다는 장점이 있습니다.\n",
    "<br><br>\n",
    "trainer API를 사용하기 위해선 `TrainingArguments`인스턴스를 생성해야 하며, 이 때 학습에 필요한 여러 arguments들이 정의됩니다. 아래 예시를 통해 이번 노드에서 살펴본 Model, Tokenizer 및 데이터셋 구성이 TFTrainingArguments를 통해서 TFTrainer에 어떻게 반영되는지 확인해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/aiffel/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616c793f662749bc8263725b434a526a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 8551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"cola\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에 사용할 데이터셋은 GLUE benchmark 중 하나인 COLA dataset입니다. 자세한 내용은 뒤이어 나올 노드에서 확인해주세요.\n",
    "\n",
    "우선 데이터셋의 구성은 `sentence`, `label`, `idx`세 항목으로 나뉘어 있습니다.\n",
    "\n",
    "데이터를 학습시킬 모델과 이들을 토큰화하기 위한 토크나이저를 불러오고, 토큰화 함수도 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b18959ac3246be87e8517569751466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc04e2a47e241939c0665fe2e38f538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b43176f99f41ee9a61de0a115bd99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, num_labels=2)    # COLA dataset의 라벨은 0(unacceptable)과 1(accpetable) 두 가지로 구분됨\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화가 완료되었다면, trainer에 필요한 `TrainingArguments`를 선언해보도록 하겠습니다.\n",
    "<br><br>\n",
    "`TrainingArguments`에는 다음과 같은 인자들이 포함됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',              # output이 저장될 경로\n",
    "    num_train_epochs=1,              # train 시킬 총 epochs\n",
    "    per_device_train_batch_size=16,  # 각 device 당 batch size\n",
    "    per_device_eval_batch_size=64,   # evaluation 시에 batch size\n",
    "    warmup_steps=500,                # learning rate scheduler에 따른 warmup_step 설정\n",
    "    weight_decay=0.01,                 # weight decay\n",
    "    logging_dir='./logs',                 # log가 저장될 경로\n",
    "    do_train=True,                        # train 수행여부\n",
    "    do_eval=True,                        # eval 수행여부\n",
    "    eval_steps=1000,\n",
    "    group_by_length=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TFTrainingArguments`의 인자 중 `group_by_length`은 학습 과정을 효율화하는 기능이 있다.\n",
    ">  group_by_length (bool, optional, defaults to False) — Whether or not to group together samples of roughly the same length in the training dataset (to minimize padding applied and be more efficient). Only useful if applying dynamic padding.\n",
    ">> 이 인자는 학습 데이터에서 크기가 비슷한 것들을 묶어 사용하는 기능이 있다.\n",
    ">>> 동적 패딩을 사용할 경우, 데이터에 붙는 패딩을 최소화할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자, 이제 위에서 완성된 내용을 바탕으로 모델 훈련과 추론을 진행해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx.\n",
      "***** Running training *****\n",
      "  Num examples = 8551\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 535\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='535' max='535' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [535/535 01:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.520700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=535, training_loss=0.5134526885558511, metrics={'train_runtime': 60.6872, 'train_samples_per_second': 140.903, 'train_steps_per_second': 8.816, 'total_flos': 91292341767000.0, 'train_loss': 0.5134526885558511, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,                                                                    # 학습시킬 model\n",
    "    args=training_args,                                                # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=tokenized_datasets[\"train\"],         # training dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"], # validation dataset\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 참고 이미지\n",
    ">> 학습 노드에서 처음(init) 학습 시켰을 때의 Training loss 결과\n",
    "\n",
    "![Training loss at 500 steps](../Images/lec_22/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤가요? 번거롭게 모델을 구현하고 데이터셋을 다듬지 않아도 쉽고 빠르게 학습시킬 수 있는 점이 Huggingface의 가장 큰 장점입니다.\n",
    "<br><br>\n",
    "이번 노드에서 공부하신 내용을 바탕으로 다음 노드에서 효과적인 모델을 만들고 훈련시켜봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-8. 마무리하며**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 Huggingface가 만든 framework인 transformers를 훑어보았습니다.\n",
    "<br><br>\n",
    "내가 어떻게 모델을 불러오고, 토크나이저를 사용하고, 데이터를 가공해야 하는구나 조금 감이 잡히셨을까요? 비단 transformers뿐만 아니라, 다른 framework를 사용하실 때에도 이처럼 framework의 전반적인 구조를 먼저 파악하신다면, 더욱 유용하게 사용하실 수 있을 거라고 자신 있게 말씀드립니다!!!!!!\n",
    "<br><br>\n",
    "이제 transformers를 이용하여 이전 시간에 배웠던 모델들의 성능과 특성을 비교해 보기도 하고, 다른 사람들이 학습시킨 모델을 이용하여 더 좋은 성능을 내는 모델을 만들 수도 있겠죠?\n",
    "<br><br>\n",
    "더.나.아.가.\n",
    "<br><br>\n",
    "나만의 모델과 데이터셋을 이용하여 문제를 풀어보기도 하면 완-벽-☆ 하게 NLP의 마스터가 되실 수 있을 거예요🤗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **종합 문제**\n",
    "\n",
    "---\n",
    "\n",
    "지금까지 여러분들이 얼마나 학습을 충실히 하셨는지 알아볼까 합니다. 여러분의 실력을 쑥쑥 향상시켜줄 수 있는 퀴즈이기도 하므로 배운 내용을 다시 생각하면서 아래의 퀴즈를 빠짐없이 풀어보세요. 🤗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q. Huggingface가 다른 NLP frame work에 비해 갖는 강점 세 가지 중 하나를 설명해볼까요?**\n",
    "1. AI 모델 지원이 광범위하며, 최신 논문을 지원하는 속도가 빠름\n",
    "> (1) 광범위하고 신속한 NLP 모델 지원 Huggingface는 많은 사람들이 최신 NLP 모델들을 더욱 손쉽게 사용하는 것을 목표로 만들기 시작했다고 합니다. 그래서 그런지 새로운 논문들이 발표될 때마다, 본인들의 framework에 흡수시키고 있습니다. 또한, pretrained model을 제공하고, dataset과 tokenizer를 더욱 쉽게 이용할 수 있도록 framework화시키고 있는 행보도 보이고 있습니다. 다른 framework들도 이런 작업을 하지 않는 것은 아니지만, Huggingface의 지원 범위가 가장 광범위하고, 최신 논문을 지원하는 속도도 빠릅니다.\n",
    "2. 대표 AI 프레임워크들(`Pytorch`, `Tensorflow`) 모두에서 사용가능함\n",
    "> (2) PyTorch와 Tensorflow 모두에서 사용 가능 transformers는 기본적으로 PyTorch를 기반으로 만들어져있습니다. 많은 utility가 PyTorch 위주로 작성이 되어있긴 하지만, 최근에는 Tensorflow로도 학습하고 사용할 수 있게끔 계속해서 framework를 확장하고 있는 중입니다. 이렇듯 Huggingface transformers를 바탕으로 Tensorflow와 PyTorch라는 Backend의 한계를 뛰어넘어 어떤 환경에든 쉽게 적용 가능한 표준 framework의 지위를 다져가고 있습니다.\n",
    "3. framework의 설계 구조가 좋음\n",
    "> (3) 잘 설계된 framework 구조 HuggingFace의 목표처럼 이 framework는 쉽고 빠르게 어떠한 환경에서도 NLP모델을 사용할 수 있도록 끊임없이 변화하고 있습니다. 이것은 또한 사용하기 쉽고 직관적일뿐더러 모델이나 태스크, 데이터셋이 달라지더라도 동일한 형태로 사용 가능하도록 잘 추상화되고 모듈화된 API 설계가 있기 때문에 가능한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q. 특정 모델을 불러올 경우, 이에 맞는 tokenizer을 불러와야 모델이 정상적으로 작동할 수 있습니다. 맞지 않는 tokenizer을 불러올 경우 어떤 오류가 발생할 수 있나요?**\n",
    "- 단어의 구분이 달라져 원하는 성능이 나오지 않을 수 있다.\n",
    "> 각 모델은 특정 토크나이저를 사용하여 입력 데이터를 토큰화하고, 이 토큰화된 데이터를 기반으로 훈련됩니다. 이 토큰화 과정은 모델이 어떻게 단어와 문장을 이해하고 처리하는지를 결정합니다. 따라서 토크나이저를 잘못 선택하면 모델의 성능이 크게 저하될 수 있습니다.\n",
    "<br><br>\n",
    "> 예를 들어, BERT 모델은 WordPiece 토크나이저를 사용하여 텍스트를 토큰화하고 이를 기반으로 훈련되었습니다. 만약 BERT 모델에 대해 WordPiece가 아닌 다른 토크나이저를 사용하면, 모델은 입력 데이터를 제대로 이해하지 못하고, 따라서 정확도가 크게 저하될 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "종합 퀴즈의 난도는 어떠셨나요?\n",
    "\n",
    "학습을 충실히 하셨다면 쉽게 해결하셨을 것이라 생각합니다. 혹시라도 답을 맞추지 못하셨다면 다시 한번 배웠던 내용을 복습해 보세요.\n",
    "\n",
    "지금까지 수고 많이 하셨습니다! 🫡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
