{"cells":[{"cell_type":"markdown","metadata":{},"source":["#19.mordern NLP의 흐름에 올라타보자"]},{"cell_type":"markdown","metadata":{},"source":["## 19-1. 들어가며"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 19-2. Transfer Learning과 Language Modeling"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 19-3. ELMO(Embedding from Language Models)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 19-4. GPT(Generative Pre-Training Transformer)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 19-5. BERT(Bidirectional Encoder Representations from Transformers)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 19-6. Transformer-XL(Transformer Extra Long)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 19-7. XLNet, BART"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 19-8. ALBERT(A Little BERT for Self-supervised Learning of Language Representations)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 19-9. T5(Text-to-Text Transfer Transformer)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 19-10. Switch Transformer"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 19-11. ERNIE"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 19-12. 마무리하며"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
