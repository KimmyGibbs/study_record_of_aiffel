{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **25. LLM Trend Note 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **25-1. 들어가며**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "안녕하세요 여러분:) LLM Trend Note1 에 오신걸 환영합니다!\n",
    "<br><br>\n",
    "최신 트렌드라는 게 따라가자니 어디서부터 시작해야할지 막막하고,<br>\n",
    "보고 있자니 이 기술이 앞으로도 유효할지 모르겠고,<br>\n",
    "아마 이 노드에서도 시간을 들여 숙고해볼 만한 가치가 없는 내용이<br>\n",
    "여러분들께서 노드를 보시게 될 시점엔 많아지게 되는 건 아닐까<br>\n",
    "저 역시 한편으론 걱정이 되기도 합니다.<br>\n",
    "이 노드를 작성하기 위해 자료를 수집하는 동안에도 closed source가 open되기도 했고<br>\n",
    "이런 자료가 있었으면 좋겠다 싶은 것들이 다음 날에 짠 하고 공개되곤 했으니까요.<br>\n",
    "<br>\n",
    "하지만 다른 한편으로 트렌드란<br>\n",
    "오랜 시간의 압력으로 굳어져 크게 변할 수 없어 보이는 현상이나<br>\n",
    "현재까지 이룩한 문명의 힘으로 규정지어진 기술적, 사회적 사실들로부터 생겨나고 바뀌는 것이기도 하지요.<br>\n",
    "그래서 우리는 아래에 적은 몇가지 fact로부터 출발해보고자 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fact 1.**\n",
    "\n",
    "전 세계에서 사용되는 언어에 대한 각종 통계를 발표하는 [에스놀로그](https://www.ethnologue.com/)에 따르면\n",
    "2022년 기준 지구상엔 7168개의 언어가 있다고 합니다.\n",
    "그 중에서 한국어는 사용자 수 기준 20위권 안팎에 있다고 기록되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처: https://www.neuralspace.ai/challenges-in-using-nlp-for-low-resource-languages-and-how-neuralspace-solves-them](../Images/lec_25/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 2023년 3월 14일 OpenAI가 발표한 [GPT-4](https://openai.com/research/gpt-4)에 따르면,\n",
    "한국어는 아프리카에서 쓰는 스와힐리, 영국 웨일스 지방의 언어인 웰시와 함께 low resource language로 분류됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fact 2.**\n",
    "\n",
    "2022년 10월에 발표된<br>\n",
    "[Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning](https://arxiv.org/pdf/2211.04325.pdf)\n",
    "이라는 흥미로운 제목의 논문에 따르면, 전 세계의 디지털화 된 고품질 텍스트 데이터의 총 재고,\n",
    "다시 말해 전세계 NLP 개발자 및 연구자들이 모델 학습에 쓸 수 있는 사용 가능한 총 토큰 개수가\n",
    "4조 6000억에서 17조 2000억 개 사이로 추정된다고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://arxiv.org/pdf/2211.04325.pdf](../Images/lec_25/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[또 다른 출처](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications)에 의하면 그보다 훨씬 적은 약 3.2조개의 토큰으로 추산된다고도 하네요!\n",
    "<br><br>\n",
    "그런데 2023년 3월 현재 가장 핫한 OpenAI의 GPT-4와 더불어 주목받고 있는 Large Language Model(이하 LLM)인\n",
    "Meta의 LLaMA가 pre-train을 하는데 사용한 토큰의 개수는 자그마치 1.4T (1조 4천억) 개라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잠깐 정리해 볼까요?\n",
    "\n",
    "1. 전세계 7천여 개 언어 가운데 사용자 수로 치면 20등 정도 하는 한국어가,\n",
    "LLM 관점에선 그다지 풍부한 자원의 언어가 아니다.\n",
    "\n",
    "2. LLM을 학습하는 데 사용가능한 고품질 토큰이 수조 개 가량 있는데,\n",
    "최신 LLM은 해당 총 토큰 개수의 최대 1/4에 달하는 양을 학습하고 있다.\n",
    "\n",
    "안타깝게도 지금 이 글을 쓰는 시점에선 chatGPT의 LM으로 쓰인 GPT3.5 및 GPT-4가\n",
    "구체적으로 어떤 언어들을 몇개나 학습했는지 공개되지 않았습니다.\n",
    "대신 자료가 공개된 모델 중 참고해 볼만한 가장 최신 모델이 있습니다.\n",
    "<br><br>\n",
    "2022년 4월에 발표된 Google의 PaLM은 5천 400억 개 파라미터를 가진 현재 시점 기준 가장 큰 모델입니다.\n",
    "이 PaLM 이란 모델이 학습에 사용한 토큰 개수는 총 7천 800억개입니다.\n",
    "여기서 한국어 토큰은 약 0.2%를 차지합니다. [참고](https://arxiv.org/pdf/2204.02311.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://lifearchitect.ai/models/](../Images/lec_25/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **자, 여기서 우리는 어떤 생각을 이어나가 볼 수 있을까요?**\n",
    "\n",
    "최신 모델 아키텍쳐와 분산학습 기술이 발전함에 따라 LLM의 모델 사이즈는 앞으로 더욱더 커질 수 있습니다.<br>\n",
    "또는 훨씬 더 좋은 성능을 내면서도 모델 사이즈는 줄어드는 쪽으로도 발전할 수 있겠죠.<br>\n",
    "어느 쪽이든 디지털 토큰의 총량을 다 먹어치우는 날은 머지 않아 도래할 것입니다.<br>\n",
    "전 세계 인구 수와 증감률, 인터넷 유저 수와 보급률의 추이를 통해 예상할 수 있는 시나리오지요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대한민국의 출산율은 OECD 회원국 중 유일하게 1명대 아래입니다.<br>\n",
    "정확히는 2013년부터 2022년까지 10년간 출산율 최하위를 기록하고 있습니다.<br>\n",
    "(2007년, 2012년 두 해만 꼴찌에서 두 번째)<br>\n",
    "(출처 : https://www.pressian.com/pages/articles/2023022215021464370)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면, UN에 따르면 세계인구 수는 2022년 11월 기준 80억명을 돌파했고,\n",
    "향후 80년간 110억명 대에 이를 것으로 추산합니다.<br>\n",
    "(출처 : https://ko.wikipedia.org/wiki/%EC%84%B8%EA%B3%84_%EC%9D%B8%EA%B5%AC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편, 전 세계의 7,000여개 언어 중 보존을 위한 조치를 하지 않으면\n",
    "향후 100년 내에 당장 소멸 위기에 처한 언어가 1,500 여개에 달한다고 합니다.<br>\n",
    "(출처 : http://thescienceplus.com/news/newsview.php?ncode=1065613046395135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 저런 통계량들을 종합해보면, Language Modeling을 위한 학습 데이터를 구성할 때,<br>\n",
    "한국어를 위시한 low resource language의 경우 심각한 불균형을 유지할 것이라고 말할 수 있습니다.<br>\n",
    "더불어 현재 이후 발표될 최신 모델들은 이 불균형한 학습 데이터 비율 구조에서 크게 벗어나지 않은 상태로<br>\n",
    "학습 가능한 모든 토큰을 학습하게 될 것이라고도 말해볼 수 있을 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼에도 불구하고, 2023년 3월 기준 chatGPT에게 low resource language인 한국어로 질문을 했을 때<br>\n",
    "아래와 같이 꽤 괜찮은 한국어 답변을 내놓는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![chatgpt_example](../Images/lec_25/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 현실이 한국어로 NLP를 연구하며 서비스하는 사람들에게 어떤 의미를 지니고,<br>\n",
    "어떤 도전을 요구하는지에 관해선 이 노드의 마무리에 함께 생각해보도록 하겠습니다.<br>\n",
    "<br>\n",
    "**저렇게 엄청난 크기의 모델 안에 어떤 능력들이 숨어 있는 걸까요?**<br>\n",
    "**그 능력들을 가능하게 하는 기술들은 무엇일까요?**<br>\n",
    "**반대로 반드시 저렇게 거대한 모델이어야만 좋은 성능을 발휘하는 걸까요?**<br>\n",
    "<br>\n",
    "오늘 이 시간 대표적인 최신 LLM 모델과 그 아키텍쳐 및 학습기법들을 살펴보면서\n",
    "위 질문들에 대한 힌트를 얻어볼 수 있게 되면 좋겠습니다.\n",
    "나아가 현재 어떤 흐름으로 NLP가 흘러왔고 흘러가게 될지 헤아려 볼 수 있는 시간이 되시길 바래봅니다.\n",
    "<br><br>\n",
    "그럼 정리해 볼까요?\n",
    "<br><br>\n",
    "아래 세 가지 학습목표와 학습내용에 있는 키워드들을 찬찬히 읽어보신 후,\n",
    "본격적으로 노드를 진행하기 앞서 노드를 보기 전 드는 생각과\n",
    "보고 난 후 들게 될 생각을 비교해보는 것도 의미가 있을 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **학습목표**\n",
    "\n",
    "1. LLM의 등장배경과 그 의의를 설명할 수 있습니다.\n",
    "\n",
    "2. 최신 LLM 모델들의 성능에 영향을 미치는 핵심 기술들을 설명할 수 있습니다.\n",
    "\n",
    "3. 당면한 문제와 한계를 파악하고 전망할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **학습내용**\n",
    "\n",
    "- Foundation Model과 Emergent Abilities\n",
    "- PaLM, FLAN, ChatGPT, LLaMA 등 최신 LLM 모델의 특징\n",
    "- 최신 LLM 모델들의 주요 아키텍쳐 (Sparse Attention, RLHF)\n",
    "- LLM을 더욱 효율적으로 학습시키는데 필요한 기술들 (LoRA, LLM.int8())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고\n",
    "1. [국립국어원(모두의 말뭉치)](https://corpus.korean.go.kr/request/reausetMain.do?lang=ko)\n",
    "2. [AI Hub](https://aihub.or.kr/aihubdata/data/list.do?pageIndex=1&currMenu=115&topMenu=100&dataSetSn=&srchdataClCode=DATACL001&srchOrder=&SrchdataClCode=DATACL002&searchKeyword=&srchDataRealmCode=REALM002&srchDataTy=DATA003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **25-2. LLM의 Emergent Abilities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Statistic LM, Neural LM, Pre-trained LM 그리고 LLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Foundation Model**\n",
    "\n",
    "foundation model 이라는 개념에 대해 들어보셨나요?<br>\n",
    "통계적 언어모델, 신경망 언어모델을 거쳐 Transfomer 아키텍쳐 등장 이후,\n",
    "우리는 사전 훈련된 모델을 사용해 downstream task를 수행하는 패러다임을 지나가고 있습니다.<br>\n",
    "2022년에 발표된 논문 [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf) 에서는\n",
    "지금까지 나온 모든 Pre-trained LM(이하 PLM)들을\n",
    "foundation model 이라는 개념으로 지칭하며 새로운 패러다임을 제시합니다.\n",
    "<br><br>\n",
    "논문의 저자들은 foundation model을 두 가지 특징으로 정의합니다.<br>\n",
    "하나는 emergence, 다른 하나는 homogenization 입니다.\n",
    "<br><br>\n",
    "논문에 따르면, 2019년 이전에 언어 모델을 사용한 self-supervised learning은\n",
    "본질적으로 NLP의 하위 영역이었으며 NLP의 다른 개발과 병행하여 진행되었습니다.<br>\n",
    "RNN(LSTM)을 활용한 언어모델링과 seq2seq 아키텍쳐, 그리고 트랜스포머는\n",
    "성능과 태스크상의 차이로 그 위계가 분류되긴 했었지만\n",
    "비교적 동등한 수준에서 NLP의 가지를 이루고 있었다는 뜻입니다.\n",
    "<br><br>\n",
    "2019년 이후 언어 모델을 사용한 self-supervised learning은\n",
    "BERT를 사용하는 것이 표준이 되면서 NLP의 기반이 되었습니다.<br>\n",
    "단일 모델이 이러한 광범위한 작업에 유용할 수 있다는 건 foundation model 패러다임의 시작을 의미합니다.<br>\n",
    "거의 모든 최첨단 NLP 모델은 이제 BERT, RoBERTa, BART, T5 등과 같은 몇 가지 기본 모델 중 하나에서 채택됩니다.<br>\n",
    "이것이 foundation model의 homogenization(균질화)가 의미하는 바입니다.\n",
    "<br><br>\n",
    "이러한 homogenization는 매우 높은 레버리지를 가집니다.<br>\n",
    "백본 모델로 쓰이는 몇가지 모델만 개선되면, NLP 전반에 즉각적으로 그 개선에 의한 이점이 퍼지게 되니까요.<br>\n",
    "반대로 모든 AI 시스템은 몇 가지 기본 모델의 동일한 문제(데이터의 편향 등)을 물려받을 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://arxiv.org/pdf/2108.07258.pdf](../Images/lec_25/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 foundation model의 또 다른 특징인 emergence란 무엇일까요?<br>\n",
    "**우리는 이번 lecture 노드를 통해 emergence란 무엇인지,**<br>\n",
    "**그리고 emergence를 둘러싼 다양한 관점과**<br>\n",
    "**이를 뒷받침하는 최신 NLP 논문 및 모델들에 대해 살펴볼 것입니다.**\n",
    "<br><br>\n",
    "foundation model 패러다임은 머신러닝과 딥러닝에서 다루는 모든 종류의 model과 task를 망라합니다.<br>\n",
    "저희는 NLP를 보다 심도 있게 공부하기 위해 모였으니 LM에 집중해서 이야기를 해야겠죠?\n",
    "<br><br>\n",
    "그럼 Emergence로 가기 위한 첫 관문인 LM 그중에서도 LLM에 첫발을 내딛어 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **LLM**\n",
    "\n",
    "일반적으로 논문이나 깃헙에 공개된 레포짓에 나와 있는 모델 설명을 보면\n",
    "모델 이름 뒤에 small, base, large, XL 등 모델의 크기를 덧붙여 모델 이름을 지은 걸 볼 수가 있습니다.<br>\n",
    "BERT-Large, XLM-R Base, Flan-T5-XXL 와 같은 식으로 말이죠.<br>\n",
    "또 어떤 모델들은 m2m100_418M, GPT-NeoX-20B 처럼 구체적인 모델 파라미터 개수를 적기도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://huggingface.co/blog/large-language-models](../Images/lec_25/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림은 지난 몇 년간 발표된 주요 모델들의 파라미터 스케일의 변화를 보여주는 그래프입니다.<br>\n",
    "Transfer learning의 효시라 부를 수 있는 ELMo에서 시작해 매 해마다 LM의 크기는 10배씩 증가했습니다.\n",
    "<br><br>\n",
    "\"크기\"란 상대적인 개념입니다.<br>\n",
    "2018년에 발표된 BERT의 large모델은 340M (3억 4천만) 개의 파라미터를 가지고 있습니다.<br>\n",
    "그 당시로선 같은 해 나온 ELMo와 비교한 다면 \"Large\" language model 이라 부르는 게 타당해보입니다.\n",
    "<br><br>\n",
    "2022년 기준 가장 큰 모델은 540B 개의 파라미터를 가진 Google의 PaLM 입니다.<br>\n",
    "(위 그래프에 top-right에 있는 Megatron-Turing NLG 530B는\n",
    "NVIDIA에서 발표한 모델로 PaLM에 조금 못 미치는 크기입니다.)<br>\n",
    "BERT는 PaLM에 비하면 더 이상 large model이라 부를 수 없을 것 같습니다.<br>\n",
    "단위가 million에서 billion으로 바뀌었으니까요.<br>\n",
    "따라서 앞으로 (trillion 단위의)더 큰 사이즈의 모델이 나온다고 했을 때, \"Large\"의 기준단위는 계속 바뀌어 나갈 것 입니다.\n",
    "<br><br>\n",
    "그러나 이제부터 **우리가 지칭할 Large Language Model (이하 LLM) 에서 Large의 의미는**<br>\n",
    "**'크기의 상대적인 개념'에 구속되지 않습니다.**<br>\n",
    "다시 말해, 크기의 절대적인 기준이 있습니다.<br>\n",
    "**수백억에서 수천억개의 파라미터를 가진 Pretrained Language Model (이하 PLM) 을 LLM 이라고 부릅니다.**<br>\n",
    "(물론 이 기준 또한 낮아질 수 있습니다. 아마 그러할 가능성이 매우 농후해보입니다.)\n",
    "<br><br>\n",
    "참고로 LLM은 Mixture-of-Expert (MoE) 기법의 사용유무에 따라\n",
    "dense transformer models 과 sparse mixture-of-expert (MoE) models로 구분할 수 있는데<br>\n",
    "후자는 전자보다 훈련/추론 계산 시 더 많은 모델 파라미터를 가집니다.<br>\n",
    "dense transformer models에는 Jurassic-1, gopher, Megatron-Turing NLG 530B, LaMDA 등이 있고,<br>\n",
    "sparse mixture-of-expert (MoE) models에는 Glam, switch transformers 등이 있습니다.<br>\n",
    "(switch transformer와 MoE에 대해서는 \"modern NLP의 흐름에 올라타보자\" 노드를 참고하세요)\n",
    "<br><br>\n",
    "그렇다면, 수백억 이상의 파라미터 스케일을 가진 PLM을 LLM으로 분류할 때,\n",
    "이러한 LLM과, 그보다 작은 사이즈의 LM 사이에는 어떤 차이점이 있는 것일까요?\n",
    "<br><br>\n",
    "조금 더 구체적으로 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Emergnet abilities를 위한 배경지식**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **In-context learning 과 Emergent Abilities**\n",
    "\n",
    "일단 LLM으로 분류되는 파라미터 스케일 기준의 배경부터 살펴보겠습니다.<br>\n",
    "아래 그림은 2019년부터 2023년 초까지 발표된 모델 중 10B 이상의 파라미터를 가진 모델들만 추려놓은 그림입니다.<br>\n",
    "(이번 노드에 언급되는 대부분의 모델들은 이 그림에서 찾아보실 수 있습니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://arxiv.org/pdf/2303.18223.pdf](../Images/lec_25/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 중에서 우리가 눈여겨 볼 LLM은 2020년에 발표된 GPT-3 입니다.\n",
    "<br><br>\n",
    "GPT-3는 175B (1천 750억)개의 파라미터를 트랜스포머 디코더 기반 아키텍쳐에 집어 넣은 LLM 입니다.<br>\n",
    "흥미롭게도, **파라미터 수가 일정 수준을 초과한 LLM은 BERT와 같은 million 단위의 소규모 LM에 없는 성능을 나타냅니다.**<br>\n",
    "GPT-3를 개발한 OpenAI의 연구자들은 [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)논문에서<br>\n",
    "**“in-context learning”** 의 일종인 zero-shot, one-shot, few-shot 이라는 개념으로 이 성능을 정의했습니다.\n",
    "<br><br>\n",
    "그리고 2년 후 Google Research, Stanford University, UNC Chapel Hill, 그리고 DeepMind의 연구자들은\n",
    "이 성능이 어디서부터 기인하는 지를 가리켜 **Emergent Abilities** 라는 개념을 공론화하였습니다. [(참고)](https://arxiv.org/pdf/2206.07682.pdf)\n",
    "\n",
    "그렇습니다.\n",
    "바로 Emergent abilities가 수백억개를 초과하는 LLM이 가진 특징이자, 바꿔 말해 소규모 LM에서는 발견되지 않는 특징입니다.\n",
    "\n",
    "Emergent abilities 가 무엇인지 본격적으로 살펴보기 전에 먼저 그 배경이 되는 핵심 개념들을 슬쩍 엿보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.  Instruction Tuning 과 Chain-of-Thought Prompting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **instruction tuning**\n",
    "\n",
    "GPT-3 논문에서 소개하고 있는 zero-shot, one-shot, few-shot 은\n",
    "모델의 inference 단계에서 이뤄지는 작업입니다.<br>\n",
    "즉, gradient를 계산해 모델 파라미터 업데이트를 함으로써 학습하는 방법이 아니라는 뜻입니다.<br>\n",
    "zero-shot, one-shot, few-shot을 간단히 설명하면 아래와 같이 요약할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> zero-shot : 적절한 instruction이 담긴 지시문을 모델에 던져서 원하는 답변을 이끌어 내는 것입니다.<br>\n",
    "> one-shot, few-shot : 단순히 지시문만 주는 게 아니라 구체적인 예제를 던져서 원하는 답변을 이끌어 내는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM에 사용되는 위와 같은 추론 방법들은 후에 prompt learning 이라고도 불리게 됩니다.<br>\n",
    "적절한 instruction 내지 prompt를 주었을 때 더 모델로부터 더 나은 답변을 얻어 낼 수 있게 하는 것이지요.<br>\n",
    "GPT-3 논문에서 소개된 이 방법들은 이후 instruction tuning, prompt engineering 같은 방법론으로 발전하게 됩니다.\n",
    "<br><br>\n",
    "instruction tuning은 앞서 살펴본 그림에서 Google이 발표한 [FLAN](https://arxiv.org/pdf/2109.01652.pdf) 논문에 처음 등장합니다.\n",
    "<br><br>\n",
    "prompt engineering의 한 방법인 chain-of-thought prompting은 같은 그림에서 역시<br>\n",
    "Google이 발표한 [PaLM](https://arxiv.org/pdf/2204.02311.pdf) 논문에 자세한 설명이 실려 있습니다.\n",
    "<br><br>\n",
    "먼저 instruction tuning을 살펴보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **FLAN : Finetuned Language Models Are Zero-Shot Learners**\n",
    "\n",
    "FLAN 논문의 저자들은 zero-shot learning abilities를 개선하는 방법을 제안하는 것이 논문의 목적이라고 말합니다.<br>\n",
    "이를 위해 연구진들은 LaMDA (이하 람다) 모델을 사용했습니다.<br>\n",
    "람다는 GPT-3보다 좀 더 작은 137B 사이즈의 디코더 기반 트랜스포머 모델로 instruction tuning을 활용해<br>\n",
    "GPT-3보다 더 나은 성능의 zero-shot learning abilities를 달성했다고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://arxiv.org/pdf/2109.01652.pdf](../Images/lec_25/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "람다에 대한 자세한 설명은 [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239.pdf)논문을 참고하세요.\n",
    "<br><br>\n",
    "instruction tuning의 핵심은 다양한 종류의 NLP task를 instruction과<br>\n",
    "그에 상응하는 label로 이뤄진 pair dataset으로 fine-tuning한 후<br>\n",
    "한 번도 보지 못한 task에서 inference를 하여 만족할 만한 성능을 내는 튜닝방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://arxiv.org/pdf/2109.01652.pdf](../Images/lec_25/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어, commonsense reasoning task의 경우<br>\n",
    "위 그림에서처럼 instruction 지문에서 질문 내용에 참고가 될 목표를 지시해준 후<br>\n",
    "상식선에서 추론 가능한 답변을 target으로 주는 dataset을 모델에 fine-tuning 시킵니다.\n",
    "<br><br>\n",
    "더 정확히는, 먼저 아래 그림과 같이 기존에 공개된 데이터셋들을 총 12개 카테고리로 분류합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://arxiv.org/pdf/2109.01652.pdf](../Images/lec_25/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음, 각 데이터셋들을 instruction 형식으로 수정합니다.<br>\n",
    "구체적으로는 각 데이터셋 마다 10개의 instruction template을 만든 다음,<br>\n",
    "전체 데이터 셋에서 무작위로 template을 뽑아 fine-tuning을 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://arxiv.org/pdf/2109.01652.pdf](../Images/lec_25/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 downstream task 별 fine-tuning 과 prompting 그리고 instruction tuning의 차이는 분명해보입니다.<br>\n",
    "일반적인 fine-tuning은 PLM을 특정 task 수행 능력에 특화시킵니다.<br>\n",
    "BART처럼 Pre-train 단계에서 여러 가지의 까다로운 pre-training objective 를 풀게 하여<br>\n",
    "서로 다른 복수의 downstream task에서 월등한 성능을 낸다하더라도<br>\n",
    "그 성능은 결국 풀고자 하는 특정 downstream task에 fine-tuning한 결과입니다.\n",
    "<br><br>\n",
    "prompting도 마찬가지입니다. GPT-3에서 제시한 few-shot은 결국<br>\n",
    "user가 의도적으로 prompting을 해야만 성능을 이끌어낼 수 있는 방법입니다.\n",
    "<br><br>\n",
    "instruction tuning은 서로 다른 종류의 downstream task를 instruction 형태의 데이터셋으로<br>\n",
    "한번에 fine-tuning하여<br>\n",
    "user가 prompt engineering을 할 필요 없이,<br>\n",
    "모델로 하여금 자연스럽게 user의 instruction에 따르는 답변을 내놓도록 하는 학습방법이라는 데 의의가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://arxiv.org/pdf/2109.01652.pdf](../Images/lec_25/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정리하면, FLAN의 의의는 GPT-3보다 더 작은 모델을 fine-tuning 하여<br>\n",
    "모델에게 **instruction following** 능력을 직접적으로 부여했다는 데 있습니다.<br>\n",
    "더 중요한 건, fine-tuning 때 학습하지 않은 task에 대해서도<br>\n",
    "user의 instruction에 following하는 능력을 가지게 되었다는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instruction following은 뒤에서 살펴볼 emergent abilities 중 하나에 속합니다.<br>\n",
    "그렇다면 이 방법이 prompting보다 우월한 방법인 걸까요?\n",
    "<br><br>\n",
    "GPT-3는 앞서 말했듯이 175B이라는 어마어마한 크기의 모델입니다.<br>\n",
    "이런 LLM을 fine-tuning하는 건 비용과 효율면에서 좋은 시도라고 보기 어렵습니다.<br>\n",
    "FLAN 연구진이 instruction tuning을 하기 위해 사용한 LaMDA가 GPT-3보다 작은 모델이라곤 하지만<br>\n",
    "137B도 결코 작다고 할 수 없는 큰 사이즈의 LLM 입니다.\n",
    "<br><br>\n",
    "우리는 처음에 LLM이 소규모 LM에서 발견되지 않는 우수한 성능의 emergent abilities를 갖고 있다는 전제에서 출발했습니다.<br>\n",
    "이 말의 의미는 아주 많은 파라미터로 엄청난 양의 데이터를 단순히 auto-regressive 한 방법으로 학습시켰을 때\n",
    "모델이 어떤 의미에서는 인간이 명시적으로 가르쳐주지 않았음에도\n",
    "스스로 탁월한 언어이해(NLU)와 언어생성능력(NLG)을 가지게 되었다는 걸 뜻하는 것 같습니다.<br>\n",
    "마치 사람이 하는 것처럼 생각하는 방법을 터득한 것처럼 말이죠.\n",
    "<br><br>\n",
    "우리는 종종 \"의식의 흐름\" 대로 글을 썼다거나 말을 한다는 표현을 하곤 합니다.<br>\n",
    "chain of thought, 말 그대로 \"생각의 흐름\"과 같은 방식으로 prompting 했을 때,\n",
    "모델이 그 흐름 끝에 나올법한 답변을 내놓는 기술에 관한 이야기를 해보고자 합니다.<br>\n",
    "바로 chain-of-thought prompting 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Chain-of-Thought Prompting**\n",
    "\n",
    "앞서 살펴본 instruction tuning은 few-shot learning과 fine-tuning의 하이브리드라고 볼 수도 있습니다.<br>\n",
    "Google 연구진들은 \"chain-of-thought prompting\" 이라는 개념을 제안한 PaLM 논문에서<br>\n",
    "LLM의 규모를 극한으로 몰아붙였을 때, few-shot 능력이 얼마나 상승하게 될지를 실험했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PaLM](https://arxiv.org/pdf/2204.02311.pdf)논문에서\n",
    "연구진들은 무려 540B에 달하는 파라미터를 탑재한 디코더 기반 트랜스포머 모델을 사용했습니다.<br>\n",
    "더불어 소셜 미디어 대화, 필터링된 웹 페이지, 책, Github, 다국어 Wikipedia 및 뉴스에서 가져온<br>\n",
    "자그마치 780B 개의 토큰을 가지고 pre-train을 했습니다.<br>\n",
    "('자그마치'라고는 했지만 앞선 스텝에서 잠시 언급한 LLaMA는 PaLM의 1/8 수준인 65B 크기의 모델로<br>\n",
    "PaLM이 학습한 토큰의 두배를 넘는 1.4T 개의 토큰을 학습하여 훨씬 더 좋은 성능을 냈다는 사실을 기억해주세요.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LM이 풀기 가장 까다로운 문제 중에 하나는 다단계의 추론을 통해 답변을 도출해내야 하는 형식의 문제입니다.<br>\n",
    "2021년 DeepMind에서 발표한 LLM인 Gopher에서는 단순히 모델 사이즈를 키우는 것만으로<br>\n",
    "논리적, 수학적 추론이 필요한 문제를 잘 풀어내기가 어렵다고 토로하고 있습니다. [참고](https://arxiv.org/pdf/2112.11446.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PaLM은 280B 짜리 모델인 Gopher 보다 2배 이상 큰 모델입니다.<br>\n",
    "Deepmind와 달리 Google의 연구진은 \"chain-of-thought\" (이하 CoT) 이라는 prompting 기법을 PaLM에 사용하여<br>\n",
    "Gopher가 어려움을 겪은 Multi-step reasoning에서 좋은 성능을 낼 수 있음을 보여주었습니다.<br>\n",
    "Multi-step reasoning 문제의 대표적인 예로는<br>\n",
    "Arithmetic reasoning(산술추론)과 Commonsense reasoning(상식추론) 이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "산술추론은 아래 그림의 왼쪽 예제처럼 두 단계 이상의 추론을 거쳐야 풀 수 있는 산술문제를 가리킵니다.<br>\n",
    "상식추론은 우리가 살고 있는 세계에 대한 일반지식으로 적절히 추론해야 하는 문제를 가리킵니다.<br>\n",
    "예컨대 \"집에 서둘러 가려고 했지만 신호등이 노란색으로 바뀌었다. 어떻게 해야할까?\" 라는 질문에<br>\n",
    "모델은 (a) \"기다리십시오\" (b) \"무단횡단을 하십시오\" (c) \"바다로 가십시오\" 등의 옵션에서<br>\n",
    "하나를 선택해야 하는 문제를 풀어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 chain-of-thought prompting 의 원리는 이렇습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://arxiv.org/pdf/2204.02311.pdf](../Images/lec_25/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림의 왼쪽에서 볼 수 있듯이, LLM에 일반적인 few-shot learning을 시키면 모델이 잘못된 답을 내곤 합니다.<br>\n",
    "그러나 모델에게 prompt를 줄 때 문제에 대한 답을 바로 주는 게 아니라<br>\n",
    "문제를 푸는데 필요한 사고과정을 함께 준 뒤 유사한 문제를 풀게 시키면<br>\n",
    "놀랍게도 그 문제에 대한 답만 맞추는 게 아니라, 자신이 풀이한 과정까지 답변에 포함시켜 돌려주는 걸 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain-of-thought prompting의 의의는, 모델의 추론결과를 토대로 오류분석이 가능해지고,<br>\n",
    "모델이 왜 그렇게 추론했는지에 대한 해석가능성을 높일 수 있다는 데 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PaLM과 같은 LLM에 chain-of-thought prompting을 했을 때<br>\n",
    "얼마나 놀라운 언어능력이 emergent 되는지 엿볼 수 있는 사례 하나를 논문에서 인용해보겠습니다.<br>\n",
    "모델에게 농담하나를 들려주고,<br>\n",
    "그것이 왜 농담으로 해석될 수 있는지에 대한 설명과 함께, 농담으로 해석될 수 있는 문장을 input으로 주었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I will explain these jokes: \"Always borrow money from a pessimist. They’ll never expect it back.\"<br><br>\n",
    "> Explanation: Most people expect you to pay them back when you borrow money,\n",
    "however a pessimist is someone who always assumes the worst,\n",
    "so if you borrow money from them, they will expect that you won't pay them back anyways.<br><br>\n",
    "> Input: I tried 10,000 random restarts of my neural network,\n",
    "but I was accused of overfitting. I guess no good seed goes unpunished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가히 놀라운 언어 이해 능력입니다.<br>\n",
    "언어모델을 만들어보신 분들은 LM이 사람의 농담을 이해하고 그 농담에 대한 해석까지 해줄 수 있게 만드는 것이<br>\n",
    "얼마나 어려운 일인지 아실텐데요, 심지어 저 농담은 신경망 학습과 오버피팅의 의미, 그리고 언어유희까지 이해했을 때야<br>\n",
    "비로소 농담으로 해석될 수 있는 수준 높은 농담이기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이밖에도 PaLM은 번역, 요약, 질문답변 등 주요 NLP task에서 이전 모델들을 넘어섰고,<br>\n",
    "다국어로 해당 task를 수행했을 때도 더 좋은 성능을 냈습니다.<br>\n",
    "이렇게 강력한 LLM이다보니 PaLM은 윤리적으로 문제가 될 수 있는 문장역시 더 잘 생성해냈고,<br>\n",
    "연구진들은 이에 대해 추가 연구의 필요성을 언급했습니다.<br>\n",
    "(이 문제는 step4에서 좀 더 깊이 있게 다뤄보겠습니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떠신가요?\n",
    "<br><br>\n",
    "LLM의 instruction following과 multi-step reasoning 능력에 대해 감을 좀 잡으셨나요?<br>\n",
    "그런데 PaLM의 CoT를 보면서 이런 궁금증이 들지는 않으셨나요?<br>\n",
    "**\"PaLM 같은 540B 사이즈 전후의 초거대 모델에서만 CoT가 먹히는 걸까?\"**<br>\n",
    "**\"그보다 훨씬 작은 모델에서 CoT prompting은 불가능한 것일까?\"**<br>\n",
    "<br>\n",
    "다음 스텝에서 이제 본격적으로 LLM의 emergent abilities 에 대해 알아보면서 위 질문에 답을 찾아보도록 합시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **25-3. LLM + Emergent Abilities = AGI?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Emergent Abilities의 정의**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emergence(창발)이란 철학과 과학에서 오랜 역사를 지닌 복잡한 개념입니다. [(참고)](https://en.wikipedia.org/wiki/Emergence)\n",
    "<br><br>\n",
    "위키에서 볼 수 있듯이 창발에 대한 수많은 정의와 해석이 있지만<br>\n",
    "우리는 노벨물리학상 수상자인 Philip Anderson가 1972년에 발표한 에세이<br>\n",
    "\"More Is Different\" 에서 정의한 창발의 개념을 가지고 접근해보겠습니다.\n",
    "<br><br>\n",
    "**\"Emergence is when quantitative changes in a system result in qualitative changes in behavior.\"**\n",
    "<br><br>\n",
    "번역하자면 **\"Emergence(창발)은 시스템에서의 양적변화가 질적변화를 가져올 때를 의미한다\"** 정도가 되겠네요.\n",
    "<br><br>\n",
    "우리는 앞선 스텝에서<br>\n",
    "파라미터 스케일의 급진적인 변화가 가져온 모델 추론 능력의 질적인 변화의 예시들을 살펴보았습니다.<br>\n",
    "주요 LLM을 개발해온 Google과 DeepMind 그리고 유수 대학 연구자들은<br>\n",
    "2022년 10월 [Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682.pdf) 이라는 논문을 발표했습니다.\n",
    "<br><br>\n",
    "연구진은 LLM의 Emergent Abilities를 소규모 모델에는 없지만 대규모 모델에는 존재하는 능력으로 정의합니다.<br>\n",
    "따라서 Emergence는 모델 파라미터 수로 측정된 모델 규모와 관련지어 집니다.\n",
    "<br><br>\n",
    "아래 그림은 task별 모델의 성능과 모델 파라미터 개수 사이의 관계에서\n",
    "Emergence가 나타나는 패턴을 보여주는 그래프입니다.\n",
    "모두 단순한 few-shot prompting 에서의 Emergence를 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://arxiv.org/pdf/2206.07682.pdf](../Images/lec_25/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 : A, B, C, D 는 BIG-Bench, (출처 : https://github.com/google/BIG-bench)\n",
    "E는 TruthfulQA benchmark,<br>\n",
    "F는 Grounded conceptual mappings,<br>\n",
    "G는 Massive Multi-task Language Understanding (MMLU) benchmark,<br>\n",
    "H는 (WiC) benchmark 입니다.<br>\n",
    "F에 관해선 [논문 Mapping Language Models To Grounded Conceptual Spaces](https://openreview.net/pdf?id=gJcEM8sxHK)을 참고하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델 크기가 특정 임계값을 넘어서는 순간 모델 performance가 확연히 달라지는 걸 볼 수 있습니다.**<br>\n",
    "우리는 이 패턴을 일종의 scaling law로 해석해볼 수 있습니다. 무어의 법칙처럼 말이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://arxiv.org/pdf/2206.07682.pdf](../Images/lec_25/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림은 단순 few-shot prompting이 아니라 Chain of Thought(CoT) 같은 좀더 고급의 prompt engineering을 사용하거나<br>\n",
    "instruction tuning 같은 고급의 fine-tuning 기법을 썼을 때 task별 performance의 Emergence 패턴을 나타낸 그림입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 : A는 PaLM의 CoT,\n",
    "B는 FLAN의 instruction tuning,\n",
    "C는 scratch pad 기법을 쓴 8자리 숫자 연산 task,\n",
    "D는 model calibration 에서의 Emergence 입니다.<br>\n",
    "C와 D는 각각<br>\n",
    "[논문 Show Your Work: Scratchpads For Intermediate Computation With Language Models](https://arxiv.org/pdf/2112.00114.pdf)과<br>\n",
    "[논문 Language Models (Mostly) Know What They Know](https://arxiv.org/pdf/2207.05221.pdf)을 참고하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 살펴본 그래프와 마찬가지로,<br>\n",
    "모델 크기가 일정 수준을 넘어야만 해당 고급 기법들을 썼을 때 Emergent abilities가 나타나는 걸 볼 수 있습니다.\n",
    "<br><br>\n",
    "그림의 A와 B는 각각 이전 스텝에서 살펴본 PaLM의 CoT에 의한 Multi-step reasoning 능력과<br>\n",
    "Instruction following 능력에 해당됩니다.<br>\n",
    "C와 D도 위에 말씀드린 논문을 읽어보시길 권합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. LLM + Emergent Abilities = AGI?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Emergent Abilities에 관해 더 생각해볼 만한 문제들**\n",
    "\n",
    "여기까지 보면 LLM의 Emergent abilities 라는게 몇 개 되지 않아 보입니다.<br>\n",
    "하지만 Emergent abilities를 보이는 LLM들이 아직 human performance에 준하는 성능에 한참 미달하는,<br>\n",
    "다시 말해 잘 풀지 못하는 task들이 즐비하다는 사실을 간과해선 안됩니다.<br>\n",
    "Big Bench에는 GPT-3나 PaLM이 힘을 못쓰는 task들이 수십 개나 남아 있습니다.<br>\n",
    "몇가지만 예를 들어보면, anachronisms, formal fallacies syllogisms negation, mathematical induction 등등,<br>\n",
    "인간에게도 쉽지 않은 task들이지요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고\n",
    "- [BIG BENCH](https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/README.md)\n",
    "\n",
    "> BIG-bench란 The Beyond the Imitation Game Benchmark 의 준말로\n",
    "LLM을 위한 벤치마크들을 총망라해 놓은 문제집\n",
    "\n",
    "- [**What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?**](https://arxiv.org/pdf/2204.05832.pdf)<br><br>\n",
    "- [**Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning**](https://arxiv.org/pdf/2106.09226.pdf)\n",
    "\n",
    "- [Emergent abilities](https://arxiv.org/pdf/2206.07682.pdf)\n",
    "- [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf)\n",
    "- [A General Language Assistant as a Laboratory for Alignment](https://arxiv.org/pdf/2112.00861.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **이제는 AGI의 시대?**\n",
    "\n",
    "이처럼 LLM의 Emergent abilities에 관해 아직 해결되지 않은 문제들이 많음에도 불구하고,<br>\n",
    "모델 파라미터의 적극적인 스케일링을 통한 LLM의 확장은 한가지 중요한 시사점을 우리에게 던져주고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바로 훈련 데이터에 명시적으로 인코딩되지 않은 다양한 task를<br>\n",
    "단일 모델이 수행해낼 수 있는 수준까지 딥러닝 모델이 발전했다는 사실입니다.<br>\n",
    "Emergent Abilities를 \"unseen task를 수행해내는 능력\" 이라고 좁게 해석한다면,<br>\n",
    "우리가 지금까지 살펴본 LLM들을 \"범용\" 모델이라고 부를 수 있을 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이로부터 촉발된 수많은 후속 연구들은 ASI(Artificial Super Intelligence)로 넘어가기 위해<br>\n",
    "반드시 넘어야 하는 AGI(Artificial General Intelligence)의 시대에 접어들었다고 볼 수 있을 법한 결과물들을 담아내고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "관련하여 몇 가지 최신 연구결과물들을 소개해드리겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Robotics and Virtual agent**\n",
    "\n",
    "우리가 prompt라는 이름으로 natural language instructions을 모델에게 주었을 때,<br>\n",
    "단순히 컴퓨터 모니터 상에서 Emergent abilities를 보여주는 데서 더 나아가<br>\n",
    "로봇이나 가상환경의 아바타가 우리의 instruction에 따라 행동할 수 있도록 하는 연구들이 진행되고 있습니다.<br>\n",
    "이와 관련한 대표적인 논문 2개를 소개합니다.<br>\n",
    "\n",
    "- [Do As I Can, Not As I Say: Grounding Language in Robotic Affordances](https://arxiv.org/pdf/2204.01691.pdf)\n",
    "- [Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents](https://arxiv.org/pdf/2201.07207.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2. Interact with users**\n",
    "\n",
    "우리가 입력한 대로만 결과물을 출력하는 모델이 아니라,<br>\n",
    "실시간으로 대화를 주고 받으며 작문을 도와주는 모델 에디터처럼<br>\n",
    "인간과 상호작용하며 결과물을 만들어내는 모델에 관한 연구도 진행되고 있습니다.<br>\n",
    "이와 관련한 Google Research의 대표적인 논문 3개를 소개합니다.<br>\n",
    "\n",
    "- [Wordcraft: a Human-AI Collaborative Editor for Story Writing](https://arxiv.org/pdf/2107.07430.pdf)\n",
    "- [AI Chains: Transparent and Controllable Human-AI Interaction\n",
    "by Chaining Large Language Model Prompts](https://arxiv.org/pdf/2110.01691.pdf)\n",
    "- [PromptChainer: Chaining Large Language Model Prompts\n",
    "through Visual Programming](https://arxiv.org/pdf/2203.06566.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3. Facilitate multi-modal reasoning**\n",
    "\n",
    "텍스트 데이터 뿐만 아니라, 비주얼 데이터 입출력도 가능한<br>\n",
    "이른바 시각적 언어모델 VLM (visual-language models)에 관한 연구도 있습니다.<br>\n",
    "이와 관련한 대표적인 논문 2개를 소개합니다.<br>\n",
    "\n",
    "- [Google의 Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/pdf/2204.00598.pdf)\n",
    "- [Deepmind의 Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/pdf/2204.14198.pdf)\n",
    "\n",
    "개인적으로는 제목부터 도발적인 논문 \"Do As I Can, Not As I Say\"이 가장 인상적인 연구가 아닐까 싶습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LLM + Emergent Abilities = AGI?**\n",
    "\n",
    "우리는 지금까지 Emergent Abilities에 관해 더 생각해볼 만한 문제들과 후속 연구들까지 살펴봤습니다.<br>\n",
    "이제 다음과 같은 질문을 던져볼 차례입니다.\n",
    "<br><br>\n",
    "**\"더 많은 데이터를 더 큰 모델에 학습시키는 방법만이, Emergent abilities 를 확장할 수 있는 유일한 길일까?\"**\n",
    "<br><br>\n",
    "상대적으로 더 작은 LLM에서 더 놀라운 Emergence가 발견되었다는 보고가 여러 논문을 통해 발표되고 있습니다.<br>\n",
    "2022년, instruction tuning 방법을 소개한 FLAN 논문에서 쓴 모델은 LaMDA 137B였습니다.<br>\n",
    "상술했듯이 LaMDA는 디코더 기반 트랜스포머 모델입니다.<br>\n",
    "LaMDA는 pre-train시 2.49T 개의 토큰을 봤다고 합니다.<br>\n",
    "([FLAN 논문](https://arxiv.org/pdf/2109.01652.pdf) 2.4 Training Details 참고)<br>\n",
    "비슷한 시점에, 허깅페이스에서 발표한 T0 모델은<br>\n",
    "그보다 훨씬 적은 11B 개의 파라미터만 사용한 인코더-디코더 아키텍쳐의 트랜스포머 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 T0가 pre-train시 학습한 토큰양은 1T에 달합니다.<br>\n",
    "모델 사이즈는 1/10 넘게 줄었지만 데이터 사이즈는 1/2 줄어든 것이죠.<br>\n",
    "그런데 T0는 baseline으로 사용했던 11B보다 더 적은 3B 사이즈의 동일 아키텍쳐의 모델로<br>\n",
    "FLAN보다 더 나은 성능을 냈다고 보고했습니다.<br>\n",
    "([논문 Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/pdf/2110.08207.pdf) 참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gopher와 Chinchilla는 2022년 전후로 Deepmind에서 발표한 LLM입니다.<br>\n",
    "Chinchilla 70B의 경우 Gopher 280B 의 1/4 수준 크기의 모델이지만 연산량은 비슷합니다.<br>\n",
    "([논문]() 부록C 참고)\n",
    "Gopher 280B의 경우 pre-train시 325B 개의 토큰을 학습했지만<br>\n",
    "Chinchilla 70B는 1.4T 개의 토큰을 학습했다는 사실이 한 가지 주요한 원인이 될 수 있죠.<br>\n",
    "Chinchilla는 Gopher가 수행한 모든 downstream task에서 더 나은 performance를 냈습니다.<br>\n",
    "(참고로 Chinchilla는 Gopher의 모델 아키텍쳐를 거의 그대로 가져왔고<br>\n",
    "토크나이저와 옵티마이저, 연산단위 및 데이터셋 구성만 약간 바꿔 훈련시킨 모델입니다)\n",
    "<br><br>\n",
    "또다른 예로 PaLM 62B 모델은 GPT-3 175B 및 LaMDA 137B에 비해<br>\n",
    "1/3에서 1/2 수준으로 작은 사이즈의 모델임에도 불구하고<br>\n",
    "자연어 이해(NLU) 및 자연어 생성(NLG)에서 GPT-3보다 나은 성능을 냈습니다.<br>\n",
    "(PaLM 논문 Appendix H1 참고)<br>\n",
    "여기서 PaLM 62B와 GPT-3가 pre-train 시 사용한 토큰 총량은 각각 795B, 300B개 였습니다.\n",
    "<br><br>\n",
    "Data size가 Model size보다 더 중요할 수 있다는 사실을 지적한<br>\n",
    "Deepmind의 논문 [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf) 은<br>\n",
    "다른 관점의 scailing law를 제시합니다.\n",
    "<br><br>\n",
    "데이터를 추가할 때 얻을 수 있는 이득은 엄청난 반면,<br>\n",
    "모델 크기를 키웠을 때 이득은 미미하다는 것이죠.\n",
    "<br><br>\n",
    "아래 그림은 Chinchilla 논문의 주요 내용을 다룬 블로그에 있는 그래프 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications](../Images/lec_25/16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인간의 지능을 구현하는 뇌 용량도 결국은 뉴런과 시냅스의 개수로 유한히 정해져 있습니다.<br>\n",
    "우리 대부분은 물리적으로 같은 뇌를 가지고 태어났지만,<br>\n",
    "어떤 정보를, 얼마나 많이, 어떻게 학습했느냐가 우리의 지능 수준을 더 좌우하는 것처럼 보입니다.\n",
    "<br><br>\n",
    "즉 모델 크기를 고정했을 땐, 데이터의 양뿐만 아니라 질도 큰 영향을 미칠 수 있다는 것일텐데<br>\n",
    "이러한 논리는 LLM에도 비슷하게 적용되는 것 같습니다.\n",
    "<br><br>\n",
    "**고품질 데이터로 훈련된 모델일 경우 모델 파라미터가 더 적어져도 Emergence가 나타날 수 있을까요?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI에서 발표한 chatGPT의 전신인 InstructGPT는<br>\n",
    "GPT-3와 동일한 아키텍쳐를 베이스 모델로 하여<br>\n",
    "파라미터 스케일을 1.3B, 6B, 175B 세가지 버젼으로 달리해 구현한 모델입니다.<br>\n",
    "그 중 가장 작은 1.3B InstructGPT가 생성해낸 문장과<br>\n",
    "175B GPT-3가 생성해낸 문장을 두고 실제 인간의 선호도를 조사한 결과,<br>\n",
    "사람은 InstructGPT가 생성해낸 문장을 더 선호했습니다.<br>\n",
    "([논문 Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf) 참고)\n",
    "<br><br>\n",
    "InstructGPT에서 눈여겨 봐야할 점은<br>\n",
    "1.3B 짜리 GPT 모델이 pre-train시 300B개의 토큰 밖에 보지 않았음에도(LaMDA와 Chinchilla, PaLM과 비교해보세요)\n",
    "SOTA의 기준이었던 GPT-3보다 좋은 성능을 냈다는 사실입니다.<br>\n",
    "**무엇이 이걸 가능하게 했을까요?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "InstructGPT에서 사용한 학습메커니즘의 핵심은<br>\n",
    "LLM + RLHF(Reinforcement Learning Human Feedback)에 있다고 분석됩니다.<br>\n",
    "그리고 RLHF의 핵심 중 하나는 모델이 \"데이터의 품질을 스스로 판별하여\"<br>\n",
    "더 나은 결과물을 만들어낼 수 있게 하는 것입니다.<br>\n",
    "InstructGPT가 다른 LLM과 다른 점이 바로 여기에 있습니다.\n",
    "<br><br>\n",
    "이를 위해선,<br>\n",
    "첫째로 고품질 데이터와 저품질 데이터가 구분되어야 하고,<br>\n",
    "둘째로 그 구분을 스스로 학습해낼 수 있는 메커니즘이 필요했을 것입니다.<br>\n",
    "즉, 모델 스케일이나 데이터의 절대적인 크기가 Emergent Abilities의 전부가 아니라,<br>\n",
    "어떤 데이터를 어떻게 학습시켰느냐 라는 문제의식이 여전히 강력하게 유효하다는 걸 여실히 보여준다고 말할 수 있습니다.<br>\n",
    "InstructGPT가 RLHF를 할 때 사용한 데이터는 11만 건 정도밖에 되지 않습니다.\n",
    "([InstructGPT 논문](https://arxiv.org/pdf/2203.02155.pdf) Appendix A.3 Dataset sizes 참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "물론 모델 사이즈와 관련해서,<br>\n",
    "\"현재 수준의 모델 사이즈는 앞으로도 필요할 것이다\" 내지<br>\n",
    "\"앞으로 모델 사이즈는 더 커져야 할 것이다\" 라는 의견이 있을 수 있습니다.<br>\n",
    "이러한 직관은 일견 일리가 있어 보입니다.<br>\n",
    "예를 들어, 앞서 살펴본 Multi-step reasoning 능력의 경우,<br>\n",
    "n단계의 순차적 추론이 필요한 경우 최소 O(n) 레이어의 깊이가 있는 모델이 필요할 수 있습니다.\n",
    "<br><br>\n",
    "commonsense reasoning 처럼 우리가 살고 있는 세계에 대한 상식에 관한 추론의 경우,<br>\n",
    "인간에게도 일정 수준 이상의 암기력은 필요하고<br>\n",
    "모델 역시 일정 수준 이상의 레이어는 최소요건으로 충족시켜야 한다고 볼 수 있을테니까요.<br>\n",
    "하지만 위 질문에 대한 답은 현재 굉장히 논쟁적인 사안입니다.<br>\n",
    "확실히 우리는 모델이 얼마나 더 커질 수 있고, 그렇게 거대한 모델이 어떤 능력을 가질 수 있을지 그 끝을 보지 못했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "commonsense reasoning 처럼 우리가 살고 있는 세계에 대한 상식에 관한 추론의 경우,<br>\n",
    "인간에게도 일정 수준 이상의 암기력은 필요하고<br>\n",
    "모델 역시 일정 수준 이상의 레이어는 최소요건으로 충족시켜야 한다고 볼 수 있을테니까요.<br>\n",
    "하지만 위 질문에 대한 답은 현재 굉장히 논쟁적인 사안입니다.<br>\n",
    "확실히 우리는 모델이 얼마나 더 커질 수 있고, 그렇게 거대한 모델이 어떤 능력을 가질 수 있을지 그 끝을 보지 못했습니다.\n",
    "<br><br>\n",
    "조금 더 조심스럽게 접근하자면<br>\n",
    "LLM의 Emergent abilities는 위에서 살펴본 요소들 외에<br>\n",
    "아직까지 발견되지 않은 미지의 요소로 나타난다고 가정할 수도 있습니다.<br>\n",
    "완전히 최적화된 훈련방법이란 무엇인지,<br>\n",
    "최신 학습기법들로 학습시켰을 때 그 기법들이 최선의 학습기법이 맞는지는<br>\n",
    "시간에 따라 재해석 될 수 있기 때문입니다.<br>\n",
    "더 나아가 Deep learning model의 interpretability 문제도 영향을 미칠 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그러나 지금까지 살펴본 모든 LLM들의 기본 아키텍쳐인 트랜스포머도,**<br>\n",
    "**더 긴 시퀀스를 더 효과적으로 처리해 낼 수 있는 셀프 어텐션 아키텍쳐에 대한 고민이 없었다면 탄생할 수 없었습니다.**<br>\n",
    "**PaLM과 같은 초거대 모델을 학습시키는 데에는,**<br>\n",
    "**대량의 데이터나 모델 파라미터를 키우는 아키텍쳐보다**<br>\n",
    "**분산컴퓨팅을 위한 AI backend Engineering 기술에 대한 고민이 선행되어야 합니다.**<br>\n",
    "\n",
    "**무엇보다도, 소수 연구자와 기술자의 전유물이 아닌**<br>\n",
    "**더 많은 사람들이 기술발전의 혜택을 누릴 수 있도록 모델 사이즈를 더 줄이면서도 비슷한 성능을 낼 수 있으면서,**<br>\n",
    "**대중 일반이 사용할 수 있는 컴퓨팅 환경에서 실행가능한 학습 테크닉을 개발하고자 하는 고민에서 비롯된**<br>\n",
    "**목소리와 움직임들이 있습니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편으론 이런 고민과 시도들이 우리 인간이 가진 Emergent abilities가 아닌가 하는 생각이 듭니다.<br>\n",
    "이런 고민과 시도까지 해낼 수 있는 인공지능이라면 AGI라는 이름을 붙여도 되지 않을까 조심스레 생각해봅니다.<br>\n",
    "\n",
    "다행히(?)도 OpenAI에서 ChatGPT를 AGI의 효시로 내걸어주었습니다.<br>\n",
    "([Planning for AGI and beyond](https://openai.com/blog/planning-for-agi-and-beyond) 참고)<br>\n",
    "\n",
    "OpenAI의 말대로 ChatGPT를 통해 우리가 AGI에 점점 다가가고 있는 것이라면,<br>\n",
    "**지금까지 살펴본 수많은 LLM중 ChatGPT에 대해 좀 더 자세히 알아볼 필요가 있지 않을까요?**<br>\n",
    "\n",
    "그럼 이어지는 다음 스텝에서 ChatGPT의 핵심 기술인 RLHF에 대해 좀 더 자세히 살펴보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고\n",
    "- 모델이 작아졌을 때, 더 많은 데이터를 학습시키면 계산비용에서 큰 이득이 없지 않을까?\n",
    "> 모델 훈련 단계에서는 그렇다\n",
    "> 모델 추론 단계에서는 얘기가 다르다\n",
    ">> 더 작은 모델이 큰 모델과 동일한 성능을 내는 경우, 추론 시 계산비용이 훨씬 낮아진다.\n",
    "\n",
    ">>> 만약 수 천억개 이상의 파라미터를 가진 AI 제품을 배포하고 서비스 제공하는 경우, 모델 추론 단계에서 컴퓨팅 비용이 개발 비용을 훌쩍 초과해버릴 수도 있다.\n",
    "\n",
    "실제로 포브스의 2023.02.10자 기사에 따르면 ChatGPT를 서비스하는 컴퓨팅 비용이 정기적으로 수백만 달러라고 한다.<br>\n",
    "(출처 : https://www.forbes.com/sites/johnkoetsier/2023/02/10/chatgpt-burns-millions-every-day-can-computer-scientists-make-ai-one-million-times-more-efficient/?sh=4fc18ed6944e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **25-4. InstructGPT : RLHF, 언어모델과 강화학습의 만남**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RLHF의 탄생배경**\n",
    "\n",
    "모든 일이 그렇듯, 어느날 갑자기 없던 게 생겨나진 않습니다.<br>\n",
    "우리가 보는 건 땅 위에 싹이 불쑥 튀어 나와 하루가 다르게 쑥쑥 자라나는 모습이지만,<br>\n",
    "싹이 트기 바로 직전까지 땅속에는 무수한 씨앗들이 꿈틀대고 있었을 테니까요.\n",
    "<br><br>\n",
    "RLHF (Reinforcement Learning Human Feedback) 에도 물론 히스토리가 있습니다.<br>\n",
    "그 전에, 눈에 보이지 않는 땅속에서 꿈틀거릴 수 있었던 저 씨앗들을 누가 뿌렸고,<br>\n",
    "그보다 앞서 저 씨앗들이 어디서부터 왔는지를 잠시 살펴보겠습니다.\n",
    "<br><br>\n",
    "ChatGPT라는 혁신적인 AI 모델이 지난 2022년 겨울 우리에게 찾아오기 훨씬 전부터<br>\n",
    "LLM이 풀지 못했던 커다란 숙제가 하나 있었습니다.<br>\n",
    "바로 알고리즘의 편향 또는 편향된 알고리즘 문제가 바로 그것입니다.\n",
    "<br><br>\n",
    "LLM은 어마어마한 양의 토큰을 먹어치웁니다.<br>\n",
    "2023년 초에 발표된 가장 최신 모델인 GPT-4와 비슷한 시점에 공개된 Meta의 LLaMA는<br>\n",
    "1.4T 개의 토큰을 학습했다고 1-2 스텝에서 말씀드린 바 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://arxiv.org/pdf/2302.13971.pdf](../Images/lec_25/17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림에서 보듯이 CommonCrawl 데이터셋이 전체 데이터의 절반이 훨씬 넘습니다.<br>\n",
    "좀 더 정제된 C4까지 합치면 전체 구성의 80%를 넘게 차지하네요.<br>\n",
    "물론 위키나 서적, 연구 논문이나 기술 블로그의 내용들에도 정치적, 학문적, 문화적, 과학적 편향이 들어갈 수 있지만<br>\n",
    "원시 웹페이지에서 긁은 텍스트 데이터에 깃들어 있을 대중적 편향은 훨씬 그 기울기가 가파를 수밖에 없습니다.\n",
    "<br><br>\n",
    "편향을 둘러싼 예를 들자면 수도 없이 많습니다.<br>\n",
    "어텐션 레이어에서 \"CEO\"라는 토큰과 \"남성\"이라는 토큰에 높은 관련성을 부여하게 되거나<br>\n",
    "\"요리\", \"청소\" 같은 집안일을 \"여성\"과 결부시켜 학습하는 식의 이야기들이 모두 해당됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알고리즘 편향에 대해 관심이 있으신 분들은<br>\n",
    "1-3 스텝의 Q8에 말씀드린 [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf)논문의<br>\n",
    "5절 society 이하 5.1.2 Harms의 Intrinsic biases 문단을 참고해보세요.\n",
    "<br><br>\n",
    "참고로 임베딩의 편향성을 제거하는 인상적인 시도를 연구한 논문도 소개해 드립니다.\n",
    "<br><br>\n",
    "상용화된 foundation model이 이런 편향을 간직한채로 서비스된다면<br>\n",
    "실생활과 밀접한 연관이 있는 고용, 대출 같은 영역에서 불공정한 결과가 초래될 수 있다는 점에서<br>\n",
    "알고리즘 편향 문제는 굉장히 심각하고, 진지하게 다뤄지는 주제입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고\n",
    "- 알고리즘의 편향을 필터링하거나 제거할 수 있는 방법을, 지도학습 기반의 단일 딥러닝 모델에 손실함수화할 수 있을까\n",
    "> 대부분의 LM은 Cross Entropy 손실함수를 사용해 next token을 예측하는 방식을 사용\n",
    "> 그리고 BLEU나 ROUGE 같은 평가 메트릭으로 모델의 추론결과를 사후 보정\n",
    "> 이 프로세스가 RLHF 이전에 대부분의 LM이 알고리즘 편향을 극복하기 위해 채택했던 주된 방법론\n",
    "\n",
    "> 이런 방식은, 학습단계에서 모델이 데이터의 편향을\n",
    "고스란히 학습할 수밖에 없다는 한계를 여전히 짊어지고 있다.\n",
    "> 다시 말해, BLEU나 ROUGE 처럼, 인간의 선호가 반영된 메트릭의 사용이 loss function의 계산결과 자체에 여전히 영향을 주진 못한다\n",
    "\n",
    "> 바꿔 말해, 고전적인 지도학습 기반의 단일 foundation model만으로는\n",
    "자신이 만들어낸 문장, 음성, 이미지, 영상 자체가 HHH 기준을 충족하는지 결코 자각할 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 윤리적으로 적절한지,<br>\n",
    "유해한 정보를 창발해내고 있는지를 손실함수 단계에서 걸러낼 수 없다면<br>\n",
    "모델의 훈련루프에 인간이 직접 참여하지 않을 이유가 없다...<br>\n",
    "이것이 RLHF라는 씨앗의 기원입니다.<br>\n",
    "그리하여 OpenAI는 라벨러들을 고용해 인간이 직접 그 씨앗을 모델에게 뿌리는 방법을 선택합니다.\n",
    "<br><br>\n",
    "RLHF는 OpenAI와 Deepmind의 연구자들이 2017년에 발표한<br>\n",
    "[Deep Reinforcement Learning from Human Preferences](https://arxiv.org/pdf/1706.03741.pdf)논문에서 처음 소개되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Video] Human Feedback training process](https://youtu.be/oC7Cw3fu3gU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영상에서처럼 위 논문은<br>\n",
    "Reinforce Learning을 사용해<br>\n",
    "가상환경에서 에이전트에게 backflip을 가르치는 과정에서<br>\n",
    "human feedback을 주는 게 목적이었습니다.<br>\n",
    "피드백을 주는 인간에게는 agent(모델)의 action(행동)에 대한 두 가지 옵션이 제공되고<br>\n",
    "목표달성에 가장 가까운 옵션을 선택해 피드백을 줍니다.<br>\n",
    "(즉 인간의 피드백이 environment에 속하게 됩니다)<br>\n",
    "human feedback 없이 순전한 강화학습만으로 에이전트를 학습시킬 때보다 훨씬 효과적이라는 걸 입증한 논문이었죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 논문에 뒤이어 2019년에 발표된 [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/pdf/1909.08593.pdf)논문에서는<br>\n",
    "강화 학습을 텍스트 요약task를 수행하는 PLM에 적용했습니다. ([깃헙 참고 링크](https://github.com/openai/lm-human-preferences))<br>\n",
    "좀 더 상술하면 TRL(Transformer Reinforce Learning) 을 구현하기 위해 요약 레이블과 ROUGE,<br>\n",
    "두 메트릭을 사용하여 강화학습 알고리즘인 PPO (Proximal Policy Optimization)를 적용할 수 있다는 걸 보여준 논문이었습니다.\n",
    "<br><br>\n",
    "그 뒤를 이어 2020년에 발표된 [Learning to summarize from human feedback](https://arxiv.org/pdf/2009.01325.pdf)논문에서는<br>\n",
    "앞선 논문에서 사용한 두 메트릭에 인간의 선호도가 충분히 반영될 수 없다는 문제점을 지적하며<br>\n",
    "인간이 직접 품질을 비교한 데이터셋을 바탕으로<br>\n",
    "인간의 선호도를 학습시킨 모델 자체를<br>\n",
    "강화학습의 보상함수(reward function)로 사용할 수 있다는 아이디어를 제시합니다. ([깃헙 참고 링크](https://github.com/openai/summarize-from-feedback))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**바로 이 부분이 핵심입니다.**<br>\n",
    "일반적인 강화학습 시스템에서는 보상함수가<br>\n",
    "human feedback과 무관한 environment에 혹은 강화학습 알고리즘 내에 결정되어 있습니다.<br>\n",
    "agent가 자신의 action을 교정하는 방법이<br>\n",
    "envrionment 내지 RL algorithm에서 결정되는 action에 대한<br>\n",
    "reward와 state변화에 의존적이라는 것입니다. (아래 그림의 푸른색 박스)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![reward predictor](../Images/lec_25/18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 RLHF에서는 보상함수가 envirionment/RL algorithm 에서 분리되어 있습니다.<br>\n",
    "나아가 그 보상함수는 human feedback을 학습한 별도의 모델로 존재합니다.<br>\n",
    "그리고 agent(모델)은 보상 모델이 주는 reward로 action을 개선해나가는 것이죠. (위 그림의 붉은색 박스)<br>\n",
    "따라서 Learning to summarize from human feedback 논문의 LM은<br>\n",
    "입력 데이터가 편향되었다 하더라도 알고리즘 편향없이<br>\n",
    "human feedback이 의도하는 품질의 요약을 수행하게 되는 것입니다.\n",
    "<br><br>\n",
    "위 그림과 아래 그림을 비교해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://arxiv.org/pdf/2009.01325.pdf](../Images/lec_25/19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1단계** : Collect human feedback 에서 원문과 복수의 요약 쌍으로 이뤄진 데이터셋에 대해,<br>\n",
    "인간이 직접 고품질 요약과 저품질 요약으로 분류합니다.<br>\n",
    "하나의 원문에 대해 j와 k요약이 있을 때, \"j is better than k\"처럼 말이죠.\n",
    "<br><br>\n",
    "**2단계** : Train reward model 에서 원문과 그에 상응하는 분류된 요약쌍들을<br>\n",
    "RM (reward model)에 넣어 각각의 reward를 계산하고<br>\n",
    "RM이 각 요약의 (human feedback이 반영된)품질을 매길 수 있도록 학습시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12. 위 그림의 빨간 박스 안에 있는 손실함수의 공식이 의미하는 바가 무엇일까요?**<br>\n",
    "**논문에 나와있는 수식과 설명을 읽어보고, 수식을 해석해 아래 퀴즈블럭에 적어보세요.**<br>\n",
    "**(hint : 시그모이드 함수 안의 왼쪽항에는 입력된 prompt에 대해 사람이 직접 작성한 답변이 들어갑니다.)**<br><br>\n",
    "\n",
    "**오른쪽항에 들어가는 것은 무엇일까요?**\n",
    "**왼쪽항에서 오른쪽항을 빼줬을 때 로그 함수는 어떻게 그려질까요?**\n",
    "\n",
    "---\n",
    "\n",
    "RM의 손실함수 (논문 [Learning to summarize from human feedback](https://arxiv.org/pdf/2009.01325.pdf)의 6p Reward models. 참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3단계** : Train policy with PPO 에서 LM(agent)에은 새로은 원문을 입력받습니다.<br>\n",
    "그리고 훈련된 RM을 사용해 LM이 생성해낸 요약이 고품질 요약이 되도록,<br>\n",
    "다시 말해 인간이 더 선호하는 요약이 되도록 LM을 강화학습시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![출처 : https://arxiv.org/pdf/2009.01325.pdf](../Images/lec_25/20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **25-5. GPT-4 vs LLaMA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **25-6. 마무리하며**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
