{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP Framework의 활용**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-1. 들어가며**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NLP 기술의 발전과 framework**\n",
    "\n",
    "---\n",
    "\n",
    "우리는 매일매일 빠르게 발전하고 있는 다양한 Modern NLP의 논문과 모델들을 확인할 수 있습니다.\n",
    "<br><br>\n",
    "이 모든 모델들을 직접 짜보는 것이 실력 향상에 도움이 되지만, 매번 새로운 논문들을 직접 구현할 수는 없겠죠. 설령 구현해 본다 하더라도 각각의 모델들이 최고의 성능을 내기 위해서는 엄청나게 많은 컴퓨팅 자원을 동원한 pre-training 작업을 필요로 합니다.\n",
    "<br><br>\n",
    "또한, 논문과 함께 공개된 리서치 코드를 이용하여 모델을 내가 가진 데이터셋과 문제에 적용할 수는 있지만, 매번 프로젝트마다 다른 코드 스타일과, 다른 framework(tensorflow, pytorch 등)을 사용하기 때문에 많은 모델들을 분석해서 구조를 파악한 후 직접 돌려보고 이를 적용하기란 쉽지 않을 것입니다.\n",
    "<br><br>\n",
    "이런 고민들을 모두 한꺼번에 해결해 줄 수 있는 게 바로 NLP Framework입니다!!\n",
    "<br><br>\n",
    "소프트웨어에서 framework란 프로젝트의 뼈대를 이루는 클래스와 인터페이스의 집합을 말합니다. 해당 분야의 베스트 프랙티스를 반영하여 확장 가능한 템플릿 형태로 설계되었기 때문에, framework를 이용해 손쉽게 다양한 응용 프로그램을 제작할 수 있습니다. 최근 NLP 분야에선 transformer 기반의 BERT 등 다양한 pretrained model이 발표되고, 이를 활용한 전이학습(transfer learning)을 통해 다양한 NLP 태스크를 손쉽게 구현하는 흐름이 두드러지고 있습니다. 이런 NLP 분야의 베스트 프랙티스를 바탕으로 다양한 NLP 분야의 framework가 속속 발표되고 있습니다. 이러한 NLP framework들은 NLP 분야의 최신 논문들의 리서치 코드를 미리 구현하여 pretrained model을 제공하고 있습니다. 그래서 framework의 사용자가 아주 손쉽게 이를 가져다가 다양한 태스크에 맞게 finetuning하거나 하여 사용할 수 있게 해 줍니다. 대부분의 NLP framework들은 태스크나 데이터셋, 모델에 무관하게 통일적인 인터페이스를 기반으로 설계된 클래스 구조를 가지고 있어서, 최소한의 코드 구현만으로도 다양한 변화에 대응할 수 있게 해주는 장점이 있습니다.\n",
    "<br><br>\n",
    "정말 다양한 framework들이 존재하고 있습니다. 어떤 멋진 NLP framework들이 있는지 간단히 살펴보겠지만, 이번 시간에 우리는 특히 **Huggingface(🤗)의 transformers**를 중심으로 다양한 NLP의 모델들을 다루는 방법에 대해 이야기 나눠볼까 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **학습 목표**\n",
    "\n",
    "---\n",
    "\n",
    "- 다양한 NLP Framework의 종류를 알고 활용할 수 있습니다.\n",
    "- Huggingface 모델을 불러와 올바른 task에 사용할 수 있습니다.\n",
    "- Huggingface 프레임워크의 구조를 파악하고, 상황에 맞게 이를 미세조정할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **학습 목차**\n",
    "- 2.다양한 NLP Framework의 출현\n",
    "    - 트랜스포머, 언제까지 구현해서 쓸 건가요?\n",
    "- 3.Huggingface transformers 개요\n",
    "    - 무엇이 Huggingface를 특별하게 만드나요?\n",
    "- 4.Huggingface transformers (1) Model\n",
    "    - Huggingface의 가장 핵심인 Model을 알아봅시다.\n",
    "- 5.Huggingface transformers (2) Tokenizer\n",
    "    - 정규표현식은 그만! 내장 Tokenizer을 써보아요.\n",
    "- 6.Huggingface transformers (4) Config\n",
    "    - 모델을 손쉽게 구성해봅시다.\n",
    "- 7.Huggingface transformers (5) Trainer\n",
    "    - 모델 학습을 위한 Trainer 클래스를 알아봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-2. 다양한 NLP Framework의 출현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오늘 우리는 주로 `Huggingface transformers` Framework를 통해 NLP framwork가 어떤 것인지를 구체적으로 살펴볼 것입니다. 그러나 `Huggingface transformers`이전에도 NLP 분야에 널리 알려진 framework들이 많이 있었습니다. 지금도 널리 활용되고 있는 것들 위주로 종류별로 몇 가지를 살펴보도록 하겠습니다.\n",
    "\n",
    "(참고) 이번 스텝의 내용은 [Popular NLP Libraries of 2022](https://medium.com/nlplanet/awesome-nlp-21-popular-nlp-libraries-of-2022-2e07a914248b)에 정리된 내용을 바탕으로 작성하였음을 밝힙니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **General Framework for NLP**\n",
    "\n",
    "---\n",
    "\n",
    "여기서 소개할 framework들은 NLP 문제를 가장 일반적으로 해결할 수 있는 통합적인 프레임워크를 목표로 설계된 것들입니다. 대표적으로는 AllenNLP, Fairseq, Fast.ai가 있으며, Google의 tensor2tensor 프로젝트도 같은 범주로 생각할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 AllenNLP\n",
    "<br><br>\n",
    "- 제공자 : Allen AI Institute\n",
    "- Website : https://allennlp.org/\n",
    "- Github : https://github.com/allenai/allennlp\n",
    "- Backend : PyTorch\n",
    "<br><br>\n",
    "\n",
    "2018년 초반에 Contextual Word Embedding의 대표적인 모델인 ELMO를 발표하면서 유명해진 Allen Institute에서 만든 NLP framework입니다. 당시 ELMO는 [GLUE Benchmark Test](https://gluebenchmark.com/)와 같이 10가지나 되는 다양한 태스크로 구성된 데이터셋을 하나의 모델을 finetune하는 것만으로도 기존의 State-of-the-art 기록을 경신하는 성능을 보여주고자 하였습니다. 그랬기에 하나의 모델로 다양한 태스크를 손쉽게 처리할 수 있는 유연한 프로젝트를 구성해야 했고, 이를 확장하면서 자연스럽게 NLP framework로 발전하게 되었습니다. 이후 AllenNLP는 Glue dataset의 baseline 프로젝트 [Starting Baseline](https://github.com/nyu-mll/GLUE-baselines)를 제공하기도 했습니다.\n",
    "<br><br>\n",
    "\n",
    "![출처 : AllenNLP Guide(https://guide.allennlp.org/building-your-model#1)](../Images/lec_21/1.png)\n",
    "<br><br>\n",
    "태스크와 모델을 분리해서, 한 가지 모델로 다양한 태스크를 처리하거나 하나의 태스크를 다양한 모델로 처리할 수 있도록 하는 설계는 AllenNLP가 처음 시도한 것은 물론 아닙니다만, ELMO와 같은 pretrained model의 성공을 바탕으로 NLP framework를 완성해 나가려는 AllenNLP의 시도는 이후 많은 아이디어를 제공하였습니다. AllenNLP는 현재는 ELMO 이외에도 BERT 등 다양한 모델의 활용이 가능합니다.\n",
    "<br><br>\n",
    "단, AllenNLP는 PyTorch 기반으로 설계되었으며 모델이 `torch.nn.Module`을 상속받는 구조로 설계되었습니다. Tensorflow나 Keras 기반으로 AllenNLP를 활용하는 것은 어렵습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 Fairseq\n",
    "<br><br>\n",
    "- 제공자 : Facebook AI Research\n",
    "- Website : https://fairseq.readthedocs.io/en/latest\n",
    "- Github : https://github.com/pytorch/fairseq\n",
    "- Backend : PyTorch\n",
    "<br><br>\n",
    "\n",
    "Fairseq는 꾸준히 NLP 연구성과를 내고 있는 Facebook AI Research의 NLP Framework입니다. 비단 자연어처리에만 국한된 것이 아니라 이름에서도 알 수 있듯이 CNN, LSTM 등 전통적인 모델로부터, 음성인식/합성 등 sequential한 데이터를 다루는 분야를 두루 다루는 다양한 pretrained model을 함께 제공하고 있습니다. 역시 Facebook의 framework답게 PyTorch 기반으로 설계되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 Fast.ai\n",
    "<br><br>\n",
    "- 제공자 : fast.ai\n",
    "- Website : http://docs.fast.ai/\n",
    "- Github : https://github.com/fastai/fastai\n",
    "- Backend : PyTorch\n",
    "<br><br>\n",
    "\n",
    "fast.ai는 이름에 걸맞게, 빠르게 배우고 쉽게 모델을 구성할 수 있도록 하이레벨 API와 Application 블록까지 손쉽게 사용할 수 있도록 구성되어 있습니다. 비단 NLP 분야 뿐 아니라 다양한 분야로 확장 가능합니다. 역시 PyTorch 기반으로 설계되었습니다.\n",
    "<br><br>\n",
    "\n",
    "![출처 : fast.ai (https://github.com/fastai/fastai)](../Images/lec_21/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 tensor2tensor\n",
    "<br><br>\n",
    "- 제공자 : Google Brain\n",
    "- Github : https://github.com/tensorflow/tensor2tensor (deprecated)\n",
    "- New Github : https://github.com/google/trax\n",
    "- Backend : Tensorflow\n",
    "<br><br>\n",
    "\n",
    "Google Brain에서 2017년에 transformer 논문을 발표하면서 그 구현체로 함께 공유했던 프로젝트가 바로 tensor2tensor였습니다. 이 프로젝트 역시 'Attention is all you need'라는 논문의 제목처럼, transformer를 중심으로 다양한 태스크와 다양한 모델을 하나의 framework에 통합하려는 시도를 하였습니다. 이후 2019년부터 Google은 Tensorflow V2 기반으로 pretrained model의 지원을 강화한 trax라는 프로젝트를 생성하면서, 2020년도부터는 tensor2tensor의 개발을 중단하고 관련 기능을 trax로 통합이관하였습니다.\n",
    "<br><br>\n",
    "Tensorflow 기반의 NLP framework이 상대적으로 드문 가운데, Tensorflow 기반의 NLP 연구개발을 진행한다면 주목해 볼 만할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preprocessing Libraries**\n",
    "\n",
    "---\n",
    "\n",
    "아래는 전통적으로 사용되었던 NLP 분야의 전처리 관련 framework들입니다. 위에서 소개한 framework들처럼 전처리-모델링-태스크 훈련/평가를 통합적으로 설계하여 NLP 태스크를 제너럴하게 수행하게 설계한 것이 아니라 tokenization, tagging, parsing 등 특정 전처리 작업을 위해 설계된 라이브러리에 가깝습니다. 여기에는 아래 예시로 든 Spacy, NLTK, TorchText 등이 있습니다. 한국어인 경우 KoNLPy 라이브러리도 동일한 역할을 한다고 볼 수 있겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 Spacy\n",
    "<br><br>\n",
    "- Website : https://spacy.io/\n",
    "- Github : https://github.com/explosion/spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 NLTK\n",
    "<br><br>\n",
    "- Website : https://www.nltk.org/\n",
    "- Github : https://github.com/nltk/nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 TorchText\n",
    "<br><br>\n",
    "- Website : https://torchtext.readthedocs.io/en/latest/\n",
    "- Github : https://github.com/pytorch/text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transformer-based Framework**\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n",
    "🔶 Huggingface transformers\n",
    "<br><br>\n",
    "- 제공자 : Huggingface.co\n",
    "- Website : https://huggingface.co/transformers/\n",
    "- Github : https://github.com/huggingface/transformers\n",
    "- Backend : PyTorch and Tensorflow\n",
    "<br><br>\n",
    "\n",
    "여기에는 현재 가장 주목받고 있는 NLP Framework인 `Huggingface transformers`가 있습니다. 사실 Huggingface의 transformers 라이브러리의 최근 모습은 이미 아주 general한 NLP framework의 모습을 충분히 가지고 있습니다. 하지만 초기에는 BERT 등 다양한 transformer 기반의 pretrained model을 사용하기 위한 PyTorch 기반의 wrapper 형태로 시작되었습니다. 그래서 전통적인 모델까지 포괄하려고 했던 이전의 general NLP Framework 들에 비해, Huggingface의 transformers는 pretrained model 활용을 주로 지원하며, tokenizer 등 전처리 부분도 pretrained model들이 주로 사용하는 Subword tokenizer 기법에 집중되어 있는 특징이 있습니다.\n",
    "\n",
    "이후 다음 스텝부터는 Huggingface의 transformers에 대해 집중적으로 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-3. Huggingface transformers 개요**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-4. Huggingface transformers (1) Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-5. Huggingface transformers (2) Tokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-6. Huggingfacae transformers (3) Config**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-7. Huggingface transformers (4) Trainer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22-8. 마무리하며**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
