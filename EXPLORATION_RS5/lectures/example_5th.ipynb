{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-2. 텍스트 감정분석의 유용성\n",
    "\n",
    "텍스트 감정분석 접근법</br>\n",
    "- 기계학습 기반 접근법\n",
    "- 감성사전 기반 접근법\n",
    "    - 기계학습 대비 다음의 단점들이 있다.\n",
    "        1. 분석 대상에 따라 단어의 감성 점수가 달라질 수 있다는 가능성에 대응하기 힘듦\n",
    "        2. 단순 긍부정을 넘어서 긍부정의 원인이 되는 대상 속성 기반의 감성 분석이 어려움\n",
    "- 데이터분석 업무 측면에서의 텍스트 분류 모델\n",
    "    - 일반적인 데이터분석 업무에서는 범주화가 잘된 정형데이터를 필요로 함\n",
    "        - 정형데이터는 규모가 커질수록 비용이 기하급수적으로 증가할 수 있다.\n",
    "    - 비정형데이터인 텍스트에 감성분석 기법을 사용\n",
    "        - 텍스트를 정형데이터로 가공하여 유용한 의사결정 보조자료로 활용 가능함\n",
    "\n",
    "\n",
    "워드 임베딩(word embedding) 기법</br>\n",
    "- 단어의 특성을 저차원 벡터 값으로 표현하는 기법\n",
    "- 머신러닝 기반 감성분석 시 비용을 절감할 수 있음\n",
    "    - 머신러닝 추론 일치도(정확도)를 크게 향상할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-4. 텍스트 데이터의 특징\n",
    "## (1) 텍스트를 숫자로 표현하는 방법\n",
    "\n",
    "단어와 그 **단어의 의미를 나타내는 벡터**를 짝지어 보자.</br>\n",
    "> 마치 사전에서 특정 단어와 그 단어에 대한 설명이 짝지어진 것 처럼\n",
    "\n",
    "텍스트 데이터를 처리하는 예제\n",
    "```text\n",
    "## 텍스트 데이터 예시\n",
    "\n",
    "    i feel hungry\n",
    "    i eat lunch\n",
    "    now i feel happy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'feel', 'hungry']\n"
     ]
    }
   ],
   "source": [
    "# 문장 입력 및 공백을 기준으로 split\n",
    "# 처리해야 할 문장을 파이썬 리스트에 옮겨 담았습니다.\n",
    "sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# 파이썬 split() 메소드를 이용해 단어 단위로 문장을 쪼개 봅니다.\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터를 기반으로 사전을 만들기 위한 예시\n",
    "## python dictionary 활용\n",
    "\n",
    "index_to_word={}  # 빈 딕셔너리를 만들어서\n",
    "\n",
    "# 단어들을 하나씩 채워 봅니다. 채우는 순서는 일단 임의로 하였습니다. 그러나 사실 순서는 중요하지 않습니다. \n",
    "# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다. \n",
    "index_to_word[0]='<PAD>'  # 패딩용 단어\n",
    "index_to_word[1]='<BOS>'  # 문장의 시작지점\n",
    "index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\n",
    "index_to_word[3]='i'\n",
    "index_to_word[4]='feel'\n",
    "index_to_word[5]='hungry'\n",
    "index_to_word[6]='eat'\n",
    "index_to_word[7]='lunch'\n",
    "index_to_word[8]='now'\n",
    "index_to_word[9]='happy'\n",
    "\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'i': 3, 'feel': 4, 'hungry': 5, 'eat': 6, 'lunch': 7, 'now': 8, 'happy': 9}\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터를 숫자로 변환하기\n",
    "## 예제 기준에서 key와 value를 바꿔버리면 된다.\n",
    "word_to_index={word:index for index, word in index_to_word.items()}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# 바뀐 단어의 값(숫자)을 확인\n",
    "print(word_to_index['feel'])  # 단어 'feel'은 숫자 인덱스 4로 바뀝니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# 가지고 있는 텍스트 데이터들을 숫자로 바꿔 표현하기\n",
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터 사전화 예시\n",
    "# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다. \n",
    "encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i feel hungry\n"
     ]
    }
   ],
   "source": [
    "# 숫자 값으로 표현된 단어들을 다시 원래 단어(sentence)로 복구하기\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i feel hungry', 'i eat lunch', 'now i feel happy']\n"
     ]
    }
   ],
   "source": [
    "# 문장 encode, decode 예시\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 가 아래와 같이 변환됩니다.\n",
    "print(get_decoded_sentences(encoded_sentences, index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-5. 텍스트 데이터의 특징\n",
    "## (2) Embedding 레이어의 등장\n",
    "임베딩(Embedding)?\n",
    "```text\n",
    "자연어 처리(Natural Language Processing)분야에서 말하는 임베딩(Embedding)\n",
    "- 사람이 쓰는 자연어를 기께가 이해할 수 있는 숫자형태(vector)로 바꾼 결과\n",
    "- 혹인 그 일련의 과정 전체\n",
    "```\n",
    "What to do with Embedding?\n",
    "```text\n",
    "단어나 문장 사이의 코사인 유사도가 가장 높은 단어 구하기 (계산)\n",
    "단어들 사이의 의미/문법적 정보 도출\n",
    "단어 사이 문법적 관계 도출 (벡터 간 연산)\n",
    "```\n",
    "> 전이 학습(transfer learning) 임베딩\n",
    ">> 다른 딥러닝 모델의 입력값으로 자주 쓰임\n",
    ">> 품질 좋은 임베딩을 사용할수록 모델의 성능이 좋아짐\n",
    "[LINK: 임베딩 레이어를 통해 word to vector](https://wikidocs.net/64779)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 06:25:20.697541: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# 숫자로 변환된 텍스트 데이터 [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 에 Embedding 레이어를 적용합니다. \u001b[39;00m\n\u001b[1;32m     14\u001b[0m raw_inputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(get_encoded_sentences(sentences, word_to_index), dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m output \u001b[39m=\u001b[39m embedding(raw_inputs)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/myconda/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/myconda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "# Embedding 레이어를 활용하여 이전 스텝의 텍스트 데이터를 워드 벡터 텐서 형태로 다시 표현\n",
    "# 아래 코드는 그대로 실행하시면 에러가 발생할 것입니다. \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 위 그림과 같이 4차원의 워드 벡터를 가정합니다. \n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# 숫자로 변환된 텍스트 데이터 [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 에 Embedding 레이어를 적용합니다. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype='object')\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer 주의점\n",
    "#### 위 cell의 코드가 에러난 이유\n",
    "> Embedding 레이어의 인풋 문장 벡터의 **길이는 일정**해야한다.</br>\n",
    ">> raw_inputs에 있는 3개 벡터의 길이는 각각 다음과 같다.\n",
    ">> 4, 4, 5\n",
    "\n",
    "```text\n",
    "Tensorflow에서는 다음 함수를 통해 문장 벡터 뒤에 패딩( <PAD> )을 추가하여 길이를 일정하게 맞춰준다.\n",
    "```\n",
    "`tf.keras.preprocessing.sequence.pad_sequences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 4 5 0]\n",
      " [1 3 6 7 0]\n",
      " [1 8 3 4 9]]\n"
     ]
    }
   ],
   "source": [
    "## tf.keras.preprocessing.sequence.pad_sequences() 코드 예시\n",
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "print(raw_inputs)\n",
    "\n",
    "## padding 값의 default는 'pre'다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<PAD>`가 0과 매핑되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.02797605 -0.00805179 -0.00140411 -0.00804634]\n",
      "  [-0.00691222 -0.01397717  0.00762901  0.01910103]\n",
      "  [ 0.03633161 -0.03766296 -0.00030882  0.00575482]\n",
      "  [ 0.00998298 -0.01156216  0.01873604 -0.03365401]\n",
      "  [-0.01435258 -0.02897682  0.0341973   0.01650895]]\n",
      "\n",
      " [[-0.02797605 -0.00805179 -0.00140411 -0.00804634]\n",
      "  [-0.00691222 -0.01397717  0.00762901  0.01910103]\n",
      "  [-0.00497545  0.01191349 -0.00841505 -0.02019   ]\n",
      "  [-0.01158576  0.04504373 -0.02730181 -0.00517433]\n",
      "  [-0.01435258 -0.02897682  0.0341973   0.01650895]]\n",
      "\n",
      " [[-0.02797605 -0.00805179 -0.00140411 -0.00804634]\n",
      "  [ 0.0422066   0.03172399  0.01955319  0.0169205 ]\n",
      "  [-0.00691222 -0.01397717  0.00762901  0.01910103]\n",
      "  [ 0.03633161 -0.03766296 -0.00030882  0.00575482]\n",
      "  [-0.0476425  -0.01281666  0.01041086  0.02598471]]], shape=(3, 5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# input 벡터들 (raw_inputs)의 길이를 맞춰주면서 word to vector를 수행하는 코드\n",
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 그림과 같이 4차원의 워드 벡터를 가정합니다.\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# tf.keras.preprocessing.sequence.pad_sequences를 통해 word vector를 모두 일정 길이로 맞춰주어야 \n",
    "# embedding 레이어의 input이 될 수 있음에 주의해 주세요. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype=object)\n",
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. output의 shape(3, 5, 4)에서 각 값의 의미\n",
    ">> 3 : 입력문장 개수\n",
    ">> 5 : 입력문장의 최대 길이\n",
    ">> 4 : 워드 벡터의 차원 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-6. 시퀀스 데이터를 다루는 RNN\n",
    "- 텍스트 데이터를 다루는데 주로 사용되는 딥러닝 모델\n",
    "    - Recurrent Neural Network (RNN)\n",
    "    - 시퀀스(Sequence) 형태의 데이터를 처리하기에 최적인 모델로 알려져 있음\n",
    "\n",
    "[stateful vs stateless 예시; Web의 관점에서](https://www.slideshare.net/xguru/ss-16106464)</br>\n",
    "[모두의 딥러닝 강좌 - 12강.RNN, 김성훈 교수](https://youtu.be/-SHPG_KMUkQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 4)           40        \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 8)                 416       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 537 (2.10 KB)\n",
      "Trainable params: 537 (2.10 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# RNN 모델을 사용하여 텍스트 데이터를 처리하는 예제 코드\n",
    "## 텍스트 데이터는 이전 cell의 텍스트를 의미\n",
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 시퀀스 데이터와 RNN 참고자료\n",
    "\n",
    "[Youtube Link](https://youtu.be/mG6N0ut9dog?t=1447)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-7. 꼭 RNN이어야 할까?\n",
    "텍스트 처리에서는 RNN이 아니라 `1-D Convolution Neural Network (1-D CNN)`를 사용할수도 있다.</br>\n",
    "```text\n",
    "이전에 이미지 분류기 구현에서 `2-D CNN`을 사용해보았다.\n",
    "\n",
    " - 이미지는 시퀀스 데이터가 아니다.\n",
    "\n",
    "따라서 이미지 분류기 모델에서 사용된 입력은 이미지 전체다.\n",
    "```\n",
    "**1-D CNN (with text data)**\n",
    "- 문장 전체를 한꺼번에 한 방향으로 스캐닝\n",
    "- 한 번에 처리하는 문장의 양은 7 (len 7)\n",
    "- 7 단어 이내에서 발견되는 특징을 추출하여 추출결과값을 문장 분류에 사용함\n",
    "\n",
    "`1-D CNN`방식도 텍스트 처리에서 RNN 못지않은 효율을 보여줌</br>\n",
    "**CNN 계열의 이점**\n",
    "- RNN 계열보다 병렬처리가 효율적이다\n",
    "- 학습 속도가 RNN에 빠르게 진행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, None, 4)           40        \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 16)          464       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, None, 16)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 16)          1808      \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 16)                0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2457 (9.60 KB)\n",
      "Trainable params: 2457 (9.60 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1-D CNN을 활용하여 텍스트 데이터 처리 및 모델 학습\n",
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1-D CNN` 기법</br>\n",
    "- `GlobalMaxPooling()` 레이어만 사용하여 모델을 쌓는 방식\n",
    "\n",
    "**GlobalMaxPooling** 방식</br>\n",
    "- 전체 문장 중에서 단 하나의 가장 중요한 단어만 feature로 추출하는 방식\n",
    "    - 추출된 feature는 문장의 긍정/부정을 평가하는데 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 4)           40        \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Gl  (None, 4)                 0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8)                 40        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89 (356.00 Byte)\n",
      "Trainable params: 89 (356.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1-D CNN 기법에서 GlobalMaxPooling() 레이어만 사용하는 예제\n",
    "\n",
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이 외에도 고려할만한 방법\n",
    "\n",
    "- 1-D CNN과 RNN 섞어 쓰기\n",
    "- FFN (FeedForward Network) 레이어만 사용해보기\n",
    "- Transformer 레이어 사용해보기\n",
    "\n",
    "[참고링크](https://wikidocs.net/80437)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-8. IMDB 영화리뷰 감성분석\n",
    "## (1) IMDB 데이터셋 분석\n",
    "IMDb 영화리뷰 감성분석 태스크 도전 예제</br>\n",
    "**IMDb Large Movie Dataset**</br>\n",
    "- 50000개의 영단어로 작성된 영화 리뷰 텍스트\n",
    "- 긍정 리뷰에는 `1`, 부정 리뷰에는 `0`의 라벨이 달려 있다.\n",
    "\n",
    "[관련논문](https://aclanthology.org/P11-1015.pdf)</br>\n",
    "\n",
    "> 아래는 노드에 언급된 내용\n",
    "```text\n",
    "50000개의 리뷰 중 절반인 25000개가 훈련용 데이터, 나머지 25000개를 테스트용 데이터로 사용하도록 지정되어 있습니다. 이 데이터셋은 tensorflow Keras 데이터셋 안에 포함되어 있어서 손쉽게 다운로드하여 사용할 수 있습니다.\n",
    "이후 스텝의 IMDb 데이터셋 처리 코드 중 일부는 Tensorflow 튜토리얼에 언급된 데이터 전처리 로직을 참고하였음을 밝힙니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 개수: 25000, 테스트 개수: 25000\n"
     ]
    }
   ],
   "source": [
    "# IMDb 데이터셋 다운로드\n",
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "# 학습 데이터 (train)와 정답 데이터(test)로 분리\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "print(f\"훈련 샘플 개수: {len(x_train)}, 테스트 개수: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "라벨:  1\n",
      "1번째 리뷰 문장 길이:  218\n",
      "2번째 리뷰 문장 길이:  189\n"
     ]
    }
   ],
   "source": [
    "# 데이터의 예시 확인\n",
    "print(x_train[0])  # 1번째 리뷰데이터\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n",
    "print('1번째 리뷰 문장 길이: ', len(x_train[0]))\n",
    "print('2번째 리뷰 문장 길이: ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMDb 데이터셋 특징**</br>\n",
    "- 텍스트 데이터가 아니다\n",
    "- 각 텍스트(단어)들은 이미 숫자로 `encode`되어 있다.\n",
    "    - 이라한 특징 때문에 IMDb 데이터셋은 encode에 사용한 사전(딕셔너리)까지 함께 제공함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# IMDb 데이터셋의 텍스트 데이터를 가져옴;\n",
    "## get_word_index()를 사용하여 이미 숫자로 encode된 텍스트 데이터를 읽어오게 됨\n",
    "word_to_index = imdb.get_word_index()\n",
    "## 읽어온 데이터(word_to_index)를 숫자키(인덱스)로도 읽을 수 있게 python dictionary 재구성 \n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])     # 'the' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 1 이 출력됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 노드에서 언급한 주의점\n",
    "\n",
    "```text\n",
    "여기서 주의할 점이 있습니다. IMDb 데이터셋의 텍스트 인코딩을 위한 word_to_index, index_to_word는 보정이 필요한데요.\n",
    "\n",
    "예를 들어 다음 코드를 실행시켜보면 보정이 되지 않은 상태라 문장이 이상함을 확인하실 겁니다. (뒤에 보정 후 다시 확인해볼 예정이에요.😊)\n",
    "```\n",
    "\n",
    "IMDB 데이터셋을 텍스트 인코딩 하기 위해서 `word_to_index`, `index_to_word`들에 대해 보정이 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n"
     ]
    }
   ],
   "source": [
    "# 보정 전 x_train[0] 데이터\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\n",
      "4\n",
      "the\n",
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "## x_train 데이터 보정 작업 예시\n",
    "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다.\n",
    "word_to_index[\"<PAD>\"] = 0      # padding\n",
    "word_to_index[\"<BOS>\"] = 1      # begining of sentence\n",
    "word_to_index[\"<UNK>\"] = 2      # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3   # unused\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 4 이 출력됩니다. \n",
    "print(index_to_word[4])     # 'the' 가 출력됩니다.\n",
    "\n",
    "# 보정 후 x_train[0] 데이터\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "라벨:  1\n"
     ]
    }
   ],
   "source": [
    "# encode 된 데이터를 decode 하여 결과 확인하는 예시\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 노드에서 언급할 주의점\n",
    "\n",
    "```text\n",
    "pad_sequences를 통해 데이터셋 상의 문장의 길이를 통일하는 것을 잊어서는 안됩니다.\n",
    "문장 최대 길이 maxlen의 값 설정도 전체 모델 성능에 영향을 미치게 됩니다. 이 길이도 적절한 값을 찾기 위해서는 전체 데이터셋의 분포를 확인해 보는 것이 좋습니다.\n",
    "```\n",
    ">> `pad_sequences`: 데이터셋 상의 문장 길이를 통일\n",
    ">> 문장 최대 길이 (`maxlen`의 값) 설정도 전체 모델 성능에 영향을 미침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  234.75892\n",
      "문장길이 최대 :  2494\n",
      "문장길이 표준편차 :  172.91149458735703\n",
      "pad_sequences maxlen :  580\n",
      "전체 문장의 0.94536%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터셋 확인예시\n",
    "\n",
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print(f'전체 문장의 {np.sum(num_tokens < max_tokens) / len(num_tokens)}%가 maxlen 설정값 이내에 포함됩니다. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 노드에서 언급한 내용\n",
    "\n",
    "```text\n",
    "위의 경우에는 maxlen=580이 됩니다.\n",
    "또 한 가지 유의해야 하는 것은 padding 방식을 문장 뒤쪽('post')과 앞쪽('pre') 중 어느 쪽으로 하느냐에 따라 RNN을 이용한 딥러닝 적용 시 성능 차이가 발생한다는 점입니다.\n",
    "두 가지 방식을 한 번씩 다 적용해서 RNN을 학습시켜 보면서 그 결과를 비교해 보시기 바랍니다.\n",
    "```\n",
    "> 벡터 길이 맞출 때, padding 방식에는 두 가지 방식이 있다.\n",
    "\n",
    "1. 길이를 맞출 때, padding 값을 뒤쪽에 삽입하는 방식: `post`\n",
    "2. 길이를 맞출 때, padding 값을 앞쪽에 삽입하는 방식: `pre`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제([Link](https://wikidocs.net/24586))를 사용하여 두 가지의 함수를 선언 및 테스트 수행</br>\n",
    "\n",
    "> post_rnn_test()\n",
    ">> post-padding으로 pre-processing한 데이터 기반 RNN 학습\n",
    "> pre_rnn_test()\n",
    ">> pre-padding으로 pre-processing한 데이터 기반 RNN 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST PADDING 방식의 rnn 실행 예제 함수 선언\n",
    "def post_rnn_test(x_train, x_test, y_train, y_test, maxlen):\n",
    "    # 예제에서 사용한 모듈\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from tensorflow.keras.models import load_model\n",
    "    # 예제 값 그대로 사용\n",
    "    vocab_size = 10000\n",
    "    # max_len은 노드 순서상 선언이 되어 있으므로 가져와서 사용함\n",
    "    # 아래는 샘플 값\n",
    "    ## max_len = 500\n",
    "\n",
    "    # POST 패딩; 구성은 노드 참고함\n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='post', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "    # 공식사이트 기준 하이퍼 파라미터\n",
    "    ## 하이퍼파라미터인 임베딩 벡터의 차원은 100, 은닉 상태의 크기는 128입니다.\n",
    "    embedding_dim = 100\n",
    "    hidden_units = 128\n",
    "\n",
    "    # Sequential 모델 적용\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim))\n",
    "    model.add(GRU(hidden_units))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    ## EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)는\n",
    "    ## 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 4회 증가하면\n",
    "    ## 정해진 에포크가 도달하지 못하였더라도 학습을 조기 종료(Early Stopping)한다는 의미입니다.\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "    ## ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장합니다.\n",
    "    mc = ModelCheckpoint('GRU_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    ## validation_split=0.2을 사용하여 훈련 데이터의 20%를 검증 데이터로 분리해서 사용하고,\n",
    "    ## 검증 데이터를 통해서 훈련이 적절히 되고 있는지 확인합니다.\n",
    "    ## 검증 데이터는 기계가 훈련 데이터에 과적합되고 있지는 않은지 확인하기 위한 용도로 사용됩니다.\n",
    "    history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)\n",
    "    \n",
    "    # GRU 모델을 불러 검증데이터의 정확도 확인\n",
    "    ## 훈련 과정에서 검증 데이터의 정확도가 가장 높았을 때 저장된 모델인 'GRU_model.h5'를 로드합니다.\n",
    "    loaded_model = load_model('GRU_model.h5')\n",
    "    print(\"\\n post-padding 기반 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "# PRE PADDING 방식의 rnn 실행 예제 함수 선언\n",
    "def pre_rnn_test(x_train, x_test, y_train, y_test, maxlen):\n",
    "    # 예제에서 사용한 모듈\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from tensorflow.keras.models import load_model\n",
    "    # 예제 값 그대로 사용\n",
    "    vocab_size = 10000\n",
    "    # max_len은 노드 순서상 선언이 되어 있으므로 가져와서 사용함\n",
    "    # 아래는 샘플 값\n",
    "    ## max_len = 500\n",
    "\n",
    "    # POST 패딩; 구성은 노드 참고함\n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'post'\n",
    "                                                        maxlen=maxlen)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 혹은 'post'\n",
    "                                                       maxlen=maxlen)\n",
    "    # 공식사이트 기준 하이퍼 파라미터\n",
    "    ## 하이퍼파라미터인 임베딩 벡터의 차원은 100, 은닉 상태의 크기는 128입니다.\n",
    "    embedding_dim = 100\n",
    "    hidden_units = 128\n",
    "\n",
    "    # Sequential 모델 적용\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim))\n",
    "    model.add(GRU(hidden_units))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    ## EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)는\n",
    "    ## 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 4회 증가하면\n",
    "    ## 정해진 에포크가 도달하지 못하였더라도 학습을 조기 종료(Early Stopping)한다는 의미입니다.\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "    ## ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장합니다.\n",
    "    mc = ModelCheckpoint('GRU_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    ## validation_split=0.2을 사용하여 훈련 데이터의 20%를 검증 데이터로 분리해서 사용하고,\n",
    "    ## 검증 데이터를 통해서 훈련이 적절히 되고 있는지 확인합니다.\n",
    "    ## 검증 데이터는 기계가 훈련 데이터에 과적합되고 있지는 않은지 확인하기 위한 용도로 사용됩니다.\n",
    "    history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)\n",
    "    \n",
    "    # GRU 모델을 불러 검증데이터의 정확도 확인\n",
    "    ## 훈련 과정에서 검증 데이터의 정확도가 가장 높았을 때 저장된 모델인 'GRU_model.h5'를 로드합니다.\n",
    "    loaded_model = load_model('GRU_model.h5')\n",
    "    print(\"\\n pre-padding 기반 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "# 함수 실행1\n",
    "print(\"post padding case\\n ===========\")\n",
    "post_rnn_test(x_train, x_test, y_train, y_test, maxlen)\n",
    "\n",
    "# 함수 실행2\n",
    "print(\"pre padding case\\n ===========\")\n",
    "pre_rnn_test(x_train, x_test, y_train, y_test, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 위 코드를 로컬(MAC)에서 검증할 경우, kernel truncated 발생\n",
    "### why? CPU default calucation\n",
    "#### 노드에서 확인한 결과 캡처본으로 대체\n",
    "> post-padding 전처리 테스트 결과 예시\n",
    "\n",
    "![post-padding example](../images/1.png)</br>\n",
    "\n",
    "> pre-padding 전처리 테스트 결과 예시\n",
    "\n",
    "![pre-padding example](../images/2.png)</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 노드(`7-8`)에서 실제로 처리해준 과정</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"], \n",
    "                                                        padding='post', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                        value=word_to_index[\"<PAD>\"], \n",
    "                                                        padding='post', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노드에서 언급된 질문</br>\n",
    "- Q. RNN 활용 시 pad_sequences의 padding 방식은 'post'와 'pre' 중 어느 것이 유리할까요? 그 이유는 무엇일까요?\n",
    "\n",
    "예시 답변</br>\n",
    "- RNN은 입력데이터가 순차적으로 처리되어, 가장 마지막 입력이 최종 state 값에 가장 영향을 많이 미치게 됩니다. 그러므로 마지막 입력이 무의미한 padding으로 채워지는 것은 비효율적입니다. 따라서 'pre'가 훨씬 유리하며, 10% 이상의 테스트 성능 차이를 보이게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-9. IMDB 영화리뷰 감상분석\n",
    "## (2) 딥러닝 모델 설계와 훈련\n",
    "\n",
    "RNN 모델 직접 설계해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, None, 16)          160000    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 10)                1080      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                176       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161273 (629.97 KB)\n",
      "Trainable params: 161273 (629.97 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000  # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요\n",
    "model = tf.keras.Sequential()\n",
    "# [[YOUR CODE]]\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))  # Embedding Layer\n",
    "model.add(tf.keras.layers.LSTM(10)) # One of the famouse RNN models; 이전 실습과 다르게 10-dimension 설정해봄\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu')) # output shape를 16으로 설정해봄\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))   # 최종출력은 긍정 / 부정을 표현하는 1-dimension\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 훈련전에 다음을 세팅함</br>\n",
    "- 훈련용 데이터셋 25,000건\n",
    "    - 이 중 10,000건을 분리\n",
    "        - 해당 데이터는 검증셋 (validation set)으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 580)\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 10000건 분리\n",
    "x_val = x_train[:10000]\n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = x_train[10000:]\n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 학습을 시작해 봅시다.</br>\n",
    "> 이전 과정에서 데이터들(`x_train`, `x_test`)에 대해 padding 전처리를 하지 않으면 Numpy Error (차원 불일치)가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs=20   # 얼마나 훈련하면 좋을지 결과를 보면서 바꾸어보기\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=epochs, batch_size=512, validation_data=(x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로컬 테스트 (only MAC) 경우 Truncated 발생</br>\n",
    "이후 과정은 노드 캡처본으로 정리 대체함</br>\n",
    "\n",
    "![20 epoch learning example](../images/3.png)</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 끝난 모델을 테스트셋으로 평가해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_text, y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![evaluation with test set](../images/4.png)</br>\n",
    "\n",
    "`model.fit()` 과정 중의 train/validation loss, accuracy 등이 매 훈련(`epoch`)마다 history 변수에 저장되어 있습니다.</br>\n",
    "이 데이터를 그래프로 그려 보면, 수행했던 딥러닝 학습이 잘 진행되었는지, 잘못되었는지 (`overfitting` 혹은 `underfitting`), 성능을 개선할 수 있는 다양한 아이디어를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())  # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![check dictionary key of 'history'](../images/5.png)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = histroy_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다.\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다.\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph of the Training and validation loss](../images/6.png)</br>\n",
    "\n",
    "validation loss의 그래프가 train loss와의 이격(벌어짐 현상)이 발생하게 되면 더 이상의 트레이닝은 무의미해지게 된다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다.\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph of the Training and validation accuracy](../images/7.png)</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-10. IMDB 영화리뷰 감성분석\n",
    "## (3) Word2Vec의 적용\n",
    "\n",
    "여기부터 워드 벡터 파일을 저장할 디렉토리를 설정한다.</br>\n",
    "\n",
    "```shell\n",
    "# 노드 예시\n",
    "$ mkdir -p ~/aiffel/sentiment_classification/data\n",
    "$ pip list | grep gensim\n",
    "\n",
    "# Local dir example --> ~/data/sentiment_classification\n",
    "```\n",
    "\n",
    "> 테스트 기준 gensim version is `4.1.2`\n",
    "\n",
    "[Youtube: 딥러닝 자연어처리](https://youtube.com/watch?v=sY4YyacSsLc&t=126s&ab_channel=MinsukHeo허민석)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape format: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다.\n",
    "# 노드 기준 path\n",
    "# word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/word2vec.txt'\n",
    "# 로컬 기준 path\n",
    "word2vec_file_path = '../data/sentiment_classification/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "# 백터 개수와 사이즈를 어떻게 기재할지 타이틀 작성\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))\n",
    "\n",
    "# 단어 개수만큼의 워드 벡터를 파일에 기록합니다.\n",
    "## 이때, 특수문자 4개는 제외시킵니다.\n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gensim` 패키지를 이용해, 위에서 작성한 파일(`임베딩 파라미터`)을 읽어서 word vector로 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07126842,  0.08547736,  0.02286462, -0.03098332,  0.0765068 ,\n",
       "       -0.01164088,  0.060796  , -0.02766533,  0.08184147, -0.06716599,\n",
       "       -0.01517043,  0.02369968,  0.0872837 , -0.02541867, -0.0553827 ,\n",
       "       -0.05494674], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 워드 벡터로 실험을 할 수 있다.</br>\n",
    "\n",
    "> 워드 벡터가 의미 벡터 공간상에 유의미하게 학습되었는지 확인하는 방법 중 하나\n",
    ">> 단어를 하나 주고 그와 가장 유사한 단어와 유사도를 확인하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('finely', 0.9213090538978577),\n",
       " ('reality', 0.9122799634933472),\n",
       " ('plant', 0.905373752117157),\n",
       " ('1st', 0.8927375674247742),\n",
       " ('relatively', 0.8903787136077881),\n",
       " ('portrayal', 0.8892521262168884),\n",
       " ('ethan', 0.8865978717803955),\n",
       " ('four', 0.8793693780899048),\n",
       " ('pleasing', 0.8763188123703003),\n",
       " ('toy', 0.8724988102912903)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gensim 패키지를 사용하면 아래와 같이 해볼 수 있음\n",
    "word_vectors.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**</br>\n",
    "\n",
    "구글에서 제공하는 사전학습된(Pretrained) 워드 임베딩 모델\n",
    "- 1억 개의 단어로 구성된 Google News dataset을 바탕으로 학습된 모델\n",
    "- 총 300만 개의 단어를 각각 300차원의 벡터로 표현함\n",
    "\n",
    "임베딩의 개념에 대한 정리 (`한국어 임베딩`)</br>\n",
    "\n",
    "- [한국어 임베딩 서문](https://ratsgo.github.io/natural%20language%20processing/2019/09/12/embedding/)\n",
    "\n",
    "---\n",
    "\n",
    "Word2Vec 모델 적용해보기</br>\n",
    "> 노드에서는 데이터 다운로드 없이 심볼릭 링크 활용함\n",
    "```shell\n",
    "$ ln -s ~/data/GoogleNew-vectors-negative300.bin.gz ~/aiffel/sentiment_classification/data\n",
    "```\n",
    "\n",
    "[파일 다운로드 링크](https://kaggle.com/datasets/leadbest/googlenewsvectorsnegative300)</br>\n",
    "> 2023.07.14 기준 파일 크기가 `3.64 GB`다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 노드 기준 path 예시\n",
    "# word2vec_path = os.getenv('HOME')+'aiffel/sentiment_classification/data/GoogleNews-vectors-negative300.bin.gz'\n",
    "# 로컬 기준 path 예시\n",
    "word2vec_path = '../data/sentiment_classification/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
    "vector = word2vec['computer']\n",
    "vector  # 무려 300-dimension 짜리 워드 벡터.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과는 노드에서 확인했던 shell 캡처로 대체함</br>\n",
    "![Word2Vec dataset load a million counts](../images/8.png)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리를 다소 많이 소비하는 작업이니 유의해 주세요.\n",
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어들의 의미적 유사도가 가까운 것들을 확인하는 코드</br>\n",
    "\n",
    "마찬가지로 노드에서 확인했던 shell 캡처로 대체함</br>\n",
    "![cosine similarity within Word2vec](../images/9.png)</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 이전 스텝에서 학습했던 과정을 다시 기술한다.</br>\n",
    "- 모델의 임베딩 레이어를 `Word2vec`의 것으로 대체한 예제 샘플\n",
    "    - 모든 과정은 위의 것들과 마찬가지로 노드 확인결과 켬처본으로 대체예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000  # 어휘 사전의 크기입니다.(10,000개의 단어)\n",
    "word_vector_dim = 300   # 워드 벡터의 차원수\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 워드 벡터를 단어별로 하나씩 차례차례 카피\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000      # 어휘 사전의 크기. 10,000개의 단어\n",
    "word_vector_dim = 300   # 워드 벡터의 차원수\n",
    "\n",
    "# 모델 구성\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, \n",
    "                                    word_vector_dim, \n",
    "                                    # 카피한 임베딩을 embedding_initializer로 활용\n",
    "                                    embedding_initializer=Constant(emedding_matrix), \n",
    "                                    input_length=maxlen, \n",
    "                                    # trainable 값이 True면 Fine-tuning\n",
    "                                    trainable=True))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sequential model summarize list](../images/10.png)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 20 # 훈련 횟수는 바꿔가면서 테스트해보자\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=epochs, batch_size=512, validation_data=(x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![learning process about 20 epochs case..](../images/11.png)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model evaluation for test set](../images/12.png)</br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
