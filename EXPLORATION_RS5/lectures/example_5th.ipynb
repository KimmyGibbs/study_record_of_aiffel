{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-2. 텍스트 감정분석의 유용성\n",
    "\n",
    "텍스트 감정분석 접근법</br>\n",
    "- 기계학습 기반 접근법\n",
    "- 감성사전 기반 접근법\n",
    "    - 기계학습 대비 다음의 단점들이 있다.\n",
    "        1. 분석 대상에 따라 단어의 감성 점수가 달라질 수 있다는 가능성에 대응하기 힘듦\n",
    "        2. 단순 긍부정을 넘어서 긍부정의 원인이 되는 대상 속성 기반의 감성 분석이 어려움\n",
    "- 데이터분석 업무 측면에서의 텍스트 분류 모델\n",
    "    - 일반적인 데이터분석 업무에서는 범주화가 잘된 정형데이터를 필요로 함\n",
    "        - 정형데이터는 규모가 커질수록 비용이 기하급수적으로 증가할 수 있다.\n",
    "    - 비정형데이터인 텍스트에 감성분석 기법을 사용\n",
    "        - 텍스트를 정형데이터로 가공하여 유용한 의사결정 보조자료로 활용 가능함\n",
    "\n",
    "\n",
    "워드 임베딩(word embedding) 기법</br>\n",
    "- 단어의 특성을 저차원 벡터 값으로 표현하는 기법\n",
    "- 머신러닝 기반 감성분석 시 비용을 절감할 수 있음\n",
    "    - 머신러닝 추론 일치도(정확도)를 크게 향상할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-4. 텍스트 데이터의 특징\n",
    "## (1) 텍스트를 숫자로 표현하는 방법\n",
    "\n",
    "단어와 그 **단어의 의미를 나타내는 벡터**를 짝지어 보자.</br>\n",
    "> 마치 사전에서 특정 단어와 그 단어에 대한 설명이 짝지어진 것 처럼\n",
    "\n",
    "텍스트 데이터를 처리하는 예제\n",
    "```text\n",
    "## 텍스트 데이터 예시\n",
    "\n",
    "    i feel hungry\n",
    "    i eat lunch\n",
    "    now i feel happy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'feel', 'hungry']\n"
     ]
    }
   ],
   "source": [
    "# 문장 입력 및 공백을 기준으로 split\n",
    "# 처리해야 할 문장을 파이썬 리스트에 옮겨 담았습니다.\n",
    "sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# 파이썬 split() 메소드를 이용해 단어 단위로 문장을 쪼개 봅니다.\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터를 기반으로 사전을 만들기 위한 예시\n",
    "## python dictionary 활용\n",
    "\n",
    "index_to_word={}  # 빈 딕셔너리를 만들어서\n",
    "\n",
    "# 단어들을 하나씩 채워 봅니다. 채우는 순서는 일단 임의로 하였습니다. 그러나 사실 순서는 중요하지 않습니다. \n",
    "# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다. \n",
    "index_to_word[0]='<PAD>'  # 패딩용 단어\n",
    "index_to_word[1]='<BOS>'  # 문장의 시작지점\n",
    "index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\n",
    "index_to_word[3]='i'\n",
    "index_to_word[4]='feel'\n",
    "index_to_word[5]='hungry'\n",
    "index_to_word[6]='eat'\n",
    "index_to_word[7]='lunch'\n",
    "index_to_word[8]='now'\n",
    "index_to_word[9]='happy'\n",
    "\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'i': 3, 'feel': 4, 'hungry': 5, 'eat': 6, 'lunch': 7, 'now': 8, 'happy': 9}\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터를 숫자로 변환하기\n",
    "## 예제 기준에서 key와 value를 바꿔버리면 된다.\n",
    "word_to_index={word:index for index, word in index_to_word.items()}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# 바뀐 단어의 값(숫자)을 확인\n",
    "print(word_to_index['feel'])  # 단어 'feel'은 숫자 인덱스 4로 바뀝니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# 가지고 있는 텍스트 데이터들을 숫자로 바꿔 표현하기\n",
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터 사전화 예시\n",
    "# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다. \n",
    "encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i feel hungry\n"
     ]
    }
   ],
   "source": [
    "# 숫자 값으로 표현된 단어들을 다시 원래 단어(sentence)로 복구하기\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i feel hungry', 'i eat lunch', 'now i feel happy']\n"
     ]
    }
   ],
   "source": [
    "# 문장 encode, decode 예시\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 가 아래와 같이 변환됩니다.\n",
    "print(get_decoded_sentences(encoded_sentences, index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-5. 텍스트 데이터의 특징\n",
    "## (2) Embedding 레이어의 등장\n",
    "임베딩(Embedding)?\n",
    "```text\n",
    "자연어 처리(Natural Language Processing)분야에서 말하는 임베딩(Embedding)\n",
    "- 사람이 쓰는 자연어를 기께가 이해할 수 있는 숫자형태(vector)로 바꾼 결과\n",
    "- 혹인 그 일련의 과정 전체\n",
    "```\n",
    "What to do with Embedding?\n",
    "```text\n",
    "단어나 문장 사이의 코사인 유사도가 가장 높은 단어 구하기 (계산)\n",
    "단어들 사이의 의미/문법적 정보 도출\n",
    "단어 사이 문법적 관계 도출 (벡터 간 연산)\n",
    "```\n",
    "> 전이 학습(transfer learning) 임베딩\n",
    ">> 다른 딥러닝 모델의 입력값으로 자주 쓰임\n",
    ">> 품질 좋은 임베딩을 사용할수록 모델의 성능이 좋아짐\n",
    "[임베딩 레이어를 통해 word to vector](https://wikidocs.net/64779)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 15:01:54.224446: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# 숫자로 변환된 텍스트 데이터 [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 에 Embedding 레이어를 적용합니다. \u001b[39;00m\n\u001b[1;32m     14\u001b[0m raw_inputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(get_encoded_sentences(sentences, word_to_index), dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m output \u001b[39m=\u001b[39m embedding(raw_inputs)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/myconda/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/myconda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "# word to vector with tensorflow 예제\n",
    "# 아래 코드는 그대로 실행하시면 에러가 발생할 것입니다. \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 위 그림과 같이 4차원의 워드 벡터를 가정합니다. \n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# 숫자로 변환된 텍스트 데이터 [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 에 Embedding 레이어를 적용합니다. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype='object')\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer 주의점\n",
    "Embedding 레이어의 인풋 문장 벡터의 **길이는 일정**해야한다.</br>\n",
    "> raw_inputs에 있는 3개 베거의 길이는 각각 다음과 같다.\n",
    ">> 4, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 4 5 0]\n",
      " [1 3 6 7 0]\n",
      " [1 8 3 4 9]]\n"
     ]
    }
   ],
   "source": [
    "# TF의 함수 사용하여 input 벡터 길이 맞추기\n",
    "## tf.keras.preprocessing.sequence.pad_sequences() 사용\n",
    "## 문장 벡터 뒤에 패팅(<PAD>)을 추가하여 길이를 일정하게 맞춰줌\n",
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "print(raw_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <PAD>에는 0과 매핑되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.03563546 -0.02319768 -0.01442961  0.02279365]\n",
      "  [ 0.03037668 -0.04669473 -0.00695539 -0.04383923]\n",
      "  [ 0.03651122 -0.04469563 -0.02924101  0.01164202]\n",
      "  [-0.0459154  -0.03801433 -0.02539301 -0.0061772 ]\n",
      "  [-0.00923066  0.00127262  0.03522557 -0.02817357]]\n",
      "\n",
      " [[ 0.03563546 -0.02319768 -0.01442961  0.02279365]\n",
      "  [ 0.03037668 -0.04669473 -0.00695539 -0.04383923]\n",
      "  [ 0.00056914  0.00822072 -0.00984446  0.02501208]\n",
      "  [-0.04352775 -0.00191113  0.00325835  0.04944073]\n",
      "  [-0.00923066  0.00127262  0.03522557 -0.02817357]]\n",
      "\n",
      " [[ 0.03563546 -0.02319768 -0.01442961  0.02279365]\n",
      "  [-0.00036765  0.00811856 -0.0046363  -0.04263892]\n",
      "  [ 0.03037668 -0.04669473 -0.00695539 -0.04383923]\n",
      "  [ 0.03651122 -0.04469563 -0.02924101  0.01164202]\n",
      "  [ 0.0451659  -0.03582686 -0.01209273 -0.04598297]]], shape=(3, 5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# input 벡터들 (raw_inputs)의 길이를 맞춰주면서 word to vector를 수행하는 코드\n",
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 그림과 같이 4차원의 워드 벡터를 가정합니다.\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# tf.keras.preprocessing.sequence.pad_sequences를 통해 word vector를 모두 일정 길이로 맞춰주어야 \n",
    "# embedding 레이어의 input이 될 수 있음에 주의해 주세요. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype=object)\n",
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> output의 shape(3, 5, 4)에서 각 값의 의미\n",
    ">> 3 : 입력문장 개수\n",
    ">> 5 : 입력문장의 최대 길이\n",
    ">> 4 : 워드 벡터의 차원 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-6. 시퀀스 데이터를 다루는 RNN\n",
    "> 텍스트 데이터를 다루는데 주로 사용되는 딥러닝 모델\n",
    ">> Recurrent Neural Network (RNN)\n",
    ">> 시퀀스(Sequence) 형태의 데이터를 처리하기에 최적인 모델로 알려져 있음\n",
    "\n",
    "[stateful vs stateless 예시; Web의 관점에서](https://www.slideshare.net/xguru/ss-16106464)</br>\n",
    "[모두의 딥러닝 강좌 - 12강.RNN, 김성훈 교수](https://youtu.be/-SHPG_KMUkQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 4)           40        \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 8)                 416       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 537 (2.10 KB)\n",
      "Trainable params: 537 (2.10 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# RNN 모델을 사용하여 텍스트 데이터를 처리하는 예제 코드\n",
    "## 텍스트 데이터는 이전의 텍스트를 의미\n",
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 시퀀스 데이터와 RNN 참고자료\n",
    "[Youtube Link](https://youtu.be/mG6N0ut9dog?t=1447)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-7. 꼭 RNN이어야 할까?\n",
    "텍스트 처리에서는 RNN이 아니라 `1-D Convolution Neural Network (1-D CNN)`를 사용할수도 있다.</br>\n",
    "```text\n",
    "이전에 이미지 분류기 구현에서 `2-D CNN`을 사용해보았다.\n",
    "\n",
    " - 이미지는 시퀀스 데이터가 아니다.\n",
    "\n",
    "따라서 이미지 분류기 모델에서 사용된 입력은 이미지 전체다.\n",
    "```\n",
    "> 1-D CNN (with text data)\n",
    ">> 문장 전체를 한꺼번에 한 방향으로 스캐닝\n",
    ">> 한 번에 처리하는 양은 7 (len 7)\n",
    ">> 7 단어 이내에서 발견되는 특징을 추출하여 추출결과값을 문장 분류에 사용함\n",
    "\n",
    "`1-D CNN`방식도 텍스트 처리에서 RNN 못지않은 효율을 보여줌</br>\n",
    "> CNN 계열의 이점\n",
    ">> RNN 계열보다 병렬처리가 효율적이다\n",
    ">> 학습 속도가 RNN에 빠르게 진행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, None, 4)           40        \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 16)          464       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, None, 16)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 16)          1808      \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 16)                0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2457 (9.60 KB)\n",
      "Trainable params: 2457 (9.60 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1-D CNN을 활용하여 텍스트 데이터 처리 및 모델 학습\n",
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 4)           40        \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Gl  (None, 4)                 0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8)                 40        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89 (356.00 Byte)\n",
      "Trainable params: 89 (356.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1-D CNN 기법에서 GlobalMaxPooling() 레이어만 사용하는 예제\n",
    "## desc.) 전체 문장 중에서 단 하나의 가장 중요한 단어만 feature로 추출\n",
    "## 추출된 feature로 문장의 궁/부정을 평가\n",
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이 외에도 고려할만한 방법\n",
    ">> 1-D CNN과 RNN 섞어 쓰기\n",
    ">> FFN (FeedForward Network) 레이어만 사용해보기\n",
    ">> Transformer 레이어 사용해보기\n",
    "\n",
    "[참고링크](https://wikidocs.net/80437)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
