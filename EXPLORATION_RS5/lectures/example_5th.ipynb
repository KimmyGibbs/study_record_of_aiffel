{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-2. í…ìŠ¤íŠ¸ ê°ì •ë¶„ì„ì˜ ìœ ìš©ì„±\n",
    "\n",
    "í…ìŠ¤íŠ¸ ê°ì •ë¶„ì„ ì ‘ê·¼ë²•</br>\n",
    "- ê¸°ê³„í•™ìŠµ ê¸°ë°˜ ì ‘ê·¼ë²•\n",
    "- ê°ì„±ì‚¬ì „ ê¸°ë°˜ ì ‘ê·¼ë²•\n",
    "    - ê¸°ê³„í•™ìŠµ ëŒ€ë¹„ ë‹¤ìŒì˜ ë‹¨ì ë“¤ì´ ìˆë‹¤.\n",
    "        1. ë¶„ì„ ëŒ€ìƒì— ë”°ë¼ ë‹¨ì–´ì˜ ê°ì„± ì ìˆ˜ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ê°€ëŠ¥ì„±ì— ëŒ€ì‘í•˜ê¸° í˜ë“¦\n",
    "        2. ë‹¨ìˆœ ê¸ë¶€ì •ì„ ë„˜ì–´ì„œ ê¸ë¶€ì •ì˜ ì›ì¸ì´ ë˜ëŠ” ëŒ€ìƒ ì†ì„± ê¸°ë°˜ì˜ ê°ì„± ë¶„ì„ì´ ì–´ë ¤ì›€\n",
    "- ë°ì´í„°ë¶„ì„ ì—…ë¬´ ì¸¡ë©´ì—ì„œì˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸\n",
    "    - ì¼ë°˜ì ì¸ ë°ì´í„°ë¶„ì„ ì—…ë¬´ì—ì„œëŠ” ë²”ì£¼í™”ê°€ ì˜ëœ ì •í˜•ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•¨\n",
    "        - ì •í˜•ë°ì´í„°ëŠ” ê·œëª¨ê°€ ì»¤ì§ˆìˆ˜ë¡ ë¹„ìš©ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•  ìˆ˜ ìˆë‹¤.\n",
    "    - ë¹„ì •í˜•ë°ì´í„°ì¸ í…ìŠ¤íŠ¸ì— ê°ì„±ë¶„ì„ ê¸°ë²•ì„ ì‚¬ìš©\n",
    "        - í…ìŠ¤íŠ¸ë¥¼ ì •í˜•ë°ì´í„°ë¡œ ê°€ê³µí•˜ì—¬ ìœ ìš©í•œ ì˜ì‚¬ê²°ì • ë³´ì¡°ìë£Œë¡œ í™œìš© ê°€ëŠ¥í•¨\n",
    "\n",
    "\n",
    "ì›Œë“œ ì„ë² ë”©(word embedding) ê¸°ë²•</br>\n",
    "- ë‹¨ì–´ì˜ íŠ¹ì„±ì„ ì €ì°¨ì› ë²¡í„° ê°’ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ë²•\n",
    "- ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„±ë¶„ì„ ì‹œ ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆìŒ\n",
    "    - ë¨¸ì‹ ëŸ¬ë‹ ì¶”ë¡  ì¼ì¹˜ë„(ì •í™•ë„)ë¥¼ í¬ê²Œ í–¥ìƒí•  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-4. í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ íŠ¹ì§•\n",
    "## (1) í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "ë‹¨ì–´ì™€ ê·¸ **ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°**ë¥¼ ì§ì§€ì–´ ë³´ì.</br>\n",
    "> ë§ˆì¹˜ ì‚¬ì „ì—ì„œ íŠ¹ì • ë‹¨ì–´ì™€ ê·¸ ë‹¨ì–´ì— ëŒ€í•œ ì„¤ëª…ì´ ì§ì§€ì–´ì§„ ê²ƒ ì²˜ëŸ¼\n",
    "\n",
    "í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì œ\n",
    "```text\n",
    "## í…ìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì‹œ\n",
    "\n",
    "    i feel hungry\n",
    "    i eat lunch\n",
    "    now i feel happy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'feel', 'hungry']\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì¥ ì…ë ¥ ë° ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ split\n",
    "# ì²˜ë¦¬í•´ì•¼ í•  ë¬¸ì¥ì„ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ì— ì˜®ê²¨ ë‹´ì•˜ìŠµë‹ˆë‹¤.\n",
    "sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# íŒŒì´ì¬ split() ë©”ì†Œë“œë¥¼ ì´ìš©í•´ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ ìª¼ê°œ ë´…ë‹ˆë‹¤.\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"
     ]
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì „ì„ ë§Œë“¤ê¸° ìœ„í•œ ì˜ˆì‹œ\n",
    "## python dictionary í™œìš©\n",
    "\n",
    "index_to_word={}  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“¤ì–´ì„œ\n",
    "\n",
    "# ë‹¨ì–´ë“¤ì„ í•˜ë‚˜ì”© ì±„ì›Œ ë´…ë‹ˆë‹¤. ì±„ìš°ëŠ” ìˆœì„œëŠ” ì¼ë‹¨ ì„ì˜ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‚¬ì‹¤ ìˆœì„œëŠ” ì¤‘ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. \n",
    "# <BOS>, <PAD>, <UNK>ëŠ” ê´€ë¡€ì ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ ë§¨ ì•ì— ë„£ì–´ì¤ë‹ˆë‹¤. \n",
    "index_to_word[0]='<PAD>'  # íŒ¨ë”©ìš© ë‹¨ì–´\n",
    "index_to_word[1]='<BOS>'  # ë¬¸ì¥ì˜ ì‹œì‘ì§€ì \n",
    "index_to_word[2]='<UNK>'  # ì‚¬ì „ì— ì—†ëŠ”(Unknown) ë‹¨ì–´\n",
    "index_to_word[3]='i'\n",
    "index_to_word[4]='feel'\n",
    "index_to_word[5]='hungry'\n",
    "index_to_word[6]='eat'\n",
    "index_to_word[7]='lunch'\n",
    "index_to_word[8]='now'\n",
    "index_to_word[9]='happy'\n",
    "\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'i': 3, 'feel': 4, 'hungry': 5, 'eat': 6, 'lunch': 7, 'now': 8, 'happy': 9}\n"
     ]
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ê¸°\n",
    "## ì˜ˆì œ ê¸°ì¤€ì—ì„œ keyì™€ valueë¥¼ ë°”ê¿”ë²„ë¦¬ë©´ ëœë‹¤.\n",
    "word_to_index={word:index for index, word in index_to_word.items()}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# ë°”ë€ ë‹¨ì–´ì˜ ê°’(ìˆ«ì)ì„ í™•ì¸\n",
    "print(word_to_index['feel'])  # ë‹¨ì–´ 'feel'ì€ ìˆ«ì ì¸ë±ìŠ¤ 4ë¡œ ë°”ë€ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# ê°€ì§€ê³  ìˆëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë“¤ì„ ìˆ«ìë¡œ ë°”ê¿” í‘œí˜„í•˜ê¸°\n",
    "# ë¬¸ì¥ 1ê°œë¥¼ í™œìš©í•  ë”•ì…”ë„ˆë¦¬ì™€ í•¨ê»˜ ì£¼ë©´, ë‹¨ì–´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.\n",
    "# ë‹¨, ëª¨ë“  ë¬¸ì¥ì€ <BOS>ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒìœ¼ë¡œ í•©ë‹ˆë‹¤. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]\n"
     ]
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ ë°ì´í„° ì‚¬ì „í™” ì˜ˆì‹œ\n",
    "# ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ í•œêº¼ë²ˆì— ìˆ«ì í…ì„œë¡œ encodeí•´ ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] ê°€ ì•„ë˜ì™€ ê°™ì´ ë³€í™˜ë©ë‹ˆë‹¤. \n",
    "encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i feel hungry\n"
     ]
    }
   ],
   "source": [
    "# ìˆ«ì ê°’ìœ¼ë¡œ í‘œí˜„ëœ ë‹¨ì–´ë“¤ì„ ë‹¤ì‹œ ì›ë˜ ë‹¨ì–´(sentence)ë¡œ ë³µêµ¬í•˜ê¸°\n",
    "# ìˆ«ì ë²¡í„°ë¡œ encodeëœ ë¬¸ì¥ì„ ì›ë˜ëŒ€ë¡œ decodeí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]ë¥¼ í†µí•´ <BOS>ë¥¼ ì œì™¸\n",
    "\n",
    "print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i feel hungry', 'i eat lunch', 'now i feel happy']\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì¥ encode, decode ì˜ˆì‹œ\n",
    "# ì—¬ëŸ¬ ê°œì˜ ìˆ«ì ë²¡í„°ë¡œ encodeëœ ë¬¸ì¥ì„ í•œêº¼ë²ˆì— ì›ë˜ëŒ€ë¡œ decodeí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] ê°€ ì•„ë˜ì™€ ê°™ì´ ë³€í™˜ë©ë‹ˆë‹¤.\n",
    "print(get_decoded_sentences(encoded_sentences, index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-5. í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ íŠ¹ì§•\n",
    "## (2) Embedding ë ˆì´ì–´ì˜ ë“±ì¥\n",
    "ì„ë² ë”©(Embedding)?\n",
    "```text\n",
    "ìì—°ì–´ ì²˜ë¦¬(Natural Language Processing)ë¶„ì•¼ì—ì„œ ë§í•˜ëŠ” ì„ë² ë”©(Embedding)\n",
    "- ì‚¬ëŒì´ ì“°ëŠ” ìì—°ì–´ë¥¼ ê¸°ê»˜ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ìí˜•íƒœ(vector)ë¡œ ë°”ê¾¼ ê²°ê³¼\n",
    "- í˜¹ì¸ ê·¸ ì¼ë ¨ì˜ ê³¼ì • ì „ì²´\n",
    "```\n",
    "What to do with Embedding?\n",
    "```text\n",
    "ë‹¨ì–´ë‚˜ ë¬¸ì¥ ì‚¬ì´ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ ë‹¨ì–´ êµ¬í•˜ê¸° (ê³„ì‚°)\n",
    "ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ì˜ë¯¸/ë¬¸ë²•ì  ì •ë³´ ë„ì¶œ\n",
    "ë‹¨ì–´ ì‚¬ì´ ë¬¸ë²•ì  ê´€ê³„ ë„ì¶œ (ë²¡í„° ê°„ ì—°ì‚°)\n",
    "```\n",
    "> ì „ì´ í•™ìŠµ(transfer learning) ì„ë² ë”©\n",
    ">> ë‹¤ë¥¸ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ìì£¼ ì“°ì„\n",
    ">> í’ˆì§ˆ ì¢‹ì€ ì„ë² ë”©ì„ ì‚¬ìš©í• ìˆ˜ë¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ì•„ì§\n",
    "[LINK: ì„ë² ë”© ë ˆì´ì–´ë¥¼ í†µí•´ word to vector](https://wikidocs.net/64779)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 06:25:20.697541: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# ìˆ«ìë¡œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ ë°ì´í„° [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] ì— Embedding ë ˆì´ì–´ë¥¼ ì ìš©í•©ë‹ˆë‹¤. \u001b[39;00m\n\u001b[1;32m     14\u001b[0m raw_inputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(get_encoded_sentences(sentences, word_to_index), dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m output \u001b[39m=\u001b[39m embedding(raw_inputs)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/myconda/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/myconda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "# Embedding ë ˆì´ì–´ë¥¼ í™œìš©í•˜ì—¬ ì´ì „ ìŠ¤í…ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì›Œë“œ ë²¡í„° í…ì„œ í˜•íƒœë¡œ ë‹¤ì‹œ í‘œí˜„\n",
    "# ì•„ë˜ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ì‹¤í–‰í•˜ì‹œë©´ ì—ëŸ¬ê°€ ë°œìƒí•  ê²ƒì…ë‹ˆë‹¤. \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "vocab_size = len(word_to_index)  # ìœ„ ì˜ˆì‹œì—ì„œ ë”•ì…”ë„ˆë¦¬ì— í¬í•¨ëœ ë‹¨ì–´ ê°œìˆ˜ëŠ” 10\n",
    "word_vector_dim = 4    # ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ 4ì°¨ì›ì˜ ì›Œë“œ ë²¡í„°ë¥¼ ê°€ì •í•©ë‹ˆë‹¤. \n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# ìˆ«ìë¡œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ ë°ì´í„° [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] ì— Embedding ë ˆì´ì–´ë¥¼ ì ìš©í•©ë‹ˆë‹¤. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype='object')\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer ì£¼ì˜ì \n",
    "#### ìœ„ cellì˜ ì½”ë“œê°€ ì—ëŸ¬ë‚œ ì´ìœ \n",
    "> Embedding ë ˆì´ì–´ì˜ ì¸í’‹ ë¬¸ì¥ ë²¡í„°ì˜ **ê¸¸ì´ëŠ” ì¼ì •**í•´ì•¼í•œë‹¤.</br>\n",
    ">> raw_inputsì— ìˆëŠ” 3ê°œ ë²¡í„°ì˜ ê¸¸ì´ëŠ” ê°ê° ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    ">> 4, 4, 5\n",
    "\n",
    "```text\n",
    "Tensorflowì—ì„œëŠ” ë‹¤ìŒ í•¨ìˆ˜ë¥¼ í†µí•´ ë¬¸ì¥ ë²¡í„° ë’¤ì— íŒ¨ë”©( <PAD> )ì„ ì¶”ê°€í•˜ì—¬ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶°ì¤€ë‹¤.\n",
    "```\n",
    "`tf.keras.preprocessing.sequence.pad_sequences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 4 5 0]\n",
      " [1 3 6 7 0]\n",
      " [1 8 3 4 9]]\n"
     ]
    }
   ],
   "source": [
    "## tf.keras.preprocessing.sequence.pad_sequences() ì½”ë“œ ì˜ˆì‹œ\n",
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "print(raw_inputs)\n",
    "\n",
    "## padding ê°’ì˜ defaultëŠ” 'pre'ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<PAD>`ê°€ 0ê³¼ ë§¤í•‘ë˜ì–´ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.02797605 -0.00805179 -0.00140411 -0.00804634]\n",
      "  [-0.00691222 -0.01397717  0.00762901  0.01910103]\n",
      "  [ 0.03633161 -0.03766296 -0.00030882  0.00575482]\n",
      "  [ 0.00998298 -0.01156216  0.01873604 -0.03365401]\n",
      "  [-0.01435258 -0.02897682  0.0341973   0.01650895]]\n",
      "\n",
      " [[-0.02797605 -0.00805179 -0.00140411 -0.00804634]\n",
      "  [-0.00691222 -0.01397717  0.00762901  0.01910103]\n",
      "  [-0.00497545  0.01191349 -0.00841505 -0.02019   ]\n",
      "  [-0.01158576  0.04504373 -0.02730181 -0.00517433]\n",
      "  [-0.01435258 -0.02897682  0.0341973   0.01650895]]\n",
      "\n",
      " [[-0.02797605 -0.00805179 -0.00140411 -0.00804634]\n",
      "  [ 0.0422066   0.03172399  0.01955319  0.0169205 ]\n",
      "  [-0.00691222 -0.01397717  0.00762901  0.01910103]\n",
      "  [ 0.03633161 -0.03766296 -0.00030882  0.00575482]\n",
      "  [-0.0476425  -0.01281666  0.01041086  0.02598471]]], shape=(3, 5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# input ë²¡í„°ë“¤ (raw_inputs)ì˜ ê¸¸ì´ë¥¼ ë§ì¶°ì£¼ë©´ì„œ word to vectorë¥¼ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œ\n",
    "vocab_size = len(word_to_index)  # ìœ„ ì˜ˆì‹œì—ì„œ ë”•ì…”ë„ˆë¦¬ì— í¬í•¨ëœ ë‹¨ì–´ ê°œìˆ˜ëŠ” 10\n",
    "word_vector_dim = 4    # ê·¸ë¦¼ê³¼ ê°™ì´ 4ì°¨ì›ì˜ ì›Œë“œ ë²¡í„°ë¥¼ ê°€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# tf.keras.preprocessing.sequence.pad_sequencesë¥¼ í†µí•´ word vectorë¥¼ ëª¨ë‘ ì¼ì • ê¸¸ì´ë¡œ ë§ì¶°ì£¼ì–´ì•¼ \n",
    "# embedding ë ˆì´ì–´ì˜ inputì´ ë  ìˆ˜ ìˆìŒì— ì£¼ì˜í•´ ì£¼ì„¸ìš”. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype=object)\n",
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. outputì˜ shape(3, 5, 4)ì—ì„œ ê° ê°’ì˜ ì˜ë¯¸\n",
    ">> 3 : ì…ë ¥ë¬¸ì¥ ê°œìˆ˜\n",
    ">> 5 : ì…ë ¥ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´\n",
    ">> 4 : ì›Œë“œ ë²¡í„°ì˜ ì°¨ì› ìˆ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-6. ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” RNN\n",
    "- í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ”ë° ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸\n",
    "    - Recurrent Neural Network (RNN)\n",
    "    - ì‹œí€€ìŠ¤(Sequence) í˜•íƒœì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸°ì— ìµœì ì¸ ëª¨ë¸ë¡œ ì•Œë ¤ì ¸ ìˆìŒ\n",
    "\n",
    "[stateful vs stateless ì˜ˆì‹œ; Webì˜ ê´€ì ì—ì„œ](https://www.slideshare.net/xguru/ss-16106464)</br>\n",
    "[ëª¨ë‘ì˜ ë”¥ëŸ¬ë‹ ê°•ì¢Œ - 12ê°•.RNN, ê¹€ì„±í›ˆ êµìˆ˜](https://youtu.be/-SHPG_KMUkQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 4)           40        \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 8)                 416       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 537 (2.10 KB)\n",
      "Trainable params: 537 (2.10 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# RNN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì œ ì½”ë“œ\n",
    "## í…ìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì´ì „ cellì˜ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸\n",
    "vocab_size = 10  # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°ì…ë‹ˆë‹¤(10ê°œì˜ ë‹¨ì–´)\n",
    "word_vector_dim = 4  # ë‹¨ì–´ í•˜ë‚˜ë¥¼ í‘œí˜„í•˜ëŠ” ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›ìˆ˜ì…ë‹ˆë‹¤. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(8))   # ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” RNNì¸ LSTM ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ë•Œ LSTM state ë²¡í„°ì˜ ì°¨ì›ìˆ˜ëŠ” 8ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤. (ë³€ê²½ ê°€ëŠ¥)\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # ìµœì¢… ì¶œë ¥ì€ ê¸ì •/ë¶€ì •ì„ ë‚˜íƒ€ë‚´ëŠ” 1dim ì…ë‹ˆë‹¤.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ì‹œí€€ìŠ¤ ë°ì´í„°ì™€ RNN ì°¸ê³ ìë£Œ\n",
    "\n",
    "[Youtube Link](https://youtu.be/mG6N0ut9dog?t=1447)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-7. ê¼­ RNNì´ì–´ì•¼ í• ê¹Œ?\n",
    "í…ìŠ¤íŠ¸ ì²˜ë¦¬ì—ì„œëŠ” RNNì´ ì•„ë‹ˆë¼ `1-D Convolution Neural Network (1-D CNN)`ë¥¼ ì‚¬ìš©í• ìˆ˜ë„ ìˆë‹¤.</br>\n",
    "```text\n",
    "ì´ì „ì— ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° êµ¬í˜„ì—ì„œ `2-D CNN`ì„ ì‚¬ìš©í•´ë³´ì•˜ë‹¤.\n",
    "\n",
    " - ì´ë¯¸ì§€ëŠ” ì‹œí€€ìŠ¤ ë°ì´í„°ê°€ ì•„ë‹ˆë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° ëª¨ë¸ì—ì„œ ì‚¬ìš©ëœ ì…ë ¥ì€ ì´ë¯¸ì§€ ì „ì²´ë‹¤.\n",
    "```\n",
    "**1-D CNN (with text data)**\n",
    "- ë¬¸ì¥ ì „ì²´ë¥¼ í•œêº¼ë²ˆì— í•œ ë°©í–¥ìœ¼ë¡œ ìŠ¤ìºë‹\n",
    "- í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ë¬¸ì¥ì˜ ì–‘ì€ 7 (len 7)\n",
    "- 7 ë‹¨ì–´ ì´ë‚´ì—ì„œ ë°œê²¬ë˜ëŠ” íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ ì¶”ì¶œê²°ê³¼ê°’ì„ ë¬¸ì¥ ë¶„ë¥˜ì— ì‚¬ìš©í•¨\n",
    "\n",
    "`1-D CNN`ë°©ì‹ë„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ì—ì„œ RNN ëª»ì§€ì•Šì€ íš¨ìœ¨ì„ ë³´ì—¬ì¤Œ</br>\n",
    "**CNN ê³„ì—´ì˜ ì´ì **\n",
    "- RNN ê³„ì—´ë³´ë‹¤ ë³‘ë ¬ì²˜ë¦¬ê°€ íš¨ìœ¨ì ì´ë‹¤\n",
    "- í•™ìŠµ ì†ë„ê°€ RNNì— ë¹ ë¥´ê²Œ ì§„í–‰ëœë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, None, 4)           40        \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 16)          464       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, None, 16)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 16)          1808      \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 16)                0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2457 (9.60 KB)\n",
      "Trainable params: 2457 (9.60 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1-D CNNì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ\n",
    "vocab_size = 10  # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°ì…ë‹ˆë‹¤(10ê°œì˜ ë‹¨ì–´)\n",
    "word_vector_dim = 4   # ë‹¨ì–´ í•˜ë‚˜ë¥¼ í‘œí˜„í•˜ëŠ” ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› ìˆ˜ì…ë‹ˆë‹¤. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # ìµœì¢… ì¶œë ¥ì€ ê¸ì •/ë¶€ì •ì„ ë‚˜íƒ€ë‚´ëŠ” 1dim ì…ë‹ˆë‹¤.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1-D CNN` ê¸°ë²•</br>\n",
    "- `GlobalMaxPooling()` ë ˆì´ì–´ë§Œ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ìŒ“ëŠ” ë°©ì‹\n",
    "\n",
    "**GlobalMaxPooling** ë°©ì‹</br>\n",
    "- ì „ì²´ ë¬¸ì¥ ì¤‘ì—ì„œ ë‹¨ í•˜ë‚˜ì˜ ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´ë§Œ featureë¡œ ì¶”ì¶œí•˜ëŠ” ë°©ì‹\n",
    "    - ì¶”ì¶œëœ featureëŠ” ë¬¸ì¥ì˜ ê¸ì •/ë¶€ì •ì„ í‰ê°€í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 4)           40        \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Gl  (None, 4)                 0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8)                 40        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89 (356.00 Byte)\n",
      "Trainable params: 89 (356.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1-D CNN ê¸°ë²•ì—ì„œ GlobalMaxPooling() ë ˆì´ì–´ë§Œ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œ\n",
    "\n",
    "vocab_size = 10  # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°ì…ë‹ˆë‹¤(10ê°œì˜ ë‹¨ì–´)\n",
    "word_vector_dim = 4   # ë‹¨ì–´ í•˜ë‚˜ë¥¼ í‘œí˜„í•˜ëŠ” ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› ìˆ˜ì…ë‹ˆë‹¤. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # ìµœì¢… ì¶œë ¥ì€ ê¸ì •/ë¶€ì •ì„ ë‚˜íƒ€ë‚´ëŠ” 1dim ì…ë‹ˆë‹¤.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ì´ ì™¸ì—ë„ ê³ ë ¤í• ë§Œí•œ ë°©ë²•\n",
    "\n",
    "- 1-D CNNê³¼ RNN ì„ì–´ ì“°ê¸°\n",
    "- FFN (FeedForward Network) ë ˆì´ì–´ë§Œ ì‚¬ìš©í•´ë³´ê¸°\n",
    "- Transformer ë ˆì´ì–´ ì‚¬ìš©í•´ë³´ê¸°\n",
    "\n",
    "[ì°¸ê³ ë§í¬](https://wikidocs.net/80437)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-8. IMDB ì˜í™”ë¦¬ë·° ê°ì„±ë¶„ì„\n",
    "## (1) IMDB ë°ì´í„°ì…‹ ë¶„ì„\n",
    "IMDb ì˜í™”ë¦¬ë·° ê°ì„±ë¶„ì„ íƒœìŠ¤í¬ ë„ì „ ì˜ˆì œ</br>\n",
    "**IMDb Large Movie Dataset**</br>\n",
    "- 50000ê°œì˜ ì˜ë‹¨ì–´ë¡œ ì‘ì„±ëœ ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸\n",
    "- ê¸ì • ë¦¬ë·°ì—ëŠ” `1`, ë¶€ì • ë¦¬ë·°ì—ëŠ” `0`ì˜ ë¼ë²¨ì´ ë‹¬ë ¤ ìˆë‹¤.\n",
    "\n",
    "[ê´€ë ¨ë…¼ë¬¸](https://aclanthology.org/P11-1015.pdf)</br>\n",
    "\n",
    "> ì•„ë˜ëŠ” ë…¸ë“œì— ì–¸ê¸‰ëœ ë‚´ìš©\n",
    "```text\n",
    "50000ê°œì˜ ë¦¬ë·° ì¤‘ ì ˆë°˜ì¸ 25000ê°œê°€ í›ˆë ¨ìš© ë°ì´í„°, ë‚˜ë¨¸ì§€ 25000ê°œë¥¼ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ë„ë¡ ì§€ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ tensorflow Keras ë°ì´í„°ì…‹ ì•ˆì— í¬í•¨ë˜ì–´ ìˆì–´ì„œ ì†ì‰½ê²Œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "ì´í›„ ìŠ¤í…ì˜ IMDb ë°ì´í„°ì…‹ ì²˜ë¦¬ ì½”ë“œ ì¤‘ ì¼ë¶€ëŠ” Tensorflow íŠœí† ë¦¬ì–¼ì— ì–¸ê¸‰ëœ ë°ì´í„° ì „ì²˜ë¦¬ ë¡œì§ì„ ì°¸ê³ í•˜ì˜€ìŒì„ ë°í™ë‹ˆë‹¤.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í›ˆë ¨ ìƒ˜í”Œ ê°œìˆ˜: 25000, í…ŒìŠ¤íŠ¸ ê°œìˆ˜: 25000\n"
     ]
    }
   ],
   "source": [
    "# IMDb ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° (train)ì™€ ì •ë‹µ ë°ì´í„°(test)ë¡œ ë¶„ë¦¬\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "print(f\"í›ˆë ¨ ìƒ˜í”Œ ê°œìˆ˜: {len(x_train)}, í…ŒìŠ¤íŠ¸ ê°œìˆ˜: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "ë¼ë²¨:  1\n",
      "1ë²ˆì§¸ ë¦¬ë·° ë¬¸ì¥ ê¸¸ì´:  218\n",
      "2ë²ˆì§¸ ë¦¬ë·° ë¬¸ì¥ ê¸¸ì´:  189\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ì˜ ì˜ˆì‹œ í™•ì¸\n",
    "print(x_train[0])  # 1ë²ˆì§¸ ë¦¬ë·°ë°ì´í„°\n",
    "print('ë¼ë²¨: ', y_train[0])  # 1ë²ˆì§¸ ë¦¬ë·°ë°ì´í„°ì˜ ë¼ë²¨\n",
    "print('1ë²ˆì§¸ ë¦¬ë·° ë¬¸ì¥ ê¸¸ì´: ', len(x_train[0]))\n",
    "print('2ë²ˆì§¸ ë¦¬ë·° ë¬¸ì¥ ê¸¸ì´: ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMDb ë°ì´í„°ì…‹ íŠ¹ì§•**</br>\n",
    "- í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì•„ë‹ˆë‹¤\n",
    "- ê° í…ìŠ¤íŠ¸(ë‹¨ì–´)ë“¤ì€ ì´ë¯¸ ìˆ«ìë¡œ `encode`ë˜ì–´ ìˆë‹¤.\n",
    "    - ì´ë¼í•œ íŠ¹ì§• ë•Œë¬¸ì— IMDb ë°ì´í„°ì…‹ì€ encodeì— ì‚¬ìš©í•œ ì‚¬ì „(ë”•ì…”ë„ˆë¦¬)ê¹Œì§€ í•¨ê»˜ ì œê³µí•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# IMDb ë°ì´í„°ì…‹ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´;\n",
    "## get_word_index()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ ìˆ«ìë¡œ encodeëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì½ì–´ì˜¤ê²Œ ë¨\n",
    "word_to_index = imdb.get_word_index()\n",
    "## ì½ì–´ì˜¨ ë°ì´í„°(word_to_index)ë¥¼ ìˆ«ìí‚¤(ì¸ë±ìŠ¤)ë¡œë„ ì½ì„ ìˆ˜ ìˆê²Œ python dictionary ì¬êµ¬ì„± \n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])     # 'the' ê°€ ì¶œë ¥ë©ë‹ˆë‹¤. \n",
    "print(word_to_index['the'])  # 1 ì´ ì¶œë ¥ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ë…¸ë“œì—ì„œ ì–¸ê¸‰í•œ ì£¼ì˜ì \n",
    "\n",
    "```text\n",
    "ì—¬ê¸°ì„œ ì£¼ì˜í•  ì ì´ ìˆìŠµë‹ˆë‹¤. IMDb ë°ì´í„°ì…‹ì˜ í…ìŠ¤íŠ¸ ì¸ì½”ë”©ì„ ìœ„í•œ word_to_index, index_to_wordëŠ” ë³´ì •ì´ í•„ìš”í•œë°ìš”.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰ì‹œì¼œë³´ë©´ ë³´ì •ì´ ë˜ì§€ ì•Šì€ ìƒíƒœë¼ ë¬¸ì¥ì´ ì´ìƒí•¨ì„ í™•ì¸í•˜ì‹¤ ê²ë‹ˆë‹¤. (ë’¤ì— ë³´ì • í›„ ë‹¤ì‹œ í™•ì¸í•´ë³¼ ì˜ˆì •ì´ì—ìš”.ğŸ˜Š)\n",
    "```\n",
    "\n",
    "IMDB ë°ì´í„°ì…‹ì„ í…ìŠ¤íŠ¸ ì¸ì½”ë”© í•˜ê¸° ìœ„í•´ì„œ `word_to_index`, `index_to_word`ë“¤ì— ëŒ€í•´ ë³´ì •ì´ í•„ìš”í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n"
     ]
    }
   ],
   "source": [
    "# ë³´ì • ì „ x_train[0] ë°ì´í„°\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\n",
      "4\n",
      "the\n",
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "## x_train ë°ì´í„° ë³´ì • ì‘ì—… ì˜ˆì‹œ\n",
    "#ì‹¤ì œ ì¸ì½”ë”© ì¸ë±ìŠ¤ëŠ” ì œê³µëœ word_to_indexì—ì„œ index ê¸°ì¤€ìœ¼ë¡œ 3ì”© ë’¤ë¡œ ë°€ë ¤ ìˆìŠµë‹ˆë‹¤.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# ì²˜ìŒ ëª‡ ê°œ ì¸ë±ìŠ¤ëŠ” ì‚¬ì „ì— ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "word_to_index[\"<PAD>\"] = 0      # padding\n",
    "word_to_index[\"<BOS>\"] = 1      # begining of sentence\n",
    "word_to_index[\"<UNK>\"] = 2      # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3   # unused\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])     # '<BOS>' ê°€ ì¶œë ¥ë©ë‹ˆë‹¤. \n",
    "print(word_to_index['the'])  # 4 ì´ ì¶œë ¥ë©ë‹ˆë‹¤. \n",
    "print(index_to_word[4])     # 'the' ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
    "\n",
    "# ë³´ì • í›„ x_train[0] ë°ì´í„°\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "ë¼ë²¨:  1\n"
     ]
    }
   ],
   "source": [
    "# encode ëœ ë°ì´í„°ë¥¼ decode í•˜ì—¬ ê²°ê³¼ í™•ì¸í•˜ëŠ” ì˜ˆì‹œ\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))\n",
    "print('ë¼ë²¨: ', y_train[0])  # 1ë²ˆì§¸ ë¦¬ë·°ë°ì´í„°ì˜ ë¼ë²¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ë…¸ë“œì—ì„œ ì–¸ê¸‰í•  ì£¼ì˜ì \n",
    "\n",
    "```text\n",
    "pad_sequencesë¥¼ í†µí•´ ë°ì´í„°ì…‹ ìƒì˜ ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ í†µì¼í•˜ëŠ” ê²ƒì„ ìŠì–´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.\n",
    "ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´ maxlenì˜ ê°’ ì„¤ì •ë„ ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ê²Œ ë©ë‹ˆë‹¤. ì´ ê¸¸ì´ë„ ì ì ˆí•œ ê°’ì„ ì°¾ê¸° ìœ„í•´ì„œëŠ” ì „ì²´ ë°ì´í„°ì…‹ì˜ ë¶„í¬ë¥¼ í™•ì¸í•´ ë³´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "```\n",
    ">> `pad_sequences`: ë°ì´í„°ì…‹ ìƒì˜ ë¬¸ì¥ ê¸¸ì´ë¥¼ í†µì¼\n",
    ">> ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´ (`maxlen`ì˜ ê°’) ì„¤ì •ë„ ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì¥ê¸¸ì´ í‰ê·  :  234.75892\n",
      "ë¬¸ì¥ê¸¸ì´ ìµœëŒ€ :  2494\n",
      "ë¬¸ì¥ê¸¸ì´ í‘œì¤€í¸ì°¨ :  172.91149458735703\n",
      "pad_sequences maxlen :  580\n",
      "ì „ì²´ ë¬¸ì¥ì˜ 0.94536%ê°€ maxlen ì„¤ì •ê°’ ì´ë‚´ì— í¬í•¨ë©ë‹ˆë‹¤. \n"
     ]
    }
   ],
   "source": [
    "# ì „ì²´ ë°ì´í„°ì…‹ í™•ì¸ì˜ˆì‹œ\n",
    "\n",
    "total_data_text = list(x_train) + list(x_test)\n",
    "# í…ìŠ¤íŠ¸ë°ì´í„° ë¬¸ì¥ê¸¸ì´ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œ í›„\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# ë¬¸ì¥ê¸¸ì´ì˜ í‰ê· ê°’, ìµœëŒ€ê°’, í‘œì¤€í¸ì°¨ë¥¼ ê³„ì‚°í•´ ë³¸ë‹¤. \n",
    "print('ë¬¸ì¥ê¸¸ì´ í‰ê·  : ', np.mean(num_tokens))\n",
    "print('ë¬¸ì¥ê¸¸ì´ ìµœëŒ€ : ', np.max(num_tokens))\n",
    "print('ë¬¸ì¥ê¸¸ì´ í‘œì¤€í¸ì°¨ : ', np.std(num_tokens))\n",
    "\n",
    "# ì˜ˆë¥¼ë“¤ì–´, ìµœëŒ€ ê¸¸ì´ë¥¼ (í‰ê·  + 2*í‘œì¤€í¸ì°¨)ë¡œ í•œë‹¤ë©´,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print(f'ì „ì²´ ë¬¸ì¥ì˜ {np.sum(num_tokens < max_tokens) / len(num_tokens)}%ê°€ maxlen ì„¤ì •ê°’ ì´ë‚´ì— í¬í•¨ë©ë‹ˆë‹¤. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ë…¸ë“œì—ì„œ ì–¸ê¸‰í•œ ë‚´ìš©\n",
    "\n",
    "```text\n",
    "ìœ„ì˜ ê²½ìš°ì—ëŠ” maxlen=580ì´ ë©ë‹ˆë‹¤.\n",
    "ë˜ í•œ ê°€ì§€ ìœ ì˜í•´ì•¼ í•˜ëŠ” ê²ƒì€ padding ë°©ì‹ì„ ë¬¸ì¥ ë’¤ìª½('post')ê³¼ ì•ìª½('pre') ì¤‘ ì–´ëŠ ìª½ìœ¼ë¡œ í•˜ëŠëƒì— ë”°ë¼ RNNì„ ì´ìš©í•œ ë”¥ëŸ¬ë‹ ì ìš© ì‹œ ì„±ëŠ¥ ì°¨ì´ê°€ ë°œìƒí•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.\n",
    "ë‘ ê°€ì§€ ë°©ì‹ì„ í•œ ë²ˆì”© ë‹¤ ì ìš©í•´ì„œ RNNì„ í•™ìŠµì‹œì¼œ ë³´ë©´ì„œ ê·¸ ê²°ê³¼ë¥¼ ë¹„êµí•´ ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
    "```\n",
    "> ë²¡í„° ê¸¸ì´ ë§ì¶œ ë•Œ, padding ë°©ì‹ì—ëŠ” ë‘ ê°€ì§€ ë°©ì‹ì´ ìˆë‹¤.\n",
    "\n",
    "1. ê¸¸ì´ë¥¼ ë§ì¶œ ë•Œ, padding ê°’ì„ ë’¤ìª½ì— ì‚½ì…í•˜ëŠ” ë°©ì‹: `post`\n",
    "2. ê¸¸ì´ë¥¼ ë§ì¶œ ë•Œ, padding ê°’ì„ ì•ìª½ì— ì‚½ì…í•˜ëŠ” ë°©ì‹: `pre`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì˜ˆì œ([Link](https://wikidocs.net/24586))ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ê°€ì§€ì˜ í•¨ìˆ˜ë¥¼ ì„ ì–¸ ë° í…ŒìŠ¤íŠ¸ ìˆ˜í–‰</br>\n",
    "\n",
    "> post_rnn_test()\n",
    ">> post-paddingìœ¼ë¡œ pre-processingí•œ ë°ì´í„° ê¸°ë°˜ RNN í•™ìŠµ\n",
    "> pre_rnn_test()\n",
    ">> pre-paddingìœ¼ë¡œ pre-processingí•œ ë°ì´í„° ê¸°ë°˜ RNN í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST PADDING ë°©ì‹ì˜ rnn ì‹¤í–‰ ì˜ˆì œ í•¨ìˆ˜ ì„ ì–¸\n",
    "def post_rnn_test(x_train, x_test, y_train, y_test, maxlen):\n",
    "    # ì˜ˆì œì—ì„œ ì‚¬ìš©í•œ ëª¨ë“ˆ\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from tensorflow.keras.models import load_model\n",
    "    # ì˜ˆì œ ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "    vocab_size = 10000\n",
    "    # max_lenì€ ë…¸ë“œ ìˆœì„œìƒ ì„ ì–¸ì´ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•¨\n",
    "    # ì•„ë˜ëŠ” ìƒ˜í”Œ ê°’\n",
    "    ## max_len = 500\n",
    "\n",
    "    # POST íŒ¨ë”©; êµ¬ì„±ì€ ë…¸ë“œ ì°¸ê³ í•¨\n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='post', # í˜¹ì€ 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post', # í˜¹ì€ 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "    # ê³µì‹ì‚¬ì´íŠ¸ ê¸°ì¤€ í•˜ì´í¼ íŒŒë¼ë¯¸í„°\n",
    "    ## í•˜ì´í¼íŒŒë¼ë¯¸í„°ì¸ ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›ì€ 100, ì€ë‹‰ ìƒíƒœì˜ í¬ê¸°ëŠ” 128ì…ë‹ˆë‹¤.\n",
    "    embedding_dim = 100\n",
    "    hidden_units = 128\n",
    "\n",
    "    # Sequential ëª¨ë¸ ì ìš©\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim))\n",
    "    model.add(GRU(hidden_units))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    ## EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)ëŠ”\n",
    "    ## ê²€ì¦ ë°ì´í„° ì†ì‹¤(val_loss)ì´ ì¦ê°€í•˜ë©´, ê³¼ì í•© ì§•í›„ë¯€ë¡œ ê²€ì¦ ë°ì´í„° ì†ì‹¤ì´ 4íšŒ ì¦ê°€í•˜ë©´\n",
    "    ## ì •í•´ì§„ ì—í¬í¬ê°€ ë„ë‹¬í•˜ì§€ ëª»í•˜ì˜€ë”ë¼ë„ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œ(Early Stopping)í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "    ## ModelCheckpointë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ì¦ ë°ì´í„°ì˜ ì •í™•ë„(val_acc)ê°€ ì´ì „ë³´ë‹¤ ì¢‹ì•„ì§ˆ ê²½ìš°ì—ë§Œ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    mc = ModelCheckpoint('GRU_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    ## validation_split=0.2ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ì˜ 20%ë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ë¶„ë¦¬í•´ì„œ ì‚¬ìš©í•˜ê³ ,\n",
    "    ## ê²€ì¦ ë°ì´í„°ë¥¼ í†µí•´ì„œ í›ˆë ¨ì´ ì ì ˆíˆ ë˜ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "    ## ê²€ì¦ ë°ì´í„°ëŠ” ê¸°ê³„ê°€ í›ˆë ¨ ë°ì´í„°ì— ê³¼ì í•©ë˜ê³  ìˆì§€ëŠ” ì•Šì€ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "    history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)\n",
    "    \n",
    "    # GRU ëª¨ë¸ì„ ë¶ˆëŸ¬ ê²€ì¦ë°ì´í„°ì˜ ì •í™•ë„ í™•ì¸\n",
    "    ## í›ˆë ¨ ê³¼ì •ì—ì„œ ê²€ì¦ ë°ì´í„°ì˜ ì •í™•ë„ê°€ ê°€ì¥ ë†’ì•˜ì„ ë•Œ ì €ì¥ëœ ëª¨ë¸ì¸ 'GRU_model.h5'ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    loaded_model = load_model('GRU_model.h5')\n",
    "    print(\"\\n post-padding ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì •í™•ë„: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "# PRE PADDING ë°©ì‹ì˜ rnn ì‹¤í–‰ ì˜ˆì œ í•¨ìˆ˜ ì„ ì–¸\n",
    "def pre_rnn_test(x_train, x_test, y_train, y_test, maxlen):\n",
    "    # ì˜ˆì œì—ì„œ ì‚¬ìš©í•œ ëª¨ë“ˆ\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from tensorflow.keras.models import load_model\n",
    "    # ì˜ˆì œ ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "    vocab_size = 10000\n",
    "    # max_lenì€ ë…¸ë“œ ìˆœì„œìƒ ì„ ì–¸ì´ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•¨\n",
    "    # ì•„ë˜ëŠ” ìƒ˜í”Œ ê°’\n",
    "    ## max_len = 500\n",
    "\n",
    "    # POST íŒ¨ë”©; êµ¬ì„±ì€ ë…¸ë“œ ì°¸ê³ í•¨\n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # í˜¹ì€ 'post'\n",
    "                                                        maxlen=maxlen)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # í˜¹ì€ 'post'\n",
    "                                                       maxlen=maxlen)\n",
    "    # ê³µì‹ì‚¬ì´íŠ¸ ê¸°ì¤€ í•˜ì´í¼ íŒŒë¼ë¯¸í„°\n",
    "    ## í•˜ì´í¼íŒŒë¼ë¯¸í„°ì¸ ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›ì€ 100, ì€ë‹‰ ìƒíƒœì˜ í¬ê¸°ëŠ” 128ì…ë‹ˆë‹¤.\n",
    "    embedding_dim = 100\n",
    "    hidden_units = 128\n",
    "\n",
    "    # Sequential ëª¨ë¸ ì ìš©\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim))\n",
    "    model.add(GRU(hidden_units))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    ## EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)ëŠ”\n",
    "    ## ê²€ì¦ ë°ì´í„° ì†ì‹¤(val_loss)ì´ ì¦ê°€í•˜ë©´, ê³¼ì í•© ì§•í›„ë¯€ë¡œ ê²€ì¦ ë°ì´í„° ì†ì‹¤ì´ 4íšŒ ì¦ê°€í•˜ë©´\n",
    "    ## ì •í•´ì§„ ì—í¬í¬ê°€ ë„ë‹¬í•˜ì§€ ëª»í•˜ì˜€ë”ë¼ë„ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œ(Early Stopping)í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "    ## ModelCheckpointë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ì¦ ë°ì´í„°ì˜ ì •í™•ë„(val_acc)ê°€ ì´ì „ë³´ë‹¤ ì¢‹ì•„ì§ˆ ê²½ìš°ì—ë§Œ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    mc = ModelCheckpoint('GRU_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    ## validation_split=0.2ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ì˜ 20%ë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ë¶„ë¦¬í•´ì„œ ì‚¬ìš©í•˜ê³ ,\n",
    "    ## ê²€ì¦ ë°ì´í„°ë¥¼ í†µí•´ì„œ í›ˆë ¨ì´ ì ì ˆíˆ ë˜ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "    ## ê²€ì¦ ë°ì´í„°ëŠ” ê¸°ê³„ê°€ í›ˆë ¨ ë°ì´í„°ì— ê³¼ì í•©ë˜ê³  ìˆì§€ëŠ” ì•Šì€ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "    history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)\n",
    "    \n",
    "    # GRU ëª¨ë¸ì„ ë¶ˆëŸ¬ ê²€ì¦ë°ì´í„°ì˜ ì •í™•ë„ í™•ì¸\n",
    "    ## í›ˆë ¨ ê³¼ì •ì—ì„œ ê²€ì¦ ë°ì´í„°ì˜ ì •í™•ë„ê°€ ê°€ì¥ ë†’ì•˜ì„ ë•Œ ì €ì¥ëœ ëª¨ë¸ì¸ 'GRU_model.h5'ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    loaded_model = load_model('GRU_model.h5')\n",
    "    print(\"\\n pre-padding ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì •í™•ë„: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "# í•¨ìˆ˜ ì‹¤í–‰1\n",
    "print(\"post padding case\\n ===========\")\n",
    "post_rnn_test(x_train, x_test, y_train, y_test, maxlen)\n",
    "\n",
    "# í•¨ìˆ˜ ì‹¤í–‰2\n",
    "print(\"pre padding case\\n ===========\")\n",
    "pre_rnn_test(x_train, x_test, y_train, y_test, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìœ„ ì½”ë“œë¥¼ ë¡œì»¬(MAC)ì—ì„œ ê²€ì¦í•  ê²½ìš°, kernel truncated ë°œìƒ\n",
    "### why? CPU default calucation\n",
    "#### ë…¸ë“œì—ì„œ í™•ì¸í•œ ê²°ê³¼ ìº¡ì²˜ë³¸ìœ¼ë¡œ ëŒ€ì²´\n",
    "> post-padding ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì˜ˆì‹œ\n",
    "\n",
    "![post-padding example](../images/1.png)</br>\n",
    "\n",
    "> pre-padding ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì˜ˆì‹œ\n",
    "\n",
    "![pre-padding example](../images/2.png)</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì•„ë˜ ì½”ë“œëŠ” ë…¸ë“œ(`7-8`)ì—ì„œ ì‹¤ì œë¡œ ì²˜ë¦¬í•´ì¤€ ê³¼ì •</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"], \n",
    "                                                        padding='post', # í˜¹ì€ 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                        value=word_to_index[\"<PAD>\"], \n",
    "                                                        padding='post', # í˜¹ì€ 'pre'\n",
    "                                                        maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë…¸ë“œì—ì„œ ì–¸ê¸‰ëœ ì§ˆë¬¸</br>\n",
    "- Q. RNN í™œìš© ì‹œ pad_sequencesì˜ padding ë°©ì‹ì€ 'post'ì™€ 'pre' ì¤‘ ì–´ëŠ ê²ƒì´ ìœ ë¦¬í• ê¹Œìš”? ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?\n",
    "\n",
    "ì˜ˆì‹œ ë‹µë³€</br>\n",
    "- RNNì€ ì…ë ¥ë°ì´í„°ê°€ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´, ê°€ì¥ ë§ˆì§€ë§‰ ì…ë ¥ì´ ìµœì¢… state ê°’ì— ê°€ì¥ ì˜í–¥ì„ ë§ì´ ë¯¸ì¹˜ê²Œ ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ë§ˆì§€ë§‰ ì…ë ¥ì´ ë¬´ì˜ë¯¸í•œ paddingìœ¼ë¡œ ì±„ì›Œì§€ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤. ë”°ë¼ì„œ 'pre'ê°€ í›¨ì”¬ ìœ ë¦¬í•˜ë©°, 10% ì´ìƒì˜ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë³´ì´ê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-9. IMDB ì˜í™”ë¦¬ë·° ê°ìƒë¶„ì„\n",
    "## (2) ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„¤ê³„ì™€ í›ˆë ¨\n",
    "\n",
    "RNN ëª¨ë¸ ì§ì ‘ ì„¤ê³„í•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, None, 16)          160000    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 10)                1080      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                176       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161273 (629.97 KB)\n",
      "Trainable params: 161273 (629.97 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000  # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°ì…ë‹ˆë‹¤(10,000ê°œì˜ ë‹¨ì–´)\n",
    "word_vector_dim = 16   # ë‹¨ì–´ í•˜ë‚˜ë¥¼ í‘œí˜„í•˜ëŠ” ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› ìˆ˜ì…ë‹ˆë‹¤. \n",
    "\n",
    "# model ì„¤ê³„ - ë”¥ëŸ¬ë‹ ëª¨ë¸ ì½”ë“œë¥¼ ì§ì ‘ ì‘ì„±í•´ ì£¼ì„¸ìš”\n",
    "model = tf.keras.Sequential()\n",
    "# [[YOUR CODE]]\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))  # Embedding Layer\n",
    "model.add(tf.keras.layers.LSTM(10)) # One of the famouse RNN models; ì´ì „ ì‹¤ìŠµê³¼ ë‹¤ë¥´ê²Œ 10-dimension ì„¤ì •í•´ë´„\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu')) # output shapeë¥¼ 16ìœ¼ë¡œ ì„¤ì •í•´ë´„\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))   # ìµœì¢…ì¶œë ¥ì€ ê¸ì • / ë¶€ì •ì„ í‘œí˜„í•˜ëŠ” 1-dimension\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model í›ˆë ¨ì „ì— ë‹¤ìŒì„ ì„¸íŒ…í•¨</br>\n",
    "- í›ˆë ¨ìš© ë°ì´í„°ì…‹ 25,000ê±´\n",
    "    - ì´ ì¤‘ 10,000ê±´ì„ ë¶„ë¦¬\n",
    "        - í•´ë‹¹ ë°ì´í„°ëŠ” ê²€ì¦ì…‹ (validation set)ìœ¼ë¡œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 580)\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 10000ê±´ ë¶„ë¦¬\n",
    "x_val = x_train[:10000]\n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation setì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ 15000ê±´\n",
    "partial_x_train = x_train[10000:]\n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model í•™ìŠµì„ ì‹œì‘í•´ ë´…ì‹œë‹¤.</br>\n",
    "> ì´ì „ ê³¼ì •ì—ì„œ ë°ì´í„°ë“¤(`x_train`, `x_test`)ì— ëŒ€í•´ padding ì „ì²˜ë¦¬ë¥¼ í•˜ì§€ ì•Šìœ¼ë©´ Numpy Error (ì°¨ì› ë¶ˆì¼ì¹˜)ê°€ ë°œìƒí•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs=20   # ì–¼ë§ˆë‚˜ í›ˆë ¨í•˜ë©´ ì¢‹ì„ì§€ ê²°ê³¼ë¥¼ ë³´ë©´ì„œ ë°”ê¾¸ì–´ë³´ê¸°\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=epochs, batch_size=512, validation_data=(x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¡œì»¬ í…ŒìŠ¤íŠ¸ (only MAC) ê²½ìš° Truncated ë°œìƒ</br>\n",
    "ì´í›„ ê³¼ì •ì€ ë…¸ë“œ ìº¡ì²˜ë³¸ìœ¼ë¡œ ì •ë¦¬ ëŒ€ì²´í•¨</br>\n",
    "\n",
    "![20 epoch learning example](../images/3.png)</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµì´ ëë‚œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ í‰ê°€í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_text, y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![evaluation with test set](../images/4.png)</br>\n",
    "\n",
    "`model.fit()` ê³¼ì • ì¤‘ì˜ train/validation loss, accuracy ë“±ì´ ë§¤ í›ˆë ¨(`epoch`)ë§ˆë‹¤ history ë³€ìˆ˜ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.</br>\n",
    "ì´ ë°ì´í„°ë¥¼ ê·¸ë˜í”„ë¡œ ê·¸ë ¤ ë³´ë©´, ìˆ˜í–‰í–ˆë˜ ë”¥ëŸ¬ë‹ í•™ìŠµì´ ì˜ ì§„í–‰ë˜ì—ˆëŠ”ì§€, ì˜ëª»ë˜ì—ˆëŠ”ì§€ (`overfitting` í˜¹ì€ `underfitting`), ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì•„ì´ë””ì–´ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())  # epochì— ë”°ë¥¸ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë³¼ ìˆ˜ ìˆëŠ” í•­ëª©ë“¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![check dictionary key of 'history'](../images/5.png)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = histroy_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"ëŠ” \"íŒŒë€ìƒ‰ ì \"ì…ë‹ˆë‹¤.\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# bëŠ” \"íŒŒë€ ì‹¤ì„ \"ì…ë‹ˆë‹¤.\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph of the Training and validation loss](../images/6.png)</br>\n",
    "\n",
    "validation lossì˜ ê·¸ë˜í”„ê°€ train lossì™€ì˜ ì´ê²©(ë²Œì–´ì§ í˜„ìƒ)ì´ ë°œìƒí•˜ê²Œ ë˜ë©´ ë” ì´ìƒì˜ íŠ¸ë ˆì´ë‹ì€ ë¬´ì˜ë¯¸í•´ì§€ê²Œ ëœë‹¤ê³  í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # ê·¸ë¦¼ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph of the Training and validation accuracy](../images/7.png)</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-10. IMDB ì˜í™”ë¦¬ë·° ê°ì„±ë¶„ì„\n",
    "## (3) Word2Vecì˜ ì ìš©\n",
    "\n",
    "ì—¬ê¸°ë¶€í„° ì›Œë“œ ë²¡í„° íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•œë‹¤.</br>\n",
    "\n",
    "```shell\n",
    "# ë…¸ë“œ ì˜ˆì‹œ\n",
    "$ mkdir -p ~/aiffel/sentiment_classification/data\n",
    "$ pip list | grep gensim\n",
    "\n",
    "# Local dir example --> ~/data/sentiment_classification\n",
    "```\n",
    "\n",
    "> í…ŒìŠ¤íŠ¸ ê¸°ì¤€ gensim version is `4.1.2`\n",
    "\n",
    "[Youtube: ë”¥ëŸ¬ë‹ ìì—°ì–´ì²˜ë¦¬](https://youtube.com/watch?v=sY4YyacSsLc&t=126s&ab_channel=MinsukHeoí—ˆë¯¼ì„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape format: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµí•œ Embedding íŒŒë¼ë¯¸í„°ë¥¼ íŒŒì¼ì— ì¨ì„œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "# ë…¸ë“œ ê¸°ì¤€ path\n",
    "# word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/word2vec.txt'\n",
    "# ë¡œì»¬ ê¸°ì¤€ path\n",
    "word2vec_file_path = '../data/sentiment_classification/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "# ë°±í„° ê°œìˆ˜ì™€ ì‚¬ì´ì¦ˆë¥¼ ì–´ë–»ê²Œ ê¸°ì¬í• ì§€ íƒ€ì´í‹€ ì‘ì„±\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))\n",
    "\n",
    "# ë‹¨ì–´ ê°œìˆ˜ë§Œí¼ì˜ ì›Œë“œ ë²¡í„°ë¥¼ íŒŒì¼ì— ê¸°ë¡í•©ë‹ˆë‹¤.\n",
    "## ì´ë•Œ, íŠ¹ìˆ˜ë¬¸ì 4ê°œëŠ” ì œì™¸ì‹œí‚µë‹ˆë‹¤.\n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gensim` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•´, ìœ„ì—ì„œ ì‘ì„±í•œ íŒŒì¼(`ì„ë² ë”© íŒŒë¼ë¯¸í„°`)ì„ ì½ì–´ì„œ word vectorë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07126842,  0.08547736,  0.02286462, -0.03098332,  0.0765068 ,\n",
       "       -0.01164088,  0.060796  , -0.02766533,  0.08184147, -0.06716599,\n",
       "       -0.01517043,  0.02369968,  0.0872837 , -0.02541867, -0.0553827 ,\n",
       "       -0.05494674], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ì˜ ì›Œë“œ ë²¡í„°ë¡œ ì‹¤í—˜ì„ í•  ìˆ˜ ìˆë‹¤.</br>\n",
    "\n",
    "> ì›Œë“œ ë²¡í„°ê°€ ì˜ë¯¸ ë²¡í„° ê³µê°„ìƒì— ìœ ì˜ë¯¸í•˜ê²Œ í•™ìŠµë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜\n",
    ">> ë‹¨ì–´ë¥¼ í•˜ë‚˜ ì£¼ê³  ê·¸ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ì™€ ìœ ì‚¬ë„ë¥¼ í™•ì¸í•˜ëŠ” ë°©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('finely', 0.9213090538978577),\n",
       " ('reality', 0.9122799634933472),\n",
       " ('plant', 0.905373752117157),\n",
       " ('1st', 0.8927375674247742),\n",
       " ('relatively', 0.8903787136077881),\n",
       " ('portrayal', 0.8892521262168884),\n",
       " ('ethan', 0.8865978717803955),\n",
       " ('four', 0.8793693780899048),\n",
       " ('pleasing', 0.8763188123703003),\n",
       " ('toy', 0.8724988102912903)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gensim íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ í•´ë³¼ ìˆ˜ ìˆìŒ\n",
    "word_vectors.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**</br>\n",
    "\n",
    "êµ¬ê¸€ì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „í•™ìŠµëœ(Pretrained) ì›Œë“œ ì„ë² ë”© ëª¨ë¸\n",
    "- 1ì–µ ê°œì˜ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ Google News datasetì„ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸\n",
    "- ì´ 300ë§Œ ê°œì˜ ë‹¨ì–´ë¥¼ ê°ê° 300ì°¨ì›ì˜ ë²¡í„°ë¡œ í‘œí˜„í•¨\n",
    "\n",
    "ì„ë² ë”©ì˜ ê°œë…ì— ëŒ€í•œ ì •ë¦¬ (`í•œêµ­ì–´ ì„ë² ë”©`)</br>\n",
    "\n",
    "- [í•œêµ­ì–´ ì„ë² ë”© ì„œë¬¸](https://ratsgo.github.io/natural%20language%20processing/2019/09/12/embedding/)\n",
    "\n",
    "---\n",
    "\n",
    "Word2Vec ëª¨ë¸ ì ìš©í•´ë³´ê¸°</br>\n",
    "> ë…¸ë“œì—ì„œëŠ” ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì—†ì´ ì‹¬ë³¼ë¦­ ë§í¬ í™œìš©í•¨\n",
    "```shell\n",
    "$ ln -s ~/data/GoogleNew-vectors-negative300.bin.gz ~/aiffel/sentiment_classification/data\n",
    "```\n",
    "\n",
    "[íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬](https://kaggle.com/datasets/leadbest/googlenewsvectorsnegative300)</br>\n",
    "> 2023.07.14 ê¸°ì¤€ íŒŒì¼ í¬ê¸°ê°€ `3.64 GB`ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# ë…¸ë“œ ê¸°ì¤€ path ì˜ˆì‹œ\n",
    "# word2vec_path = os.getenv('HOME')+'aiffel/sentiment_classification/data/GoogleNews-vectors-negative300.bin.gz'\n",
    "# ë¡œì»¬ ê¸°ì¤€ path ì˜ˆì‹œ\n",
    "word2vec_path = '../data/sentiment_classification/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
    "vector = word2vec['computer']\n",
    "vector  # ë¬´ë ¤ 300-dimension ì§œë¦¬ ì›Œë“œ ë²¡í„°.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê²°ê³¼ëŠ” ë…¸ë“œì—ì„œ í™•ì¸í–ˆë˜ shell ìº¡ì²˜ë¡œ ëŒ€ì²´í•¨</br>\n",
    "![Word2Vec dataset load a million counts](../images/8.png)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ë¥¼ ë‹¤ì†Œ ë§ì´ ì†Œë¹„í•˜ëŠ” ì‘ì—…ì´ë‹ˆ ìœ ì˜í•´ ì£¼ì„¸ìš”.\n",
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¨ì–´ë“¤ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ê°€ ê°€ê¹Œìš´ ê²ƒë“¤ì„ í™•ì¸í•˜ëŠ” ì½”ë“œ</br>\n",
    "\n",
    "ë§ˆì°¬ê°€ì§€ë¡œ ë…¸ë“œì—ì„œ í™•ì¸í–ˆë˜ shell ìº¡ì²˜ë¡œ ëŒ€ì²´í•¨</br>\n",
    "![cosine similarity within Word2vec](../images/9.png)</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì•„ë˜ ì½”ë“œëŠ” ì´ì „ ìŠ¤í…ì—ì„œ í•™ìŠµí–ˆë˜ ê³¼ì •ì„ ë‹¤ì‹œ ê¸°ìˆ í•œë‹¤.</br>\n",
    "- ëª¨ë¸ì˜ ì„ë² ë”© ë ˆì´ì–´ë¥¼ `Word2vec`ì˜ ê²ƒìœ¼ë¡œ ëŒ€ì²´í•œ ì˜ˆì œ ìƒ˜í”Œ\n",
    "    - ëª¨ë“  ê³¼ì •ì€ ìœ„ì˜ ê²ƒë“¤ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ë…¸ë“œ í™•ì¸ê²°ê³¼ ì¼¬ì²˜ë³¸ìœ¼ë¡œ ëŒ€ì²´ì˜ˆì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000  # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°ì…ë‹ˆë‹¤.(10,000ê°œì˜ ë‹¨ì–´)\n",
    "word_vector_dim = 300   # ì›Œë“œ ë²¡í„°ì˜ ì°¨ì›ìˆ˜\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrixì— ì›Œë“œ ë²¡í„°ë¥¼ ë‹¨ì–´ë³„ë¡œ í•˜ë‚˜ì”© ì°¨ë¡€ì°¨ë¡€ ì¹´í”¼\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000      # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°. 10,000ê°œì˜ ë‹¨ì–´\n",
    "word_vector_dim = 300   # ì›Œë“œ ë²¡í„°ì˜ ì°¨ì›ìˆ˜\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„±\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, \n",
    "                                    word_vector_dim, \n",
    "                                    # ì¹´í”¼í•œ ì„ë² ë”©ì„ embedding_initializerë¡œ í™œìš©\n",
    "                                    embedding_initializer=Constant(emedding_matrix), \n",
    "                                    input_length=maxlen, \n",
    "                                    # trainable ê°’ì´ Trueë©´ Fine-tuning\n",
    "                                    trainable=True))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sequential model summarize list](../images/10.png)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµì˜ ì§„í–‰\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 20 # í›ˆë ¨ íšŸìˆ˜ëŠ” ë°”ê¿”ê°€ë©´ì„œ í…ŒìŠ¤íŠ¸í•´ë³´ì\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=epochs, batch_size=512, validation_data=(x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![learning process about 20 epochs case..](../images/11.png)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ì…‹ì„ í†µí•œ ëª¨ë¸ í‰ê°€\n",
    "results = model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model evaluation for test set](../images/12.png)</br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
