{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-2. í…ìŠ¤íŠ¸ ê°ì •ë¶„ì„ì˜ ìœ ìš©ì„±\n",
    "\n",
    "í…ìŠ¤íŠ¸ ê°ì •ë¶„ì„ ì ‘ê·¼ë²•</br>\n",
    "- ê¸°ê³„í•™ìŠµ ê¸°ë°˜ ì ‘ê·¼ë²•\n",
    "- ê°ì„±ì‚¬ì „ ê¸°ë°˜ ì ‘ê·¼ë²•\n",
    "    - ê¸°ê³„í•™ìŠµ ëŒ€ë¹„ ë‹¤ìŒì˜ ë‹¨ì ë“¤ì´ ìˆë‹¤.\n",
    "        1. ë¶„ì„ ëŒ€ìƒì— ë”°ë¼ ë‹¨ì–´ì˜ ê°ì„± ì ìˆ˜ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ê°€ëŠ¥ì„±ì— ëŒ€ì‘í•˜ê¸° í˜ë“¦\n",
    "        2. ë‹¨ìˆœ ê¸ë¶€ì •ì„ ë„˜ì–´ì„œ ê¸ë¶€ì •ì˜ ì›ì¸ì´ ë˜ëŠ” ëŒ€ìƒ ì†ì„± ê¸°ë°˜ì˜ ê°ì„± ë¶„ì„ì´ ì–´ë ¤ì›€\n",
    "- ë°ì´í„°ë¶„ì„ ì—…ë¬´ ì¸¡ë©´ì—ì„œì˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸\n",
    "    - ì¼ë°˜ì ì¸ ë°ì´í„°ë¶„ì„ ì—…ë¬´ì—ì„œëŠ” ë²”ì£¼í™”ê°€ ì˜ëœ ì •í˜•ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•¨\n",
    "        - ì •í˜•ë°ì´í„°ëŠ” ê·œëª¨ê°€ ì»¤ì§ˆìˆ˜ë¡ ë¹„ìš©ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•  ìˆ˜ ìˆë‹¤.\n",
    "    - ë¹„ì •í˜•ë°ì´í„°ì¸ í…ìŠ¤íŠ¸ì— ê°ì„±ë¶„ì„ ê¸°ë²•ì„ ì‚¬ìš©\n",
    "        - í…ìŠ¤íŠ¸ë¥¼ ì •í˜•ë°ì´í„°ë¡œ ê°€ê³µí•˜ì—¬ ìœ ìš©í•œ ì˜ì‚¬ê²°ì • ë³´ì¡°ìë£Œë¡œ í™œìš© ê°€ëŠ¥í•¨\n",
    "\n",
    "\n",
    "ì›Œë“œ ì„ë² ë”©(word embedding) ê¸°ë²•</br>\n",
    "- ë‹¨ì–´ì˜ íŠ¹ì„±ì„ ì €ì°¨ì› ë²¡í„° ê°’ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ë²•\n",
    "- ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„±ë¶„ì„ ì‹œ ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆìŒ\n",
    "    - ë¨¸ì‹ ëŸ¬ë‹ ì¶”ë¡  ì¼ì¹˜ë„(ì •í™•ë„)ë¥¼ í¬ê²Œ í–¥ìƒí•  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-4. í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ íŠ¹ì§•\n",
    "## (1) í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "ë‹¨ì–´ì™€ ê·¸ **ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°**ë¥¼ ì§ì§€ì–´ ë³´ì.</br>\n",
    "> ë§ˆì¹˜ ì‚¬ì „ì—ì„œ íŠ¹ì • ë‹¨ì–´ì™€ ê·¸ ë‹¨ì–´ì— ëŒ€í•œ ì„¤ëª…ì´ ì§ì§€ì–´ì§„ ê²ƒ ì²˜ëŸ¼\n",
    "\n",
    "í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì œ\n",
    "```text\n",
    "## í…ìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì‹œ\n",
    "\n",
    "    i feel hungry\n",
    "    i eat lunch\n",
    "    now i feel happy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'feel', 'hungry']\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì¥ ì…ë ¥ ë° ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ split\n",
    "# ì²˜ë¦¬í•´ì•¼ í•  ë¬¸ì¥ì„ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ì— ì˜®ê²¨ ë‹´ì•˜ìŠµë‹ˆë‹¤.\n",
    "sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# íŒŒì´ì¬ split() ë©”ì†Œë“œë¥¼ ì´ìš©í•´ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ ìª¼ê°œ ë´…ë‹ˆë‹¤.\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"
     ]
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì „ì„ ë§Œë“¤ê¸° ìœ„í•œ ì˜ˆì‹œ\n",
    "## python dictionary í™œìš©\n",
    "\n",
    "index_to_word={}  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“¤ì–´ì„œ\n",
    "\n",
    "# ë‹¨ì–´ë“¤ì„ í•˜ë‚˜ì”© ì±„ì›Œ ë´…ë‹ˆë‹¤. ì±„ìš°ëŠ” ìˆœì„œëŠ” ì¼ë‹¨ ì„ì˜ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‚¬ì‹¤ ìˆœì„œëŠ” ì¤‘ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. \n",
    "# <BOS>, <PAD>, <UNK>ëŠ” ê´€ë¡€ì ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ ë§¨ ì•ì— ë„£ì–´ì¤ë‹ˆë‹¤. \n",
    "index_to_word[0]='<PAD>'  # íŒ¨ë”©ìš© ë‹¨ì–´\n",
    "index_to_word[1]='<BOS>'  # ë¬¸ì¥ì˜ ì‹œì‘ì§€ì \n",
    "index_to_word[2]='<UNK>'  # ì‚¬ì „ì— ì—†ëŠ”(Unknown) ë‹¨ì–´\n",
    "index_to_word[3]='i'\n",
    "index_to_word[4]='feel'\n",
    "index_to_word[5]='hungry'\n",
    "index_to_word[6]='eat'\n",
    "index_to_word[7]='lunch'\n",
    "index_to_word[8]='now'\n",
    "index_to_word[9]='happy'\n",
    "\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'i': 3, 'feel': 4, 'hungry': 5, 'eat': 6, 'lunch': 7, 'now': 8, 'happy': 9}\n"
     ]
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ê¸°\n",
    "## ì˜ˆì œ ê¸°ì¤€ì—ì„œ keyì™€ valueë¥¼ ë°”ê¿”ë²„ë¦¬ë©´ ëœë‹¤.\n",
    "word_to_index={word:index for index, word in index_to_word.items()}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# ë°”ë€ ë‹¨ì–´ì˜ ê°’(ìˆ«ì)ì„ í™•ì¸\n",
    "print(word_to_index['feel'])  # ë‹¨ì–´ 'feel'ì€ ìˆ«ì ì¸ë±ìŠ¤ 4ë¡œ ë°”ë€ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# ê°€ì§€ê³  ìˆëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë“¤ì„ ìˆ«ìë¡œ ë°”ê¿” í‘œí˜„í•˜ê¸°\n",
    "# ë¬¸ì¥ 1ê°œë¥¼ í™œìš©í•  ë”•ì…”ë„ˆë¦¬ì™€ í•¨ê»˜ ì£¼ë©´, ë‹¨ì–´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.\n",
    "# ë‹¨, ëª¨ë“  ë¬¸ì¥ì€ <BOS>ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒìœ¼ë¡œ í•©ë‹ˆë‹¤. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]\n"
     ]
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ ë°ì´í„° ì‚¬ì „í™” ì˜ˆì‹œ\n",
    "# ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ í•œêº¼ë²ˆì— ìˆ«ì í…ì„œë¡œ encodeí•´ ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] ê°€ ì•„ë˜ì™€ ê°™ì´ ë³€í™˜ë©ë‹ˆë‹¤. \n",
    "encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i feel hungry\n"
     ]
    }
   ],
   "source": [
    "# ìˆ«ì ê°’ìœ¼ë¡œ í‘œí˜„ëœ ë‹¨ì–´ë“¤ì„ ë‹¤ì‹œ ì›ë˜ ë‹¨ì–´(sentence)ë¡œ ë³µêµ¬í•˜ê¸°\n",
    "# ìˆ«ì ë²¡í„°ë¡œ encodeëœ ë¬¸ì¥ì„ ì›ë˜ëŒ€ë¡œ decodeí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]ë¥¼ í†µí•´ <BOS>ë¥¼ ì œì™¸\n",
    "\n",
    "print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i feel hungry', 'i eat lunch', 'now i feel happy']\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì¥ encode, decode ì˜ˆì‹œ\n",
    "# ì—¬ëŸ¬ ê°œì˜ ìˆ«ì ë²¡í„°ë¡œ encodeëœ ë¬¸ì¥ì„ í•œêº¼ë²ˆì— ì›ë˜ëŒ€ë¡œ decodeí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] ê°€ ì•„ë˜ì™€ ê°™ì´ ë³€í™˜ë©ë‹ˆë‹¤.\n",
    "print(get_decoded_sentences(encoded_sentences, index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-5. í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ íŠ¹ì§•\n",
    "## (2) Embedding ë ˆì´ì–´ì˜ ë“±ì¥\n",
    "ì„ë² ë”©(Embedding)?\n",
    "```text\n",
    "ìì—°ì–´ ì²˜ë¦¬(Natural Language Processing)ë¶„ì•¼ì—ì„œ ë§í•˜ëŠ” ì„ë² ë”©(Embedding)\n",
    "- ì‚¬ëŒì´ ì“°ëŠ” ìì—°ì–´ë¥¼ ê¸°ê»˜ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ìí˜•íƒœ(vector)ë¡œ ë°”ê¾¼ ê²°ê³¼\n",
    "- í˜¹ì¸ ê·¸ ì¼ë ¨ì˜ ê³¼ì • ì „ì²´\n",
    "```\n",
    "What to do with Embedding?\n",
    "```text\n",
    "ë‹¨ì–´ë‚˜ ë¬¸ì¥ ì‚¬ì´ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ ë‹¨ì–´ êµ¬í•˜ê¸° (ê³„ì‚°)\n",
    "ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ì˜ë¯¸/ë¬¸ë²•ì  ì •ë³´ ë„ì¶œ\n",
    "ë‹¨ì–´ ì‚¬ì´ ë¬¸ë²•ì  ê´€ê³„ ë„ì¶œ (ë²¡í„° ê°„ ì—°ì‚°)\n",
    "```\n",
    "> ì „ì´ í•™ìŠµ(transfer learning) ì„ë² ë”©\n",
    ">> ë‹¤ë¥¸ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ìì£¼ ì“°ì„\n",
    ">> í’ˆì§ˆ ì¢‹ì€ ì„ë² ë”©ì„ ì‚¬ìš©í• ìˆ˜ë¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ì•„ì§\n",
    "[ì„ë² ë”© ë ˆì´ì–´ë¥¼ í†µí•´ word to vector](https://wikidocs.net/64779)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# ìˆ«ìë¡œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ ë°ì´í„° [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] ì— Embedding ë ˆì´ì–´ë¥¼ ì ìš©í•©ë‹ˆë‹¤. \u001b[39;00m\n\u001b[1;32m     14\u001b[0m raw_inputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(get_encoded_sentences(sentences, word_to_index), dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m output \u001b[39m=\u001b[39m embedding(raw_inputs)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/myconda/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/myconda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "# word to vector with tensorflow ì˜ˆì œ\n",
    "# ì•„ë˜ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ì‹¤í–‰í•˜ì‹œë©´ ì—ëŸ¬ê°€ ë°œìƒí•  ê²ƒì…ë‹ˆë‹¤. \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "vocab_size = len(word_to_index)  # ìœ„ ì˜ˆì‹œì—ì„œ ë”•ì…”ë„ˆë¦¬ì— í¬í•¨ëœ ë‹¨ì–´ ê°œìˆ˜ëŠ” 10\n",
    "word_vector_dim = 4    # ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ 4ì°¨ì›ì˜ ì›Œë“œ ë²¡í„°ë¥¼ ê°€ì •í•©ë‹ˆë‹¤. \n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# ìˆ«ìë¡œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ ë°ì´í„° [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] ì— Embedding ë ˆì´ì–´ë¥¼ ì ìš©í•©ë‹ˆë‹¤. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype='object')\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer ì£¼ì˜ì \n",
    "Embedding ë ˆì´ì–´ì˜ ì¸í’‹ ë¬¸ì¥ ë²¡í„°ì˜ **ê¸¸ì´ëŠ” ì¼ì •**í•´ì•¼í•œë‹¤.</br>\n",
    "> raw_inputsì— ìˆëŠ” 3ê°œ ë² ê±°ì˜ ê¸¸ì´ëŠ” ê°ê° ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    ">> 4, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFì˜ í•¨ìˆ˜ ì‚¬ìš©í•˜ì—¬ input ë²¡í„° ê¸¸ì´ ë§ì¶”ê¸°\n",
    "## tf.keras.preprocessing.sequence.pad_sequences() ì‚¬ìš©\n",
    "## ë¬¸ì¥ ë²¡í„° ë’¤ì— íŒ¨íŒ…(<PAD>)ì„ ì¶”ê°€í•˜ì—¬ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶°ì¤Œ\n",
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "print(raw_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <PAD>ì—ëŠ” 0ê³¼ ë§¤í•‘ë˜ì–´ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input ë²¡í„°ë“¤ (raw_inputs)ì˜ ê¸¸ì´ë¥¼ ë§ì¶°ì£¼ë©´ì„œ word to vectorë¥¼ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œ\n",
    "vocab_size = len(word_to_index)  # ìœ„ ì˜ˆì‹œì—ì„œ ë”•ì…”ë„ˆë¦¬ì— í¬í•¨ëœ ë‹¨ì–´ ê°œìˆ˜ëŠ” 10\n",
    "word_vector_dim = 4    # ê·¸ë¦¼ê³¼ ê°™ì´ 4ì°¨ì›ì˜ ì›Œë“œ ë²¡í„°ë¥¼ ê°€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# tf.keras.preprocessing.sequence.pad_sequencesë¥¼ í†µí•´ word vectorë¥¼ ëª¨ë‘ ì¼ì • ê¸¸ì´ë¡œ ë§ì¶°ì£¼ì–´ì•¼ \n",
    "# embedding ë ˆì´ì–´ì˜ inputì´ ë  ìˆ˜ ìˆìŒì— ì£¼ì˜í•´ ì£¼ì„¸ìš”. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype=object)\n",
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> outputì˜ shape(3, 5, 4)ì—ì„œ ê° ê°’ì˜ ì˜ë¯¸\n",
    ">> 3 : ì…ë ¥ë¬¸ì¥ ê°œìˆ˜\n",
    ">> 5 : ì…ë ¥ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´\n",
    ">> 4 : ì›Œë“œ ë²¡í„°ì˜ ì°¨ì› ìˆ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-6. ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” RNN\n",
    "> í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ”ë° ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸\n",
    ">> Recurrent Neural Network (RNN)\n",
    ">> ì‹œí€€ìŠ¤(Sequence) í˜•íƒœì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸°ì— ìµœì ì¸ ëª¨ë¸ë¡œ ì•Œë ¤ì ¸ ìˆìŒ\n",
    "\n",
    "[stateful vs stateless ì˜ˆì‹œ; Webì˜ ê´€ì ì—ì„œ](https://www.slideshare.net/xguru/ss-16106464)</br>\n",
    "[ëª¨ë‘ì˜ ë”¥ëŸ¬ë‹ ê°•ì¢Œ - 12ê°•.RNN, ê¹€ì„±í›ˆ êµìˆ˜](https://youtu.be/-SHPG_KMUkQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì œ ì½”ë“œ\n",
    "## í…ìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì´ì „ì˜ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸\n",
    "vocab_size = 10  # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°ì…ë‹ˆë‹¤(10ê°œì˜ ë‹¨ì–´)\n",
    "word_vector_dim = 4  # ë‹¨ì–´ í•˜ë‚˜ë¥¼ í‘œí˜„í•˜ëŠ” ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›ìˆ˜ì…ë‹ˆë‹¤. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(8))   # ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” RNNì¸ LSTM ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ë•Œ LSTM state ë²¡í„°ì˜ ì°¨ì›ìˆ˜ëŠ” 8ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤. (ë³€ê²½ ê°€ëŠ¥)\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # ìµœì¢… ì¶œë ¥ì€ ê¸ì •/ë¶€ì •ì„ ë‚˜íƒ€ë‚´ëŠ” 1dim ì…ë‹ˆë‹¤.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ì‹œí€€ìŠ¤ ë°ì´í„°ì™€ RNN ì°¸ê³ ìë£Œ\n",
    "[Youtube Link](https://youtu.be/mG6N0ut9dog?t=1447)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-7. ê¼­ RNNì´ì–´ì•¼ í• ê¹Œ?\n",
    "í…ìŠ¤íŠ¸ ì²˜ë¦¬ì—ì„œëŠ” RNNì´ ì•„ë‹ˆë¼ `1-D Convolution Neural Network (1-D CNN)`ë¥¼ ì‚¬ìš©í• ìˆ˜ë„ ìˆë‹¤.</br>\n",
    "```text\n",
    "ì´ì „ì— ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° êµ¬í˜„ì—ì„œ `2-D CNN`ì„ ì‚¬ìš©í•´ë³´ì•˜ë‹¤.\n",
    "\n",
    " - ì´ë¯¸ì§€ëŠ” ì‹œí€€ìŠ¤ ë°ì´í„°ê°€ ì•„ë‹ˆë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° ëª¨ë¸ì—ì„œ ì‚¬ìš©ëœ ì…ë ¥ì€ ì´ë¯¸ì§€ ì „ì²´ë‹¤.\n",
    "```\n",
    "> 1-D CNN (with text data)\n",
    ">> ë¬¸ì¥ ì „ì²´ë¥¼ í•œêº¼ë²ˆì— í•œ ë°©í–¥ìœ¼ë¡œ ìŠ¤ìºë‹\n",
    ">> í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ì–‘ì€ 7 (len 7)\n",
    ">> 7 ë‹¨ì–´ ì´ë‚´ì—ì„œ ë°œê²¬ë˜ëŠ” íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ ì¶”ì¶œê²°ê³¼ê°’ì„ ë¬¸ì¥ ë¶„ë¥˜ì— ì‚¬ìš©í•¨\n",
    "\n",
    "`1-D CNN`ë°©ì‹ë„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ì—ì„œ RNN ëª»ì§€ì•Šì€ íš¨ìœ¨ì„ ë³´ì—¬ì¤Œ</br>\n",
    "> CNN ê³„ì—´ì˜ ì´ì \n",
    ">> RNN ê³„ì—´ë³´ë‹¤ ë³‘ë ¬ì²˜ë¦¬ê°€ íš¨ìœ¨ì ì´ë‹¤\n",
    ">> í•™ìŠµ ì†ë„ê°€ RNNì— ë¹ ë¥´ê²Œ ì§„í–‰ëœë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-D CNNì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ\n",
    "vocab_size = 10  # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°ì…ë‹ˆë‹¤(10ê°œì˜ ë‹¨ì–´)\n",
    "word_vector_dim = 4   # ë‹¨ì–´ í•˜ë‚˜ë¥¼ í‘œí˜„í•˜ëŠ” ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› ìˆ˜ì…ë‹ˆë‹¤. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # ìµœì¢… ì¶œë ¥ì€ ê¸ì •/ë¶€ì •ì„ ë‚˜íƒ€ë‚´ëŠ” 1dim ì…ë‹ˆë‹¤.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-D CNN ê¸°ë²•ì—ì„œ GlobalMaxPooling() ë ˆì´ì–´ë§Œ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œ\n",
    "## desc.) ì „ì²´ ë¬¸ì¥ ì¤‘ì—ì„œ ë‹¨ í•˜ë‚˜ì˜ ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´ë§Œ featureë¡œ ì¶”ì¶œ\n",
    "## ì¶”ì¶œëœ featureë¡œ ë¬¸ì¥ì˜ ê¶/ë¶€ì •ì„ í‰ê°€\n",
    "vocab_size = 10  # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°ì…ë‹ˆë‹¤(10ê°œì˜ ë‹¨ì–´)\n",
    "word_vector_dim = 4   # ë‹¨ì–´ í•˜ë‚˜ë¥¼ í‘œí˜„í•˜ëŠ” ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› ìˆ˜ì…ë‹ˆë‹¤. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # ìµœì¢… ì¶œë ¥ì€ ê¸ì •/ë¶€ì •ì„ ë‚˜íƒ€ë‚´ëŠ” 1dim ì…ë‹ˆë‹¤.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ì´ ì™¸ì—ë„ ê³ ë ¤í• ë§Œí•œ ë°©ë²•\n",
    ">> 1-D CNNê³¼ RNN ì„ì–´ ì“°ê¸°\n",
    ">> FFN (FeedForward Network) ë ˆì´ì–´ë§Œ ì‚¬ìš©í•´ë³´ê¸°\n",
    ">> Transformer ë ˆì´ì–´ ì‚¬ìš©í•´ë³´ê¸°\n",
    "\n",
    "[ì°¸ê³ ë§í¬](https://wikidocs.net/80437)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-8. IMDB ì˜í™”ë¦¬ë·° ê°ì„±ë¶„ì„\n",
    "## (1) IMDB ë°ì´í„°ì…‹ ë¶„ì„\n",
    "> ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ IMDb ì˜í™”ë¦¬ë·° ê°ì„±ë¶„ì„ íƒœìŠ¤í¬ì— ë„ì „í•´ ë³´ê² ìŠµë‹ˆë‹¤. IMDb Large Movie Datasetì€ 50000ê°œì˜ ì˜ì–´ë¡œ ì‘ì„±ëœ ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ê¸ì •ì€ 1, ë¶€ì •ì€ 0ì˜ ë¼ë²¨ì´ ë‹¬ë ¤ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "[ê´€ë ¨ë…¼ë¬¸](https://aclanthology.org/P11-1015.pdf)</br>\n",
    "\n",
    "> ì•„ë˜ëŠ” ë…¸ë“œì— ì–¸ê¸‰ëœ ë‚´ìš©\n",
    "```text\n",
    "50000ê°œì˜ ë¦¬ë·° ì¤‘ ì ˆë°˜ì¸ 25000ê°œê°€ í›ˆë ¨ìš© ë°ì´í„°, ë‚˜ë¨¸ì§€ 25000ê°œë¥¼ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ë„ë¡ ì§€ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ tensorflow Keras ë°ì´í„°ì…‹ ì•ˆì— í¬í•¨ë˜ì–´ ìˆì–´ì„œ ì†ì‰½ê²Œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "ì´í›„ ìŠ¤í…ì˜ IMDb ë°ì´í„°ì…‹ ì²˜ë¦¬ ì½”ë“œ ì¤‘ ì¼ë¶€ëŠ” Tensorflow íŠœí† ë¦¬ì–¼ì— ì–¸ê¸‰ëœ ë°ì´í„° ì „ì²˜ë¦¬ ë¡œì§ì„ ì°¸ê³ í•˜ì˜€ìŒì„ ë°í™ë‹ˆë‹¤.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDb ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° (train)ì™€ ì •ë‹µ ë°ì´í„°(test)ë¡œ ë¶„ë¦¬\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "print(f\"í›ˆë ¨ ìƒ˜í”Œ ê°œìˆ˜: {len(x_train)}, í…ŒìŠ¤íŠ¸ ê°œìˆ˜: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì˜ ì˜ˆì‹œ í™•ì¸\n",
    "print(x_train[0])  # 1ë²ˆì§¸ ë¦¬ë·°ë°ì´í„°\n",
    "print('ë¼ë²¨: ', y_train[0])  # 1ë²ˆì§¸ ë¦¬ë·°ë°ì´í„°ì˜ ë¼ë²¨\n",
    "print('1ë²ˆì§¸ ë¦¬ë·° ë¬¸ì¥ ê¸¸ì´: ', len(x_train[0]))\n",
    "print('2ë²ˆì§¸ ë¦¬ë·° ë¬¸ì¥ ê¸¸ì´: ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ë…¸ë“œì—ì„œ ì–¸ê¸‰í•œ ë‚´ìš©\n",
    "```text\n",
    "í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì•„ë‹ˆë¼ ì´ë¯¸ ìˆ«ìë¡œ encodeëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí–ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "ì´ë¯¸ í…ìŠ¤íŠ¸ê°€ encodeë˜ì—ˆìœ¼ë¯€ë¡œ IMDb ë°ì´í„°ì…‹ì—ëŠ” encodeì— ì‚¬ìš©í•œ ë”•ì…”ë„ˆë¦¬ê¹Œì§€ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word to vector ì˜ˆì‹œ\n",
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])     # 'the' ê°€ ì¶œë ¥ë©ë‹ˆë‹¤. \n",
    "print(word_to_index['the'])  # 1 ì´ ì¶œë ¥ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ë…¸ë“œì—ì„œ ì–¸ê¸‰í•œ ì£¼ì˜ì \n",
    "```text\n",
    "ì—¬ê¸°ì„œ ì£¼ì˜í•  ì ì´ ìˆìŠµë‹ˆë‹¤. IMDb ë°ì´í„°ì…‹ì˜ í…ìŠ¤íŠ¸ ì¸ì½”ë”©ì„ ìœ„í•œ word_to_index, index_to_wordëŠ” ë³´ì •ì´ í•„ìš”í•œë°ìš”.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰ì‹œì¼œë³´ë©´ ë³´ì •ì´ ë˜ì§€ ì•Šì€ ìƒíƒœë¼ ë¬¸ì¥ì´ ì´ìƒí•¨ì„ í™•ì¸í•˜ì‹¤ ê²ë‹ˆë‹¤. (ë’¤ì— ë³´ì • í›„ ë‹¤ì‹œ í™•ì¸í•´ë³¼ ì˜ˆì •ì´ì—ìš”.ğŸ˜Š)\n",
    "```\n",
    ">> IMDB ë°ì´í„°ì…‹ì„ í…ìŠ¤íŠ¸ ì¸ì½”ë”© í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒì˜ ë³´ì •ì´ í•„ìš”\n",
    ">> `word_to_index`\n",
    ">> `index_to_word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³´ì • ì „ x_train[0] ë°ì´í„°\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## x_train ë°ì´í„° ë³´ì • ì‘ì—… ì˜ˆì‹œ\n",
    "#ì‹¤ì œ ì¸ì½”ë”© ì¸ë±ìŠ¤ëŠ” ì œê³µëœ word_to_indexì—ì„œ index ê¸°ì¤€ìœ¼ë¡œ 3ì”© ë’¤ë¡œ ë°€ë ¤ ìˆìŠµë‹ˆë‹¤.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# ì²˜ìŒ ëª‡ ê°œ ì¸ë±ìŠ¤ëŠ” ì‚¬ì „ì— ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])     # '<BOS>' ê°€ ì¶œë ¥ë©ë‹ˆë‹¤. \n",
    "print(word_to_index['the'])  # 4 ì´ ì¶œë ¥ë©ë‹ˆë‹¤. \n",
    "print(index_to_word[4])     # 'the' ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
    "\n",
    "# ë³´ì • í›„ x_train[0] ë°ì´í„°\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode ëœ ë°ì´í„°ë¥¼ decode í•˜ì—¬ ê²°ê³¼ í™•ì¸í•˜ëŠ” ì˜ˆì‹œ\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))\n",
    "print('ë¼ë²¨: ', y_train[0])  # 1ë²ˆì§¸ ë¦¬ë·°ë°ì´í„°ì˜ ë¼ë²¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ë…¸ë“œì—ì„œ ì–¸ê¸‰í•  ì£¼ì˜ì \n",
    "```text\n",
    "pad_sequencesë¥¼ í†µí•´ ë°ì´í„°ì…‹ ìƒì˜ ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ í†µì¼í•˜ëŠ” ê²ƒì„ ìŠì–´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.\n",
    "ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´ maxlenì˜ ê°’ ì„¤ì •ë„ ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ê²Œ ë©ë‹ˆë‹¤. ì´ ê¸¸ì´ë„ ì ì ˆí•œ ê°’ì„ ì°¾ê¸° ìœ„í•´ì„œëŠ” ì „ì²´ ë°ì´í„°ì…‹ì˜ ë¶„í¬ë¥¼ í™•ì¸í•´ ë³´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "```\n",
    ">> `pad_sequences`: ë°ì´í„°ì…‹ ìƒì˜ ë¬¸ì¥ ê¸¸ì´ë¥¼ í†µì¼\n",
    ">> ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´ (`maxlen`ì˜ ê°’) ì„¤ì •ë„ ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ë°ì´í„°ì…‹ í™•ì¸ì˜ˆì‹œ\n",
    "\n",
    "total_data_text = list(x_train) + list(x_test)\n",
    "# í…ìŠ¤íŠ¸ë°ì´í„° ë¬¸ì¥ê¸¸ì´ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œ í›„\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# ë¬¸ì¥ê¸¸ì´ì˜ í‰ê· ê°’, ìµœëŒ€ê°’, í‘œì¤€í¸ì°¨ë¥¼ ê³„ì‚°í•´ ë³¸ë‹¤. \n",
    "print('ë¬¸ì¥ê¸¸ì´ í‰ê·  : ', np.mean(num_tokens))\n",
    "print('ë¬¸ì¥ê¸¸ì´ ìµœëŒ€ : ', np.max(num_tokens))\n",
    "print('ë¬¸ì¥ê¸¸ì´ í‘œì¤€í¸ì°¨ : ', np.std(num_tokens))\n",
    "\n",
    "# ì˜ˆë¥¼ë“¤ì–´, ìµœëŒ€ ê¸¸ì´ë¥¼ (í‰ê·  + 2*í‘œì¤€í¸ì°¨)ë¡œ í•œë‹¤ë©´,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print(f'ì „ì²´ ë¬¸ì¥ì˜ {np.sum(num_tokens < max_tokens) / len(num_tokens)}%ê°€ maxlen ì„¤ì •ê°’ ì´ë‚´ì— í¬í•¨ë©ë‹ˆë‹¤. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ë…¸ë“œì—ì„œ ì–¸ê¸‰í•œ ë‚´ìš©\n",
    "```text\n",
    "ìœ„ì˜ ê²½ìš°ì—ëŠ” maxlen=580ì´ ë©ë‹ˆë‹¤.\n",
    "ë˜ í•œ ê°€ì§€ ìœ ì˜í•´ì•¼ í•˜ëŠ” ê²ƒì€ padding ë°©ì‹ì„ ë¬¸ì¥ ë’¤ìª½('post')ê³¼ ì•ìª½('pre') ì¤‘ ì–´ëŠ ìª½ìœ¼ë¡œ í•˜ëŠëƒì— ë”°ë¼ RNNì„ ì´ìš©í•œ ë”¥ëŸ¬ë‹ ì ìš© ì‹œ ì„±ëŠ¥ ì°¨ì´ê°€ ë°œìƒí•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.\n",
    "ë‘ ê°€ì§€ ë°©ì‹ì„ í•œ ë²ˆì”© ë‹¤ ì ìš©í•´ì„œ RNNì„ í•™ìŠµì‹œì¼œ ë³´ë©´ì„œ ê·¸ ê²°ê³¼ë¥¼ ë¹„êµí•´ ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
    "```\n",
    ">> ë²¡í„° ê¸¸ì´ ë§ì¶œ ë•Œ, padding ë°©ì‹ì—ëŠ” ë‘ ê°€ì§€ ë°©ì‹ì´ ìˆë‹¤.\n",
    "1. ê¸¸ì´ë¥¼ ë§ì¶œ ë•Œ, padding ê°’ì„ ë’¤ìª½ì— ì‚½ì…í•˜ëŠ” ë°©ì‹: `post`\n",
    "2. ê¸¸ì´ë¥¼ ë§ì¶œ ë•Œ, padding ê°’ì„ ì•ìª½ì— ì‚½ì…í•˜ëŠ” ë°©ì‹: `pre`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
